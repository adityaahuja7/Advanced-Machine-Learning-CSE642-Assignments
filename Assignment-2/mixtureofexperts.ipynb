{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=training_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Data/cf_train_no_noise.csv\")\n",
    "encoding_map = {0: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1: 4}\n",
    "\n",
    "# Define a function to map the values to classes\n",
    "def encode_to_classes(value):\n",
    "    for key, val in encoding_map.items():\n",
    "        if value == key:\n",
    "            return val\n",
    "    return None  # return None if value doesn't match any key\n",
    "\n",
    "# Apply the function to create a new column with encoded classes\n",
    "df['target_10_val'] = df['target_10_val'].apply(encode_to_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self,dataframe,batch_size,device =training_device,shuffle=False):\n",
    "        self.df=dataframe\n",
    "        self.batch_size=batch_size\n",
    "        self.columns_to_drop=['row_num','day','era','target_10_val','target_5_val','sigma','day_no']\n",
    "        self.X = self.df.drop(self.columns_to_drop, axis=1)\n",
    "        self.y=self.df['target_10_val']\n",
    "        self.device=device\n",
    "        self.shuffle=shuffle\n",
    "\n",
    "    def generate_batches_with_labels(self,start_idx,end_idx):\n",
    "        data=self.X.iloc[start_idx:end_idx]\n",
    "        labels=self.y.iloc[start_idx:end_idx]\n",
    "        dataset =  torch.utils.data.TensorDataset(torch.tensor(data.values),torch.tensor(labels.values))\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "        # data_unseen = self.X.iloc[max(0,idx-9):idx+1]\n",
    "        # labels_unseen=self.y.iloc[max(0,idx-9):idx+1]\n",
    "        # data_unseen,labels_unseen = torch.tensor(data_unseen.values).to(self.device),torch.tensor(labels_unseen.values).to(self.device)\n",
    "        return dataloader\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "customDS = CustomDataset(df,64,shuffle = False)\n",
    "# supervised_dl, unsupervised_data = customDS.generate_batches_with_labels(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(torch.nn.Module):\n",
    "    def __init__ (self, layers,activation = torch.nn.ReLU(), dropout = 0.5):\n",
    "        super().__init__()\n",
    "        linear = [torch.nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.layers = []\n",
    "        for i in range(len(linear)):\n",
    "            if (i == len(linear)-1):\n",
    "                self.layers.append(linear[i])\n",
    "            else:\n",
    "                self.layers.append(linear[i])\n",
    "                self.layers.append(activation)\n",
    "                if (dropout!=0): self.layers.append(torch.nn.Dropout(dropout))\n",
    "        self.layers = torch.nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.softmax = torch.nn.LogSoftmax(dim = 1)\n",
    "        \n",
    "    def forward(self, X):          \n",
    "        X = self.layers(X)\n",
    "        X = self.softmax(X)\n",
    "        return X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10,verbose=True):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        if val_loader is None:\n",
    "            continue\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.long)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train')\n",
    "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = MyMLP(layers = [24,64,32,12], dropout=0).to(device)\n",
    "Model = Model.to(device)\n",
    "print(Model)\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr=0.01)\n",
    "train_model(Model,torch.nn.NLLLoss(), optimizer,Noise_0_era_train_loader, Noise_0_era_test_loader,num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62400"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experts(no_of_experts):\n",
    "    len_of_individual_train=len(df)//no_of_experts\n",
    "    start=[i*len_of_individual_train for i in range(no_of_experts)]\n",
    "    for i in range(len(start)):\n",
    "        if i==len(start)-1:\n",
    "            train_loader=customDS.generate_batches_with_labels(start[i],len(df))\n",
    "        else:\n",
    "            train_loader=customDS.generate_batches_with_labels(start[i],start[i+1])\n",
    "        Model = MyMLP(layers = [24,64,32,5], dropout=0).to(device)\n",
    "        Model = Model.to(device)\n",
    "        optimizer = torch.optim.Adam(Model.parameters(), lr=0.001)\n",
    "        train_model(Model,torch.nn.NLLLoss(), optimizer,train_loader, None,num_epochs=200)\n",
    "        PATH=\"Experts/expert_no_\"+str(i+1)+\".pt\"\n",
    "        torch.save(Model, PATH)\n",
    "    print(\"EVERYTHING COMPLETED SUCCESSFULLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need 6 experts to prevent cases where all vote for one class and we get atleast some majority\n",
    "train_experts(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winnow(no_of_experts):\n",
    "    experts=[]\n",
    "    weights=[]\n",
    "    balance=[]\n",
    "    for i in range(no_of_experts):\n",
    "        model=torch.load(\"no_noise_expert_\"+str(i+1)+\".pt\")\n",
    "        model.eval()\n",
    "        experts.append(model)\n",
    "        weights.append(1)\n",
    "        balance.append(0)\n",
    "    increase_count=0\n",
    "    reduce_count=0\n",
    "    # winnow majority on all of datasets\n",
    "    train_loader=customDS.generate_batches_with_labels(0,len(df))\n",
    "    for inputs,labels in tqdm(train_loader):\n",
    "#         print(inputs.shape)\n",
    "        for i in range(len(inputs)):\n",
    "            predictions=[0,0,0,0,0]\n",
    "            dict_of_predictions={0:[],1:[],2:[],3:[],4:[]}\n",
    "            \n",
    "            # Gathering Predictions\n",
    "            for j in range(len(experts)):\n",
    "                model=experts[j]\n",
    "                input_copy = inputs[i].unsqueeze(0).clone()\n",
    "                output=model(input_copy.to(device).to(torch.float32))\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                predicted=int(predicted)\n",
    "                predictions[predicted]+=balance[j]\n",
    "                dict_of_predictions[predicted].append(j)\n",
    "\n",
    "            result_pred=predictions.index(max(predictions))\n",
    "            correct_result=int(labels[i])\n",
    "            # reducing/increasing weights only if prediction wrong\n",
    "            if result_pred!=correct_result:\n",
    "                # increasing Correct weights by 2\n",
    "                for w in dict_of_predictions[correct_result]:\n",
    "                    weights[w]=weights[w]*2\n",
    "                    balance[w]+=1\n",
    "                    increase_count+=1\n",
    "                    \n",
    "                # decreasing incorrect weights by 2\n",
    "                for incorr_label in range(5):\n",
    "                    if incorr_label!=correct_result:\n",
    "                        for w in dict_of_predictions[incorr_label]:\n",
    "                                weights[w]=weights[w]/2\n",
    "                                balance[w]-=1\n",
    "                                reduce_count+=1\n",
    "        print(\"After Batch\",weights)\n",
    "        print(\"After Batch balance\",balance)\n",
    "        print(\"increased\",increase_count,\"Reduced\",reduce_count)\n",
    "    with open('weights/weights_no_noise.pkl', 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=winnow(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_majority(no_of_experts):\n",
    "    experts=[]\n",
    "    weights=[]\n",
    "    balance=[]\n",
    "    for i in range(no_of_experts):\n",
    "        model=torch.load(\"no_noise_expert_\"+str(i+1)+\".pt\")\n",
    "        model.eval()\n",
    "        experts.append(model)\n",
    "        weights.append(1)\n",
    "        balance.append(0)\n",
    "    increase_count=0\n",
    "    reduce_count=0\n",
    "    # winnow majority on all of datasets\n",
    "    train_loader=customDS.generate_batches_with_labels(0,len(df))\n",
    "    for inputs,labels in tqdm(train_loader):\n",
    "#         print(inputs.shape)\n",
    "        for i in range(len(inputs)):\n",
    "            predictions=[0,0,0,0,0]\n",
    "            dict_of_predictions={0:[],1:[],2:[],3:[],4:[]}\n",
    "            \n",
    "            # Gathering Predictions\n",
    "            for j in range(len(experts)):\n",
    "                model=experts[j]\n",
    "                input_copy = inputs[i].unsqueeze(0).clone()\n",
    "                output=model(input_copy.to(device).to(torch.float32))\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                predicted=int(predicted)\n",
    "                predictions[predicted]+=balance[j]\n",
    "                dict_of_predictions[predicted].append(j)\n",
    "\n",
    "            result_pred=predictions.index(max(predictions))\n",
    "            correct_result=int(labels[i])\n",
    "            # reducing/increasing weights only if prediction wrong\n",
    "            if result_pred!=correct_result:\n",
    "                # increasing Correct weights by 2\n",
    "                # for w in dict_of_predictions[correct_result]:\n",
    "                #     weights[w]=weights[w]*2\n",
    "                #     balance[w]+=1\n",
    "                #     increase_count+=1\n",
    "                    \n",
    "                # decreasing incorrect weights by 2\n",
    "                for incorr_label in range(5):\n",
    "                    if incorr_label!=correct_result:\n",
    "                        for w in dict_of_predictions[incorr_label]:\n",
    "                                weights[w]=weights[w]/2\n",
    "                                balance[w]-=1\n",
    "                                reduce_count+=1\n",
    "        print(\"After Batch\",weights)\n",
    "        print(\"After Batch balance\",balance)\n",
    "        print(\"increased\",increase_count,\"Reduced\",reduce_count)\n",
    "    with open('weights/weighted_majority_in_power_no_noise.pkl', 'wb') as f:\n",
    "        pickle.dump(balance, f)\n",
    "    return balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=weighted_majority(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
