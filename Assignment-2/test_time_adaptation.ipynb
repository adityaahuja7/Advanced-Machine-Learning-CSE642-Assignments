{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas \n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "training_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"USING DEVICE:\", training_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_noise=pd.read_csv(\"Data/cf_train.csv\")\n",
    "df_no_noise=pd.read_csv(\"Data/cf_train_no_noise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self,dataframe,batch_size,device =training_device,shuffle=False):\n",
    "        self.df=dataframe\n",
    "        self.batch_size=batch_size\n",
    "        self.columns_to_drop=['row_num','day','era','target_10_val','target_5_val','sigma','day_no']\n",
    "        self.X = self.df.drop(self.columns_to_drop, axis=1)\n",
    "        self.y=self.df['target_10_val']\n",
    "        self.device=device\n",
    "        self.shuffle=shuffle\n",
    "\n",
    "    def generate_batches_with_labels(self,idx):\n",
    "        data=self.X.iloc[:max(0,idx-10)]\n",
    "        labels=self.y.iloc[:max(0,idx-10)]\n",
    "        dataset =  torch.utils.data.TensorDataset(torch.tensor(data.values),torch.tensor(labels.values))\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "        data_unseen = self.X.iloc[max(0,idx-9):idx+1]\n",
    "        labels_unseen=self.y.iloc[max(0,idx-9):idx+1]\n",
    "        data_unseen,labels_unseen = torch.tensor(data_unseen.values).to(self.device),torch.tensor(labels_unseen.values).to(self.device)\n",
    "        return dataloader, (data_unseen,labels_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Adaptation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTimeModelMLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        classification_output_size,\n",
    "        shared_layers,\n",
    "        classification_layers,\n",
    "        auxiliary_layers,\n",
    "    ):\n",
    "        super(TestTimeModelMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        shared = []\n",
    "        for i in range(len(shared_layers)):\n",
    "            if i == 0:\n",
    "                shared.append(torch.nn.Linear(input_size, shared_layers[i]))\n",
    "                shared.append(torch.nn.ReLU())\n",
    "            elif i < len(shared_layers) - 1:\n",
    "                shared.append(torch.nn.Linear(shared_layers[i - 1], shared_layers[i]))\n",
    "                shared.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                shared.append(torch.nn.Linear(shared_layers[i - 1], shared_layers[i]))\n",
    "                shared.append(torch.nn.ReLU())\n",
    "                shared.append(torch.nn.BatchNorm1d(shared_layers[i]))\n",
    "\n",
    "        self.shared_layers = torch.nn.Sequential(*shared)\n",
    "\n",
    "        classification = []\n",
    "        for i in range(len(classification_layers)):\n",
    "            if i == 0 and len(classification_layers) == 1:\n",
    "                classification.append(torch.nn.Linear(shared_layers[-1], classification_output_size))\n",
    "            elif i == 0:\n",
    "                classification.append(\n",
    "                    torch.nn.Linear(shared_layers[-1], classification_layers[i])\n",
    "                )\n",
    "                classification.append(torch.nn.ReLU())\n",
    "            elif i < len(classification_layers) - 1:\n",
    "                classification.append(\n",
    "                    torch.nn.Linear(\n",
    "                        classification_layers[i - 1], classification_layers[i]\n",
    "                    )\n",
    "                )\n",
    "                classification.append(torch.nn.ReLU())\n",
    "                classification.append(torch.nn.BatchNorm1d(classification_layers[i]))\n",
    "            else:\n",
    "                classification.append(\n",
    "                    torch.nn.Linear(classification_layers[i - 1], classification_output_size)\n",
    "                )\n",
    "\n",
    "        self.classification_layers = torch.nn.Sequential(*classification)\n",
    "\n",
    "        auxiliary = []\n",
    "        for i in range(len(auxiliary_layers)):\n",
    "            if i == 0 and len(auxiliary_layers) == 1:\n",
    "                auxiliary.append(torch.nn.Linear(shared_layers[-1], self.input_size))\n",
    "            elif i == 0:\n",
    "                auxiliary.append(\n",
    "                    torch.nn.Linear(shared_layers[-1], auxiliary_layers[i])\n",
    "                )\n",
    "                auxiliary.append(torch.nn.ReLU())\n",
    "            elif i < len(auxiliary_layers) - 1:\n",
    "                auxiliary.append(\n",
    "                    torch.nn.Linear(auxiliary_layers[i - 1], auxiliary_layers[i])\n",
    "                )\n",
    "                auxiliary.append(torch.nn.ReLU())\n",
    "                auxiliary.append(torch.nn.BatchNorm1d(auxiliary_layers[i]))\n",
    "            else:\n",
    "                auxiliary.append(\n",
    "                    torch.nn.Linear(auxiliary_layers[i - 1], self.input_size)\n",
    "                )\n",
    "\n",
    "        self.auxiliary_layers = torch.nn.Sequential(*auxiliary)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layers(x)\n",
    "        classification_out = self.classification_layers(shared_out)\n",
    "        auxiliary_out = self.auxiliary_layers(shared_out)\n",
    "        return classification_out, auxiliary_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_model = TestTimeModelMLP(input_size=24, classification_output_size=5, shared_layers=[64, 64], classification_layers=[64,64], auxiliary_layers=[64,64])\n",
    "tta_model = tta_model.to(training_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestTimeModelMLP(\n",
       "  (shared_layers): Sequential(\n",
       "    (0): Linear(in_features=24, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classification_layers): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       "  (auxiliary_layers): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
