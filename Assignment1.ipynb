{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVH3nOPYBNw0"
      },
      "source": [
        "# Assignment-1\n",
        "## Team: Aditya Ahuja (2020275), Deeptanshu Barman Chowdhuri (2020293)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro-XJmUCBNw3"
      },
      "source": [
        "##  Imports & Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhU92hp7HylX",
        "outputId": "cd0680cc-507f-436d-dd67-490ef5a4dc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (1.24.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
            "[notice] To update, run: C:\\Users\\Deeptanshu Barman\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.1.4-py3-none-any.whl (2.0 MB)\n",
            "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "     ------------------- -------------------- 1.0/2.0 MB 20.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  2.0/2.0 MB 31.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.0/2.0 MB 25.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (0.11.1)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (2023.12.2)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (4.5.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (1.24.2)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n",
            "     ---------------------------------------- 0.0/778.1 kB ? eta -:--:--\n",
            "     ------------------------------------- 778.1/778.1 kB 24.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning) (23.0)\n",
            "Requirement already satisfied: requests in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.28.2)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl (365 kB)\n",
            "     ---------------------------------------- 0.0/365.2 kB ? eta -:--:--\n",
            "     ------------------------------------- 365.2/365.2 kB 11.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (65.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<6.0,>=4.57.0->lightning) (0.4.6)\n",
            "Collecting async-timeout<5.0,>=4.0\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
            "     ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
            "     ---------------------------------------- 50.4/50.4 kB 2.5 MB/s eta 0:00:00\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
            "     ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
            "     ---------------------------------------- 76.4/76.4 kB 4.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deeptanshu barman\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.0.1)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, pytorch-lightning, lightning\n",
            "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 lightning-2.1.4 lightning-utilities-0.10.1 multidict-6.0.5 pytorch-lightning-2.1.4 yarl-1.9.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
            "[notice] To update, run: C:\\Users\\Deeptanshu Barman\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchmetrics\n",
        "# !pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2NmATk9NBNw3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "import lightning as L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "spHzIt_dFTM6"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VXoJtlecBNw4"
      },
      "outputs": [],
      "source": [
        "Noise_0_data = requests.get(\n",
        "    \"http://AdityaAhuja01.pythonanywhere.com/data/df_syn_train_0_0_.csv\"\n",
        ")\n",
        "Noise_Low_data = requests.get(\n",
        "    \"http://AdityaAhuja01.pythonanywhere.com/data/df_synA_train_shuffled.csv\"\n",
        ")\n",
        "Noise_High_data = requests.get(\n",
        "    \"http://AdityaAhuja01.pythonanywhere.com/data/df_synA_test_hard_shuffled_sample.csv\"\n",
        ")\n",
        "\n",
        "if Noise_0_data.status_code == 200 and Noise_Low_data.status_code == 200 and Noise_High_data.status_code == 200:\n",
        "    datafolder = \"Data/Assignment1\"\n",
        "\n",
        "    if not os.path.exists(datafolder):\n",
        "        os.makedirs(datafolder)\n",
        "\n",
        "    with open(os.path.join(datafolder, \"data_0_noise\"), \"wb\") as f:\n",
        "        f.write(Noise_0_data.text.encode(\"utf-8\"))\n",
        "\n",
        "    with open(os.path.join(datafolder, \"data_Low_noise\"), \"wb\") as f:\n",
        "        f.write(Noise_Low_data.text.encode(\"utf-8\"))\n",
        "\n",
        "    with open(os.path.join(datafolder, \"data_High_noise\"), \"wb\") as f:\n",
        "        f.write(Noise_High_data.text.encode(\"utf-8\"))\n",
        "else:\n",
        "    print(\"Error in fetching data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sZCz-48ZBNw5"
      },
      "outputs": [],
      "source": [
        "Noise_0_dataframe = pd.read_csv(\"Data/Assignment1/data_0_noise\")\n",
        "Noise_Low_dataframe = pd.read_csv(\"Data/Assignment1/data_Low_noise\")\n",
        "Noise_High_dataframe = pd.read_csv(\"Data/Assignment1/data_High_noise\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKrqVPONBNw5",
        "outputId": "43ed3248-a536-464e-8ccd-ed9c9578dd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 4, 5, 7, 9, 10, 12, 15, 16, 18, 19, 21]\n"
          ]
        }
      ],
      "source": [
        "class_index = list(Noise_0_dataframe[\"era\"].unique())\n",
        "print(class_index)\n",
        "def encode(value, class_index = class_index):\n",
        "    return class_index.index(value)\n",
        "\n",
        "Noise_0_dataframe[\"era\"] = Noise_0_dataframe[\"era\"].apply(encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open_n_val</th>\n",
              "      <th>High_n_val</th>\n",
              "      <th>Low_n_val</th>\n",
              "      <th>Close_n_val</th>\n",
              "      <th>Volume_n_val</th>\n",
              "      <th>SMA_10_val</th>\n",
              "      <th>SMA_20_val</th>\n",
              "      <th>CMO_14_val</th>\n",
              "      <th>High_n-Low_n_val</th>\n",
              "      <th>Open_n-Close_n_val</th>\n",
              "      <th>...</th>\n",
              "      <th>Open_n-Close_n_changelen_val</th>\n",
              "      <th>SMA_20-SMA_10_changelen_val</th>\n",
              "      <th>Close_n_slope_3_changelen_val</th>\n",
              "      <th>Close_n_slope_5_changelen_val</th>\n",
              "      <th>Close_n_slope_10_changelen_val</th>\n",
              "      <th>row_num</th>\n",
              "      <th>day</th>\n",
              "      <th>era</th>\n",
              "      <th>target_10_val</th>\n",
              "      <th>target_5_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>75</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>76</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>77</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>78</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>79</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7795</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>135</td>\n",
              "      <td>555</td>\n",
              "      <td>11</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7796</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>136</td>\n",
              "      <td>555</td>\n",
              "      <td>11</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7797</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>137</td>\n",
              "      <td>555</td>\n",
              "      <td>11</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7798</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>138</td>\n",
              "      <td>555</td>\n",
              "      <td>11</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7799</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>139</td>\n",
              "      <td>555</td>\n",
              "      <td>11</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7800 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Open_n_val  High_n_val  Low_n_val  Close_n_val  Volume_n_val  \\\n",
              "0           0.50        0.50       0.50         0.50           0.0   \n",
              "1           0.50        0.50       0.50         0.50           0.0   \n",
              "2           0.50        0.50       0.50         0.50           0.0   \n",
              "3           0.50        0.50       0.50         0.50           0.0   \n",
              "4           0.50        0.50       0.50         0.50           0.0   \n",
              "...          ...         ...        ...          ...           ...   \n",
              "7795        0.25        0.25       0.25         0.25           0.0   \n",
              "7796        0.25        0.25       0.25         0.25           0.0   \n",
              "7797        0.25        0.25       0.25         0.25           0.0   \n",
              "7798        0.25        0.25       0.25         0.25           0.0   \n",
              "7799        0.25        0.25       0.25         0.25           0.0   \n",
              "\n",
              "      SMA_10_val  SMA_20_val  CMO_14_val  High_n-Low_n_val  \\\n",
              "0           0.50        0.25         1.0               1.0   \n",
              "1           0.50        0.50         1.0               1.0   \n",
              "2           0.50        0.50         1.0               1.0   \n",
              "3           0.50        0.50         1.0               1.0   \n",
              "4           0.50        0.50         1.0               1.0   \n",
              "...          ...         ...         ...               ...   \n",
              "7795        0.25        0.25         0.0               1.0   \n",
              "7796        0.25        0.25         0.0               1.0   \n",
              "7797        0.25        0.25         0.0               1.0   \n",
              "7798        0.25        0.25         0.0               1.0   \n",
              "7799        0.25        0.25         0.0               1.0   \n",
              "\n",
              "      Open_n-Close_n_val  ...  Open_n-Close_n_changelen_val  \\\n",
              "0                    0.5  ...                          0.50   \n",
              "1                    0.5  ...                          0.50   \n",
              "2                    0.5  ...                          0.50   \n",
              "3                    0.5  ...                          0.25   \n",
              "4                    0.5  ...                          0.75   \n",
              "...                  ...  ...                           ...   \n",
              "7795                 0.5  ...                          0.75   \n",
              "7796                 0.5  ...                          0.75   \n",
              "7797                 0.5  ...                          0.75   \n",
              "7798                 0.5  ...                          0.50   \n",
              "7799                 0.5  ...                          0.50   \n",
              "\n",
              "      SMA_20-SMA_10_changelen_val  Close_n_slope_3_changelen_val  \\\n",
              "0                            0.75                           0.25   \n",
              "1                            0.50                           0.75   \n",
              "2                            0.25                           0.50   \n",
              "3                            0.25                           0.50   \n",
              "4                            0.25                           0.50   \n",
              "...                           ...                            ...   \n",
              "7795                         0.75                           0.50   \n",
              "7796                         0.50                           0.50   \n",
              "7797                         0.50                           0.50   \n",
              "7798                         0.50                           0.75   \n",
              "7799                         0.75                           0.75   \n",
              "\n",
              "      Close_n_slope_5_changelen_val  Close_n_slope_10_changelen_val  row_num  \\\n",
              "0                              0.75                            0.50       75   \n",
              "1                              0.50                            0.50       76   \n",
              "2                              0.50                            0.75       77   \n",
              "3                              0.75                            0.50       78   \n",
              "4                              0.50                            0.50       79   \n",
              "...                             ...                             ...      ...   \n",
              "7795                           0.50                            0.50      135   \n",
              "7796                           0.75                            0.75      136   \n",
              "7797                           0.50                            0.50      137   \n",
              "7798                           0.50                            0.50      138   \n",
              "7799                           0.75                            0.75      139   \n",
              "\n",
              "      day  era  target_10_val  target_5_val  \n",
              "0     537    0           0.75          0.75  \n",
              "1     537    0           0.75          0.75  \n",
              "2     537    0           0.75          0.75  \n",
              "3     537    0           0.75          0.75  \n",
              "4     537    0           0.75          0.75  \n",
              "...   ...  ...            ...           ...  \n",
              "7795  555   11           0.25          0.25  \n",
              "7796  555   11           0.25          0.25  \n",
              "7797  555   11           0.25          0.25  \n",
              "7798  555   11           0.25          0.25  \n",
              "7799  555   11           0.25          0.25  \n",
              "\n",
              "[7800 rows x 29 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Noise_0_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Jfvo6jBNw5"
      },
      "source": [
        "## Setting up Dataset & Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VYmSaMKKBNw6"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, noise, transform=None, target_transform=None,drop = None):\n",
        "        self.dataframe = dataframe\n",
        "        if drop != None:\n",
        "            self.X = dataframe.drop(drop, axis=1).values\n",
        "        else:\n",
        "            self.X = dataframe.values\n",
        "        self.y = dataframe[\"era\"].values\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.noise = noise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item, label = self.X[idx], self.y[idx]\n",
        "        return item, label\n",
        "\n",
        "    def get_noise(self):\n",
        "        return self.noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iUzwd5RYBNw6"
      },
      "outputs": [],
      "source": [
        "#Setting up datasets\n",
        "Noise_0_dataset = CustomDataset(Noise_0_dataframe, \"0\",drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\"])\n",
        "# Noise_Low_dataset = CustomDataset(Noise_Low_dataframe, \"Low\", drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\"])\n",
        "# Noise_High_dataset = CustomDataset(Noise_High_dataframe, \"High\", drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\"])\n",
        "Noise_0_train, Noise_0_test = random_split(Noise_0_dataset, [int(0.8 * len(Noise_0_dataset)), len(Noise_0_dataset) - int(0.8 * len(Noise_0_dataset))])\n",
        "# Noise_Low_train, Noise_Low_test = random_split(Noise_Low_dataset, [int(0.8 * len(Noise_Low_dataset)), len(Noise_Low_dataset) - int(0.8 * len(Noise_Low_dataset))])\n",
        "# Noise_High_train, Noise_High_test = random_split(Noise_High_dataset, [int(0.8 * len(Noise_High_dataset)), len(Noise_High_dataset) - int(0.8 * len(Noise_High_dataset))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q2QH7ZDlBNw6"
      },
      "outputs": [],
      "source": [
        "#Setting up dataloaders\n",
        "Noise_0_train_loader = DataLoader(Noise_0_train, batch_size=64, shuffle=True)\n",
        "Noise_0_test_loader = DataLoader(Noise_0_test, batch_size=64, shuffle=True)\n",
        "# Noise_Low_train_loader = DataLoader(Noise_Low_train, batch_size=64, shuffle=True)\n",
        "# Noise_Low_test_loader = DataLoader(Noise_Low_test, batch_size=64, shuffle=True)\n",
        "# Noise_High_train_loader = DataLoader(Noise_High_train, batch_size=64, shuffle=True)\n",
        "# Noise_High_test_loader = DataLoader(Noise_High_test, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "g8bv2SOZBNw7",
        "outputId": "7b81681d-d0cb-49ac-8878-4f6d8bbc18da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open_n_val</th>\n",
              "      <th>High_n_val</th>\n",
              "      <th>Low_n_val</th>\n",
              "      <th>Close_n_val</th>\n",
              "      <th>Volume_n_val</th>\n",
              "      <th>SMA_10_val</th>\n",
              "      <th>SMA_20_val</th>\n",
              "      <th>CMO_14_val</th>\n",
              "      <th>High_n-Low_n_val</th>\n",
              "      <th>Open_n-Close_n_val</th>\n",
              "      <th>...</th>\n",
              "      <th>Open_n-Close_n_changelen_val</th>\n",
              "      <th>SMA_20-SMA_10_changelen_val</th>\n",
              "      <th>Close_n_slope_3_changelen_val</th>\n",
              "      <th>Close_n_slope_5_changelen_val</th>\n",
              "      <th>Close_n_slope_10_changelen_val</th>\n",
              "      <th>row_num</th>\n",
              "      <th>day</th>\n",
              "      <th>era</th>\n",
              "      <th>target_10_val</th>\n",
              "      <th>target_5_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>75</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>76</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>77</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>78</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>79</td>\n",
              "      <td>537</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Open_n_val  High_n_val  Low_n_val  Close_n_val  Volume_n_val  SMA_10_val  \\\n",
              "0         0.5         0.5        0.5          0.5           0.0         0.5   \n",
              "1         0.5         0.5        0.5          0.5           0.0         0.5   \n",
              "2         0.5         0.5        0.5          0.5           0.0         0.5   \n",
              "3         0.5         0.5        0.5          0.5           0.0         0.5   \n",
              "4         0.5         0.5        0.5          0.5           0.0         0.5   \n",
              "\n",
              "   SMA_20_val  CMO_14_val  High_n-Low_n_val  Open_n-Close_n_val  ...  \\\n",
              "0        0.25         1.0               1.0                 0.5  ...   \n",
              "1        0.50         1.0               1.0                 0.5  ...   \n",
              "2        0.50         1.0               1.0                 0.5  ...   \n",
              "3        0.50         1.0               1.0                 0.5  ...   \n",
              "4        0.50         1.0               1.0                 0.5  ...   \n",
              "\n",
              "   Open_n-Close_n_changelen_val  SMA_20-SMA_10_changelen_val  \\\n",
              "0                          0.50                         0.75   \n",
              "1                          0.50                         0.50   \n",
              "2                          0.50                         0.25   \n",
              "3                          0.25                         0.25   \n",
              "4                          0.75                         0.25   \n",
              "\n",
              "   Close_n_slope_3_changelen_val  Close_n_slope_5_changelen_val  \\\n",
              "0                           0.25                           0.75   \n",
              "1                           0.75                           0.50   \n",
              "2                           0.50                           0.50   \n",
              "3                           0.50                           0.75   \n",
              "4                           0.50                           0.50   \n",
              "\n",
              "   Close_n_slope_10_changelen_val  row_num  day  era  target_10_val  \\\n",
              "0                            0.50       75  537    0           0.75   \n",
              "1                            0.50       76  537    0           0.75   \n",
              "2                            0.75       77  537    0           0.75   \n",
              "3                            0.50       78  537    0           0.75   \n",
              "4                            0.50       79  537    0           0.75   \n",
              "\n",
              "   target_5_val  \n",
              "0          0.75  \n",
              "1          0.75  \n",
              "2          0.75  \n",
              "3          0.75  \n",
              "4          0.75  \n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Noise_0_dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWFHHoWmBNw7"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zxOjBg4cBNw7"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Zdu0tjhABNw7"
      },
      "outputs": [],
      "source": [
        "class MyMLP(torch.nn.Module):\n",
        "    def __init__ (self, modules):\n",
        "        super().__init__()\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        MyModuleList = torch.nn.ModuleList([m for m in modules])\n",
        "        self.layers = torch.nn.Sequential(*MyModuleList)\n",
        "        self.softmax = torch.nn.Softmax(dim = 1)\n",
        "\n",
        "\n",
        "    def forward(self, X: torch.Tensor):\n",
        "        if (X.shape[1] != 24):\n",
        "            raise ValueError(\"Input shape must be (batch_size, 24)\")\n",
        "        X = X.to(device)\n",
        "        X = self.layers(X)\n",
        "        X = self.softmax(X)\n",
        "\n",
        "        return X\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFmCKdxHBNw7",
        "outputId": "bb93b7c0-3639-43a1-fd1f-cfe4a5931986"
      },
      "outputs": [],
      "source": [
        "Model = MyMLP([torch.nn.Linear(24,64),torch.nn.ReLU(),torch.nn.Linear(64,128),torch.nn.ReLU(),torch.nn.Linear(128,64),torch.nn.ReLU(),torch.nn.Linear(64,12)])\n",
        "Model = Model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10,verbose=True):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.long)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_train_loss / len(train_loader)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.long)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = running_val_loss / len(val_loader)\n",
        "        val_accuracy = correct_val / total_val\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train')\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train')\n",
        "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/4000], Train Loss: 2.4820, Train Acc: 0.1287, Val Loss: 2.4647, Val Acc: 0.1987\n",
            "Epoch [2/4000], Train Loss: 2.3376, Train Acc: 0.2769, Val Loss: 2.2524, Val Acc: 0.3314\n",
            "Epoch [3/4000], Train Loss: 2.1926, Train Acc: 0.4575, Val Loss: 2.1189, Val Acc: 0.5468\n",
            "Epoch [4/4000], Train Loss: 2.0728, Train Acc: 0.5838, Val Loss: 2.0497, Val Acc: 0.6071\n",
            "Epoch [5/4000], Train Loss: 2.0145, Train Acc: 0.6369, Val Loss: 1.9872, Val Acc: 0.6776\n",
            "Epoch [6/4000], Train Loss: 1.9737, Train Acc: 0.6800, Val Loss: 1.9639, Val Acc: 0.6974\n",
            "Epoch [7/4000], Train Loss: 1.9516, Train Acc: 0.7032, Val Loss: 1.9436, Val Acc: 0.7077\n",
            "Epoch [8/4000], Train Loss: 1.9321, Train Acc: 0.7216, Val Loss: 1.9235, Val Acc: 0.7327\n",
            "Epoch [9/4000], Train Loss: 1.9177, Train Acc: 0.7401, Val Loss: 1.9052, Val Acc: 0.7609\n",
            "Epoch [10/4000], Train Loss: 1.9073, Train Acc: 0.7420, Val Loss: 1.8964, Val Acc: 0.7513\n",
            "Epoch [11/4000], Train Loss: 1.8969, Train Acc: 0.7551, Val Loss: 1.8966, Val Acc: 0.7487\n",
            "Epoch [12/4000], Train Loss: 1.8952, Train Acc: 0.7529, Val Loss: 1.8874, Val Acc: 0.7647\n",
            "Epoch [13/4000], Train Loss: 1.8904, Train Acc: 0.7567, Val Loss: 1.8892, Val Acc: 0.7609\n",
            "Epoch [14/4000], Train Loss: 1.8838, Train Acc: 0.7623, Val Loss: 1.8761, Val Acc: 0.7654\n",
            "Epoch [15/4000], Train Loss: 1.8794, Train Acc: 0.7623, Val Loss: 1.8693, Val Acc: 0.7724\n",
            "Epoch [16/4000], Train Loss: 1.8751, Train Acc: 0.7739, Val Loss: 1.8594, Val Acc: 0.7949\n",
            "Epoch [17/4000], Train Loss: 1.8663, Train Acc: 0.7787, Val Loss: 1.8568, Val Acc: 0.7949\n",
            "Epoch [18/4000], Train Loss: 1.8635, Train Acc: 0.7809, Val Loss: 1.8467, Val Acc: 0.8077\n",
            "Epoch [19/4000], Train Loss: 1.8543, Train Acc: 0.7885, Val Loss: 1.8498, Val Acc: 0.7994\n",
            "Epoch [20/4000], Train Loss: 1.8494, Train Acc: 0.7920, Val Loss: 1.8500, Val Acc: 0.8103\n",
            "Epoch [21/4000], Train Loss: 1.8454, Train Acc: 0.7992, Val Loss: 1.8406, Val Acc: 0.7974\n",
            "Epoch [22/4000], Train Loss: 1.8416, Train Acc: 0.8032, Val Loss: 1.8310, Val Acc: 0.8237\n",
            "Epoch [23/4000], Train Loss: 1.8356, Train Acc: 0.8135, Val Loss: 1.8285, Val Acc: 0.8122\n",
            "Epoch [24/4000], Train Loss: 1.8323, Train Acc: 0.8130, Val Loss: 1.8272, Val Acc: 0.8135\n",
            "Epoch [25/4000], Train Loss: 1.8307, Train Acc: 0.8167, Val Loss: 1.8223, Val Acc: 0.8301\n",
            "Epoch [26/4000], Train Loss: 1.8325, Train Acc: 0.8104, Val Loss: 1.8347, Val Acc: 0.8058\n",
            "Epoch [27/4000], Train Loss: 1.8318, Train Acc: 0.8103, Val Loss: 1.8164, Val Acc: 0.8301\n",
            "Epoch [28/4000], Train Loss: 1.8283, Train Acc: 0.8154, Val Loss: 1.8333, Val Acc: 0.8115\n",
            "Epoch [29/4000], Train Loss: 1.8258, Train Acc: 0.8171, Val Loss: 1.8123, Val Acc: 0.8327\n",
            "Epoch [30/4000], Train Loss: 1.8214, Train Acc: 0.8223, Val Loss: 1.8476, Val Acc: 0.7885\n",
            "Epoch [31/4000], Train Loss: 1.8202, Train Acc: 0.8208, Val Loss: 1.8186, Val Acc: 0.8218\n",
            "Epoch [32/4000], Train Loss: 1.8159, Train Acc: 0.8300, Val Loss: 1.8081, Val Acc: 0.8417\n",
            "Epoch [33/4000], Train Loss: 1.8146, Train Acc: 0.8285, Val Loss: 1.8160, Val Acc: 0.8282\n",
            "Epoch [34/4000], Train Loss: 1.8130, Train Acc: 0.8316, Val Loss: 1.8163, Val Acc: 0.8250\n",
            "Epoch [35/4000], Train Loss: 1.8098, Train Acc: 0.8341, Val Loss: 1.8038, Val Acc: 0.8378\n",
            "Epoch [36/4000], Train Loss: 1.8089, Train Acc: 0.8348, Val Loss: 1.8084, Val Acc: 0.8372\n",
            "Epoch [37/4000], Train Loss: 1.8063, Train Acc: 0.8385, Val Loss: 1.8043, Val Acc: 0.8417\n",
            "Epoch [38/4000], Train Loss: 1.8051, Train Acc: 0.8415, Val Loss: 1.8035, Val Acc: 0.8372\n",
            "Epoch [39/4000], Train Loss: 1.8001, Train Acc: 0.8447, Val Loss: 1.8009, Val Acc: 0.8429\n",
            "Epoch [40/4000], Train Loss: 1.8011, Train Acc: 0.8421, Val Loss: 1.8036, Val Acc: 0.8417\n",
            "Epoch [41/4000], Train Loss: 1.7982, Train Acc: 0.8436, Val Loss: 1.7968, Val Acc: 0.8442\n",
            "Epoch [42/4000], Train Loss: 1.8015, Train Acc: 0.8405, Val Loss: 1.8025, Val Acc: 0.8442\n",
            "Epoch [43/4000], Train Loss: 1.8026, Train Acc: 0.8413, Val Loss: 1.8053, Val Acc: 0.8417\n",
            "Epoch [44/4000], Train Loss: 1.8030, Train Acc: 0.8389, Val Loss: 1.8066, Val Acc: 0.8288\n",
            "Epoch [45/4000], Train Loss: 1.7992, Train Acc: 0.8441, Val Loss: 1.7992, Val Acc: 0.8462\n",
            "Epoch [46/4000], Train Loss: 1.8040, Train Acc: 0.8378, Val Loss: 1.8039, Val Acc: 0.8372\n",
            "Epoch [47/4000], Train Loss: 1.7987, Train Acc: 0.8444, Val Loss: 1.8060, Val Acc: 0.8410\n",
            "Epoch [48/4000], Train Loss: 1.7961, Train Acc: 0.8458, Val Loss: 1.8026, Val Acc: 0.8301\n",
            "Epoch [49/4000], Train Loss: 1.7959, Train Acc: 0.8465, Val Loss: 1.7959, Val Acc: 0.8442\n",
            "Epoch [50/4000], Train Loss: 1.7949, Train Acc: 0.8462, Val Loss: 1.7926, Val Acc: 0.8474\n",
            "Epoch [51/4000], Train Loss: 1.7917, Train Acc: 0.8521, Val Loss: 1.8054, Val Acc: 0.8449\n",
            "Epoch [52/4000], Train Loss: 1.7964, Train Acc: 0.8457, Val Loss: 1.7927, Val Acc: 0.8494\n",
            "Epoch [53/4000], Train Loss: 1.7941, Train Acc: 0.8460, Val Loss: 1.7939, Val Acc: 0.8442\n",
            "Epoch [54/4000], Train Loss: 1.7898, Train Acc: 0.8506, Val Loss: 1.7941, Val Acc: 0.8506\n",
            "Epoch [55/4000], Train Loss: 1.7913, Train Acc: 0.8516, Val Loss: 1.7883, Val Acc: 0.8538\n",
            "Epoch [56/4000], Train Loss: 1.7895, Train Acc: 0.8522, Val Loss: 1.7940, Val Acc: 0.8519\n",
            "Epoch [57/4000], Train Loss: 1.7913, Train Acc: 0.8478, Val Loss: 1.7914, Val Acc: 0.8494\n",
            "Epoch [58/4000], Train Loss: 1.7890, Train Acc: 0.8516, Val Loss: 1.7859, Val Acc: 0.8558\n",
            "Epoch [59/4000], Train Loss: 1.7850, Train Acc: 0.8572, Val Loss: 1.7903, Val Acc: 0.8500\n",
            "Epoch [60/4000], Train Loss: 1.7865, Train Acc: 0.8537, Val Loss: 1.7926, Val Acc: 0.8500\n",
            "Epoch [61/4000], Train Loss: 1.7972, Train Acc: 0.8441, Val Loss: 1.7903, Val Acc: 0.8468\n",
            "Epoch [62/4000], Train Loss: 1.7877, Train Acc: 0.8538, Val Loss: 1.8000, Val Acc: 0.8468\n",
            "Epoch [63/4000], Train Loss: 1.7858, Train Acc: 0.8543, Val Loss: 1.7994, Val Acc: 0.8359\n",
            "Epoch [64/4000], Train Loss: 1.7849, Train Acc: 0.8566, Val Loss: 1.7834, Val Acc: 0.8564\n",
            "Epoch [65/4000], Train Loss: 1.7834, Train Acc: 0.8572, Val Loss: 1.7856, Val Acc: 0.8571\n",
            "Epoch [66/4000], Train Loss: 1.7809, Train Acc: 0.8607, Val Loss: 1.7897, Val Acc: 0.8481\n",
            "Epoch [67/4000], Train Loss: 1.7818, Train Acc: 0.8606, Val Loss: 1.7853, Val Acc: 0.8526\n",
            "Epoch [68/4000], Train Loss: 1.7843, Train Acc: 0.8559, Val Loss: 1.7824, Val Acc: 0.8506\n",
            "Epoch [69/4000], Train Loss: 1.7912, Train Acc: 0.8490, Val Loss: 1.7992, Val Acc: 0.8449\n",
            "Epoch [70/4000], Train Loss: 1.7835, Train Acc: 0.8579, Val Loss: 1.7825, Val Acc: 0.8590\n",
            "Epoch [71/4000], Train Loss: 1.7794, Train Acc: 0.8614, Val Loss: 1.7806, Val Acc: 0.8596\n",
            "Epoch [72/4000], Train Loss: 1.7779, Train Acc: 0.8628, Val Loss: 1.7908, Val Acc: 0.8442\n",
            "Epoch [73/4000], Train Loss: 1.7803, Train Acc: 0.8619, Val Loss: 1.7851, Val Acc: 0.8571\n",
            "Epoch [74/4000], Train Loss: 1.7775, Train Acc: 0.8630, Val Loss: 1.7847, Val Acc: 0.8583\n",
            "Epoch [75/4000], Train Loss: 1.7766, Train Acc: 0.8654, Val Loss: 1.7808, Val Acc: 0.8622\n",
            "Epoch [76/4000], Train Loss: 1.7751, Train Acc: 0.8659, Val Loss: 1.7844, Val Acc: 0.8622\n",
            "Epoch [77/4000], Train Loss: 1.7753, Train Acc: 0.8651, Val Loss: 1.7746, Val Acc: 0.8673\n",
            "Epoch [78/4000], Train Loss: 1.7752, Train Acc: 0.8676, Val Loss: 1.7843, Val Acc: 0.8558\n",
            "Epoch [79/4000], Train Loss: 1.7777, Train Acc: 0.8627, Val Loss: 1.7885, Val Acc: 0.8538\n",
            "Epoch [80/4000], Train Loss: 1.7761, Train Acc: 0.8665, Val Loss: 1.7789, Val Acc: 0.8583\n",
            "Epoch [81/4000], Train Loss: 1.7739, Train Acc: 0.8670, Val Loss: 1.7791, Val Acc: 0.8635\n",
            "Epoch [82/4000], Train Loss: 1.7756, Train Acc: 0.8654, Val Loss: 1.7749, Val Acc: 0.8622\n",
            "Epoch [83/4000], Train Loss: 1.7753, Train Acc: 0.8662, Val Loss: 1.7811, Val Acc: 0.8628\n",
            "Epoch [84/4000], Train Loss: 1.7752, Train Acc: 0.8655, Val Loss: 1.7781, Val Acc: 0.8583\n",
            "Epoch [85/4000], Train Loss: 1.7724, Train Acc: 0.8670, Val Loss: 1.7785, Val Acc: 0.8641\n",
            "Epoch [86/4000], Train Loss: 1.7728, Train Acc: 0.8699, Val Loss: 1.7765, Val Acc: 0.8667\n",
            "Epoch [87/4000], Train Loss: 1.7694, Train Acc: 0.8724, Val Loss: 1.7723, Val Acc: 0.8654\n",
            "Epoch [88/4000], Train Loss: 1.7710, Train Acc: 0.8681, Val Loss: 1.7712, Val Acc: 0.8641\n",
            "Epoch [89/4000], Train Loss: 1.7711, Train Acc: 0.8689, Val Loss: 1.7710, Val Acc: 0.8641\n",
            "Epoch [90/4000], Train Loss: 1.7705, Train Acc: 0.8734, Val Loss: 1.7836, Val Acc: 0.8494\n",
            "Epoch [91/4000], Train Loss: 1.7737, Train Acc: 0.8655, Val Loss: 1.7740, Val Acc: 0.8609\n",
            "Epoch [92/4000], Train Loss: 1.7663, Train Acc: 0.8761, Val Loss: 1.7720, Val Acc: 0.8654\n",
            "Epoch [93/4000], Train Loss: 1.7666, Train Acc: 0.8758, Val Loss: 1.7724, Val Acc: 0.8654\n",
            "Epoch [94/4000], Train Loss: 1.7653, Train Acc: 0.8744, Val Loss: 1.7734, Val Acc: 0.8679\n",
            "Epoch [95/4000], Train Loss: 1.7698, Train Acc: 0.8696, Val Loss: 1.7680, Val Acc: 0.8712\n",
            "Epoch [96/4000], Train Loss: 1.7653, Train Acc: 0.8768, Val Loss: 1.7734, Val Acc: 0.8660\n",
            "Epoch [97/4000], Train Loss: 1.7655, Train Acc: 0.8747, Val Loss: 1.7642, Val Acc: 0.8737\n",
            "Epoch [98/4000], Train Loss: 1.7649, Train Acc: 0.8771, Val Loss: 1.7671, Val Acc: 0.8724\n",
            "Epoch [99/4000], Train Loss: 1.7676, Train Acc: 0.8748, Val Loss: 1.7651, Val Acc: 0.8724\n",
            "Epoch [100/4000], Train Loss: 1.7629, Train Acc: 0.8790, Val Loss: 1.7749, Val Acc: 0.8609\n",
            "Epoch [101/4000], Train Loss: 1.7625, Train Acc: 0.8790, Val Loss: 1.7672, Val Acc: 0.8750\n",
            "Epoch [102/4000], Train Loss: 1.7608, Train Acc: 0.8811, Val Loss: 1.7646, Val Acc: 0.8776\n",
            "Epoch [103/4000], Train Loss: 1.7603, Train Acc: 0.8812, Val Loss: 1.7616, Val Acc: 0.8776\n",
            "Epoch [104/4000], Train Loss: 1.7603, Train Acc: 0.8821, Val Loss: 1.7608, Val Acc: 0.8808\n",
            "Epoch [105/4000], Train Loss: 1.7609, Train Acc: 0.8808, Val Loss: 1.7702, Val Acc: 0.8763\n",
            "Epoch [106/4000], Train Loss: 1.7616, Train Acc: 0.8824, Val Loss: 1.7602, Val Acc: 0.8840\n",
            "Epoch [107/4000], Train Loss: 1.7602, Train Acc: 0.8803, Val Loss: 1.7601, Val Acc: 0.8846\n",
            "Epoch [108/4000], Train Loss: 1.7541, Train Acc: 0.8881, Val Loss: 1.7608, Val Acc: 0.8808\n",
            "Epoch [109/4000], Train Loss: 1.7559, Train Acc: 0.8864, Val Loss: 1.7678, Val Acc: 0.8718\n",
            "Epoch [110/4000], Train Loss: 1.7588, Train Acc: 0.8829, Val Loss: 1.7570, Val Acc: 0.8814\n",
            "Epoch [111/4000], Train Loss: 1.7584, Train Acc: 0.8830, Val Loss: 1.7788, Val Acc: 0.8699\n",
            "Epoch [112/4000], Train Loss: 1.7566, Train Acc: 0.8869, Val Loss: 1.7689, Val Acc: 0.8705\n",
            "Epoch [113/4000], Train Loss: 1.7571, Train Acc: 0.8837, Val Loss: 1.7659, Val Acc: 0.8718\n",
            "Epoch [114/4000], Train Loss: 1.7543, Train Acc: 0.8857, Val Loss: 1.7638, Val Acc: 0.8763\n",
            "Epoch [115/4000], Train Loss: 1.7534, Train Acc: 0.8872, Val Loss: 1.7588, Val Acc: 0.8814\n",
            "Epoch [116/4000], Train Loss: 1.7543, Train Acc: 0.8857, Val Loss: 1.7550, Val Acc: 0.8853\n",
            "Epoch [117/4000], Train Loss: 1.7539, Train Acc: 0.8864, Val Loss: 1.7580, Val Acc: 0.8859\n",
            "Epoch [118/4000], Train Loss: 1.7556, Train Acc: 0.8864, Val Loss: 1.7602, Val Acc: 0.8756\n",
            "Epoch [119/4000], Train Loss: 1.7534, Train Acc: 0.8851, Val Loss: 1.7569, Val Acc: 0.8821\n",
            "Epoch [120/4000], Train Loss: 1.7504, Train Acc: 0.8894, Val Loss: 1.7528, Val Acc: 0.8897\n",
            "Epoch [121/4000], Train Loss: 1.7533, Train Acc: 0.8867, Val Loss: 1.7601, Val Acc: 0.8801\n",
            "Epoch [122/4000], Train Loss: 1.7515, Train Acc: 0.8904, Val Loss: 1.7605, Val Acc: 0.8821\n",
            "Epoch [123/4000], Train Loss: 1.7535, Train Acc: 0.8891, Val Loss: 1.7547, Val Acc: 0.8878\n",
            "Epoch [124/4000], Train Loss: 1.7531, Train Acc: 0.8893, Val Loss: 1.7760, Val Acc: 0.8641\n",
            "Epoch [125/4000], Train Loss: 1.7514, Train Acc: 0.8905, Val Loss: 1.7530, Val Acc: 0.8846\n",
            "Epoch [126/4000], Train Loss: 1.7490, Train Acc: 0.8923, Val Loss: 1.7500, Val Acc: 0.8833\n",
            "Epoch [127/4000], Train Loss: 1.7473, Train Acc: 0.8925, Val Loss: 1.7563, Val Acc: 0.8821\n",
            "Epoch [128/4000], Train Loss: 1.7530, Train Acc: 0.8872, Val Loss: 1.7599, Val Acc: 0.8731\n",
            "Epoch [129/4000], Train Loss: 1.7490, Train Acc: 0.8933, Val Loss: 1.7537, Val Acc: 0.8853\n",
            "Epoch [130/4000], Train Loss: 1.7487, Train Acc: 0.8933, Val Loss: 1.7522, Val Acc: 0.8865\n",
            "Epoch [131/4000], Train Loss: 1.7463, Train Acc: 0.8949, Val Loss: 1.7575, Val Acc: 0.8827\n",
            "Epoch [132/4000], Train Loss: 1.7474, Train Acc: 0.8936, Val Loss: 1.7471, Val Acc: 0.8910\n",
            "Epoch [133/4000], Train Loss: 1.7499, Train Acc: 0.8915, Val Loss: 1.7488, Val Acc: 0.8917\n",
            "Epoch [134/4000], Train Loss: 1.7466, Train Acc: 0.8933, Val Loss: 1.7527, Val Acc: 0.8859\n",
            "Epoch [135/4000], Train Loss: 1.7468, Train Acc: 0.8942, Val Loss: 1.7501, Val Acc: 0.8910\n",
            "Epoch [136/4000], Train Loss: 1.7466, Train Acc: 0.8936, Val Loss: 1.7505, Val Acc: 0.8891\n",
            "Epoch [137/4000], Train Loss: 1.7535, Train Acc: 0.8883, Val Loss: 1.7630, Val Acc: 0.8808\n",
            "Epoch [138/4000], Train Loss: 1.7473, Train Acc: 0.8955, Val Loss: 1.7487, Val Acc: 0.8897\n",
            "Epoch [139/4000], Train Loss: 1.7483, Train Acc: 0.8936, Val Loss: 1.7440, Val Acc: 0.8949\n",
            "Epoch [140/4000], Train Loss: 1.7455, Train Acc: 0.8973, Val Loss: 1.7604, Val Acc: 0.8827\n",
            "Epoch [141/4000], Train Loss: 1.7507, Train Acc: 0.8867, Val Loss: 1.7458, Val Acc: 0.8878\n",
            "Epoch [142/4000], Train Loss: 1.7479, Train Acc: 0.8934, Val Loss: 1.7586, Val Acc: 0.8782\n",
            "Epoch [143/4000], Train Loss: 1.7480, Train Acc: 0.8913, Val Loss: 1.7474, Val Acc: 0.8878\n",
            "Epoch [144/4000], Train Loss: 1.7470, Train Acc: 0.8949, Val Loss: 1.7537, Val Acc: 0.8821\n",
            "Epoch [145/4000], Train Loss: 1.7475, Train Acc: 0.8936, Val Loss: 1.7457, Val Acc: 0.8962\n",
            "Epoch [146/4000], Train Loss: 1.7466, Train Acc: 0.8944, Val Loss: 1.7565, Val Acc: 0.8788\n",
            "Epoch [147/4000], Train Loss: 1.7452, Train Acc: 0.8973, Val Loss: 1.7521, Val Acc: 0.8808\n",
            "Epoch [148/4000], Train Loss: 1.7466, Train Acc: 0.8931, Val Loss: 1.7444, Val Acc: 0.8955\n",
            "Epoch [149/4000], Train Loss: 1.7449, Train Acc: 0.8973, Val Loss: 1.7416, Val Acc: 0.8974\n",
            "Epoch [150/4000], Train Loss: 1.7443, Train Acc: 0.8978, Val Loss: 1.7588, Val Acc: 0.8795\n",
            "Epoch [151/4000], Train Loss: 1.7432, Train Acc: 0.8970, Val Loss: 1.7457, Val Acc: 0.8929\n",
            "Epoch [152/4000], Train Loss: 1.7437, Train Acc: 0.8952, Val Loss: 1.7478, Val Acc: 0.8929\n",
            "Epoch [153/4000], Train Loss: 1.7438, Train Acc: 0.8989, Val Loss: 1.7570, Val Acc: 0.8808\n",
            "Epoch [154/4000], Train Loss: 1.7439, Train Acc: 0.8954, Val Loss: 1.7519, Val Acc: 0.8923\n",
            "Epoch [155/4000], Train Loss: 1.7415, Train Acc: 0.8981, Val Loss: 1.7463, Val Acc: 0.8962\n",
            "Epoch [156/4000], Train Loss: 1.7472, Train Acc: 0.8942, Val Loss: 1.7460, Val Acc: 0.8968\n",
            "Epoch [157/4000], Train Loss: 1.7413, Train Acc: 0.9000, Val Loss: 1.7424, Val Acc: 0.8929\n",
            "Epoch [158/4000], Train Loss: 1.7486, Train Acc: 0.8920, Val Loss: 1.7509, Val Acc: 0.8872\n",
            "Epoch [159/4000], Train Loss: 1.7454, Train Acc: 0.8963, Val Loss: 1.7473, Val Acc: 0.8987\n",
            "Epoch [160/4000], Train Loss: 1.7418, Train Acc: 0.8981, Val Loss: 1.7418, Val Acc: 0.9000\n",
            "Epoch [161/4000], Train Loss: 1.7403, Train Acc: 0.9021, Val Loss: 1.7459, Val Acc: 0.8974\n",
            "Epoch [162/4000], Train Loss: 1.7455, Train Acc: 0.8928, Val Loss: 1.7440, Val Acc: 0.8949\n",
            "Epoch [163/4000], Train Loss: 1.7414, Train Acc: 0.8989, Val Loss: 1.7432, Val Acc: 0.8974\n",
            "Epoch [164/4000], Train Loss: 1.7466, Train Acc: 0.8954, Val Loss: 1.7681, Val Acc: 0.8833\n",
            "Epoch [165/4000], Train Loss: 1.7416, Train Acc: 0.8995, Val Loss: 1.7409, Val Acc: 0.8987\n",
            "Epoch [166/4000], Train Loss: 1.7436, Train Acc: 0.8968, Val Loss: 1.7440, Val Acc: 0.8974\n",
            "Epoch [167/4000], Train Loss: 1.7435, Train Acc: 0.8958, Val Loss: 1.7409, Val Acc: 0.8968\n",
            "Epoch [168/4000], Train Loss: 1.7421, Train Acc: 0.8987, Val Loss: 1.7513, Val Acc: 0.8891\n",
            "Epoch [169/4000], Train Loss: 1.7387, Train Acc: 0.9010, Val Loss: 1.7480, Val Acc: 0.8968\n",
            "Epoch [170/4000], Train Loss: 1.7413, Train Acc: 0.8995, Val Loss: 1.7520, Val Acc: 0.8910\n",
            "Epoch [171/4000], Train Loss: 1.7410, Train Acc: 0.8981, Val Loss: 1.7473, Val Acc: 0.8962\n",
            "Epoch [172/4000], Train Loss: 1.7399, Train Acc: 0.9008, Val Loss: 1.7387, Val Acc: 0.9026\n",
            "Epoch [173/4000], Train Loss: 1.7423, Train Acc: 0.8998, Val Loss: 1.7428, Val Acc: 0.8968\n",
            "Epoch [174/4000], Train Loss: 1.7440, Train Acc: 0.8971, Val Loss: 1.7463, Val Acc: 0.8955\n",
            "Epoch [175/4000], Train Loss: 1.7399, Train Acc: 0.9006, Val Loss: 1.7438, Val Acc: 0.8987\n",
            "Epoch [176/4000], Train Loss: 1.7395, Train Acc: 0.9014, Val Loss: 1.7448, Val Acc: 0.8955\n",
            "Epoch [177/4000], Train Loss: 1.7405, Train Acc: 0.8997, Val Loss: 1.7440, Val Acc: 0.8942\n",
            "Epoch [178/4000], Train Loss: 1.7396, Train Acc: 0.9016, Val Loss: 1.7411, Val Acc: 0.8981\n",
            "Epoch [179/4000], Train Loss: 1.7404, Train Acc: 0.9008, Val Loss: 1.7488, Val Acc: 0.8904\n",
            "Epoch [180/4000], Train Loss: 1.7411, Train Acc: 0.8987, Val Loss: 1.7466, Val Acc: 0.8955\n",
            "Epoch [181/4000], Train Loss: 1.7406, Train Acc: 0.8995, Val Loss: 1.7430, Val Acc: 0.8994\n",
            "Epoch [182/4000], Train Loss: 1.7373, Train Acc: 0.9027, Val Loss: 1.7448, Val Acc: 0.8987\n",
            "Epoch [183/4000], Train Loss: 1.7454, Train Acc: 0.8962, Val Loss: 1.7387, Val Acc: 0.9026\n",
            "Epoch [184/4000], Train Loss: 1.7374, Train Acc: 0.9032, Val Loss: 1.7423, Val Acc: 0.8955\n",
            "Epoch [185/4000], Train Loss: 1.7395, Train Acc: 0.9006, Val Loss: 1.7410, Val Acc: 0.9000\n",
            "Epoch [186/4000], Train Loss: 1.7388, Train Acc: 0.9021, Val Loss: 1.7401, Val Acc: 0.8987\n",
            "Epoch [187/4000], Train Loss: 1.7375, Train Acc: 0.9038, Val Loss: 1.7430, Val Acc: 0.8974\n",
            "Epoch [188/4000], Train Loss: 1.7365, Train Acc: 0.9048, Val Loss: 1.7409, Val Acc: 0.9013\n",
            "Epoch [189/4000], Train Loss: 1.7397, Train Acc: 0.9003, Val Loss: 1.7427, Val Acc: 0.8942\n",
            "Epoch [190/4000], Train Loss: 1.7381, Train Acc: 0.9011, Val Loss: 1.7411, Val Acc: 0.8955\n",
            "Epoch [191/4000], Train Loss: 1.7424, Train Acc: 0.8979, Val Loss: 1.7390, Val Acc: 0.9045\n",
            "Epoch [192/4000], Train Loss: 1.7378, Train Acc: 0.9018, Val Loss: 1.7473, Val Acc: 0.8987\n",
            "Epoch [193/4000], Train Loss: 1.7373, Train Acc: 0.9054, Val Loss: 1.7394, Val Acc: 0.8994\n",
            "Epoch [194/4000], Train Loss: 1.7378, Train Acc: 0.9014, Val Loss: 1.7400, Val Acc: 0.9045\n",
            "Epoch [195/4000], Train Loss: 1.7403, Train Acc: 0.9010, Val Loss: 1.7412, Val Acc: 0.8962\n",
            "Epoch [196/4000], Train Loss: 1.7373, Train Acc: 0.9029, Val Loss: 1.7458, Val Acc: 0.8872\n",
            "Epoch [197/4000], Train Loss: 1.7382, Train Acc: 0.9027, Val Loss: 1.7402, Val Acc: 0.8994\n",
            "Epoch [198/4000], Train Loss: 1.7388, Train Acc: 0.9003, Val Loss: 1.7504, Val Acc: 0.8897\n",
            "Epoch [199/4000], Train Loss: 1.7386, Train Acc: 0.9013, Val Loss: 1.7484, Val Acc: 0.8981\n",
            "Epoch [200/4000], Train Loss: 1.7401, Train Acc: 0.9002, Val Loss: 1.7406, Val Acc: 0.9045\n",
            "Epoch [201/4000], Train Loss: 1.7379, Train Acc: 0.9014, Val Loss: 1.7402, Val Acc: 0.9019\n",
            "Epoch [202/4000], Train Loss: 1.7391, Train Acc: 0.8997, Val Loss: 1.7379, Val Acc: 0.8981\n",
            "Epoch [203/4000], Train Loss: 1.7375, Train Acc: 0.9026, Val Loss: 1.7401, Val Acc: 0.9032\n",
            "Epoch [204/4000], Train Loss: 1.7369, Train Acc: 0.9051, Val Loss: 1.7380, Val Acc: 0.9026\n",
            "Epoch [205/4000], Train Loss: 1.7352, Train Acc: 0.9053, Val Loss: 1.7389, Val Acc: 0.9000\n",
            "Epoch [206/4000], Train Loss: 1.7403, Train Acc: 0.8992, Val Loss: 1.7414, Val Acc: 0.8949\n",
            "Epoch [207/4000], Train Loss: 1.7369, Train Acc: 0.9043, Val Loss: 1.7378, Val Acc: 0.9038\n",
            "Epoch [208/4000], Train Loss: 1.7392, Train Acc: 0.9021, Val Loss: 1.7357, Val Acc: 0.8987\n",
            "Epoch [209/4000], Train Loss: 1.7363, Train Acc: 0.9034, Val Loss: 1.7370, Val Acc: 0.9006\n",
            "Epoch [210/4000], Train Loss: 1.7382, Train Acc: 0.9040, Val Loss: 1.7327, Val Acc: 0.9064\n",
            "Epoch [211/4000], Train Loss: 1.7378, Train Acc: 0.9043, Val Loss: 1.7389, Val Acc: 0.9013\n",
            "Epoch [212/4000], Train Loss: 1.7355, Train Acc: 0.9038, Val Loss: 1.7406, Val Acc: 0.8968\n",
            "Epoch [213/4000], Train Loss: 1.7373, Train Acc: 0.9046, Val Loss: 1.7380, Val Acc: 0.8987\n",
            "Epoch [214/4000], Train Loss: 1.7349, Train Acc: 0.9050, Val Loss: 1.7421, Val Acc: 0.9026\n",
            "Epoch [215/4000], Train Loss: 1.7378, Train Acc: 0.9040, Val Loss: 1.7418, Val Acc: 0.9000\n",
            "Epoch [216/4000], Train Loss: 1.7350, Train Acc: 0.9045, Val Loss: 1.7417, Val Acc: 0.8974\n",
            "Epoch [217/4000], Train Loss: 1.7361, Train Acc: 0.9034, Val Loss: 1.7454, Val Acc: 0.8923\n",
            "Epoch [218/4000], Train Loss: 1.7369, Train Acc: 0.9038, Val Loss: 1.7400, Val Acc: 0.9006\n",
            "Epoch [219/4000], Train Loss: 1.7369, Train Acc: 0.9038, Val Loss: 1.7490, Val Acc: 0.8897\n",
            "Epoch [220/4000], Train Loss: 1.7366, Train Acc: 0.9040, Val Loss: 1.7335, Val Acc: 0.9038\n",
            "Epoch [221/4000], Train Loss: 1.7368, Train Acc: 0.9035, Val Loss: 1.7344, Val Acc: 0.9026\n",
            "Epoch [222/4000], Train Loss: 1.7387, Train Acc: 0.9030, Val Loss: 1.7567, Val Acc: 0.8833\n",
            "Epoch [223/4000], Train Loss: 1.7339, Train Acc: 0.9045, Val Loss: 1.7434, Val Acc: 0.8968\n",
            "Epoch [224/4000], Train Loss: 1.7373, Train Acc: 0.9024, Val Loss: 1.7357, Val Acc: 0.9032\n",
            "Epoch [225/4000], Train Loss: 1.7341, Train Acc: 0.9050, Val Loss: 1.7328, Val Acc: 0.9058\n",
            "Epoch [226/4000], Train Loss: 1.7355, Train Acc: 0.9035, Val Loss: 1.7354, Val Acc: 0.9038\n",
            "Epoch [227/4000], Train Loss: 1.7370, Train Acc: 0.9042, Val Loss: 1.7355, Val Acc: 0.9090\n",
            "Epoch [228/4000], Train Loss: 1.7346, Train Acc: 0.9053, Val Loss: 1.7316, Val Acc: 0.9083\n",
            "Epoch [229/4000], Train Loss: 1.7322, Train Acc: 0.9080, Val Loss: 1.7368, Val Acc: 0.9006\n",
            "Epoch [230/4000], Train Loss: 1.7322, Train Acc: 0.9082, Val Loss: 1.7378, Val Acc: 0.9006\n",
            "Epoch [231/4000], Train Loss: 1.7309, Train Acc: 0.9107, Val Loss: 1.7341, Val Acc: 0.9051\n",
            "Epoch [232/4000], Train Loss: 1.7335, Train Acc: 0.9087, Val Loss: 1.7464, Val Acc: 0.8955\n",
            "Epoch [233/4000], Train Loss: 1.7359, Train Acc: 0.9030, Val Loss: 1.7446, Val Acc: 0.8949\n",
            "Epoch [234/4000], Train Loss: 1.7336, Train Acc: 0.9067, Val Loss: 1.7304, Val Acc: 0.9058\n",
            "Epoch [235/4000], Train Loss: 1.7323, Train Acc: 0.9072, Val Loss: 1.7380, Val Acc: 0.9038\n",
            "Epoch [236/4000], Train Loss: 1.7347, Train Acc: 0.9069, Val Loss: 1.7391, Val Acc: 0.9064\n",
            "Epoch [237/4000], Train Loss: 1.7344, Train Acc: 0.9054, Val Loss: 1.7516, Val Acc: 0.9000\n",
            "Epoch [238/4000], Train Loss: 1.7362, Train Acc: 0.9059, Val Loss: 1.7390, Val Acc: 0.9032\n",
            "Epoch [239/4000], Train Loss: 1.7357, Train Acc: 0.9037, Val Loss: 1.7326, Val Acc: 0.9083\n",
            "Epoch [240/4000], Train Loss: 1.7347, Train Acc: 0.9045, Val Loss: 1.7313, Val Acc: 0.9128\n",
            "Epoch [241/4000], Train Loss: 1.7331, Train Acc: 0.9072, Val Loss: 1.7392, Val Acc: 0.8987\n",
            "Epoch [242/4000], Train Loss: 1.7387, Train Acc: 0.8997, Val Loss: 1.7339, Val Acc: 0.9038\n",
            "Epoch [243/4000], Train Loss: 1.7334, Train Acc: 0.9072, Val Loss: 1.7551, Val Acc: 0.8833\n",
            "Epoch [244/4000], Train Loss: 1.7364, Train Acc: 0.9051, Val Loss: 1.7276, Val Acc: 0.9103\n",
            "Epoch [245/4000], Train Loss: 1.7330, Train Acc: 0.9080, Val Loss: 1.7298, Val Acc: 0.9103\n",
            "Epoch [246/4000], Train Loss: 1.7308, Train Acc: 0.9109, Val Loss: 1.7350, Val Acc: 0.9090\n",
            "Epoch [247/4000], Train Loss: 1.7318, Train Acc: 0.9082, Val Loss: 1.7351, Val Acc: 0.9038\n",
            "Epoch [248/4000], Train Loss: 1.7338, Train Acc: 0.9071, Val Loss: 1.7565, Val Acc: 0.8814\n",
            "Epoch [249/4000], Train Loss: 1.7331, Train Acc: 0.9074, Val Loss: 1.7296, Val Acc: 0.9077\n",
            "Epoch [250/4000], Train Loss: 1.7345, Train Acc: 0.9058, Val Loss: 1.7445, Val Acc: 0.9000\n",
            "Epoch [251/4000], Train Loss: 1.7311, Train Acc: 0.9103, Val Loss: 1.7296, Val Acc: 0.9115\n",
            "Epoch [252/4000], Train Loss: 1.7342, Train Acc: 0.9059, Val Loss: 1.7357, Val Acc: 0.9077\n",
            "Epoch [253/4000], Train Loss: 1.7307, Train Acc: 0.9093, Val Loss: 1.7473, Val Acc: 0.8904\n",
            "Epoch [254/4000], Train Loss: 1.7305, Train Acc: 0.9104, Val Loss: 1.7308, Val Acc: 0.9103\n",
            "Epoch [255/4000], Train Loss: 1.7324, Train Acc: 0.9093, Val Loss: 1.7337, Val Acc: 0.9077\n",
            "Epoch [256/4000], Train Loss: 1.7317, Train Acc: 0.9090, Val Loss: 1.7349, Val Acc: 0.9045\n",
            "Epoch [257/4000], Train Loss: 1.7321, Train Acc: 0.9103, Val Loss: 1.7321, Val Acc: 0.9045\n",
            "Epoch [258/4000], Train Loss: 1.7315, Train Acc: 0.9088, Val Loss: 1.7296, Val Acc: 0.9071\n",
            "Epoch [259/4000], Train Loss: 1.7305, Train Acc: 0.9104, Val Loss: 1.7327, Val Acc: 0.9071\n",
            "Epoch [260/4000], Train Loss: 1.7317, Train Acc: 0.9101, Val Loss: 1.7330, Val Acc: 0.9006\n",
            "Epoch [261/4000], Train Loss: 1.7325, Train Acc: 0.9064, Val Loss: 1.7327, Val Acc: 0.9090\n",
            "Epoch [262/4000], Train Loss: 1.7312, Train Acc: 0.9103, Val Loss: 1.7363, Val Acc: 0.9032\n",
            "Epoch [263/4000], Train Loss: 1.7330, Train Acc: 0.9088, Val Loss: 1.7311, Val Acc: 0.9032\n",
            "Epoch [264/4000], Train Loss: 1.7316, Train Acc: 0.9098, Val Loss: 1.7346, Val Acc: 0.9071\n",
            "Epoch [265/4000], Train Loss: 1.7365, Train Acc: 0.9040, Val Loss: 1.7354, Val Acc: 0.9090\n",
            "Epoch [266/4000], Train Loss: 1.7316, Train Acc: 0.9087, Val Loss: 1.7316, Val Acc: 0.9058\n",
            "Epoch [267/4000], Train Loss: 1.7319, Train Acc: 0.9079, Val Loss: 1.7329, Val Acc: 0.9064\n",
            "Epoch [268/4000], Train Loss: 1.7311, Train Acc: 0.9072, Val Loss: 1.7326, Val Acc: 0.9090\n",
            "Epoch [269/4000], Train Loss: 1.7316, Train Acc: 0.9091, Val Loss: 1.7379, Val Acc: 0.9109\n",
            "Epoch [270/4000], Train Loss: 1.7296, Train Acc: 0.9103, Val Loss: 1.7355, Val Acc: 0.9077\n",
            "Epoch [271/4000], Train Loss: 1.7332, Train Acc: 0.9069, Val Loss: 1.7410, Val Acc: 0.9000\n",
            "Epoch [272/4000], Train Loss: 1.7326, Train Acc: 0.9069, Val Loss: 1.7313, Val Acc: 0.9090\n",
            "Epoch [273/4000], Train Loss: 1.7319, Train Acc: 0.9093, Val Loss: 1.7404, Val Acc: 0.9026\n",
            "Epoch [274/4000], Train Loss: 1.7309, Train Acc: 0.9088, Val Loss: 1.7365, Val Acc: 0.9019\n",
            "Epoch [275/4000], Train Loss: 1.7308, Train Acc: 0.9098, Val Loss: 1.7335, Val Acc: 0.9032\n",
            "Epoch [276/4000], Train Loss: 1.7289, Train Acc: 0.9103, Val Loss: 1.7299, Val Acc: 0.9096\n",
            "Epoch [277/4000], Train Loss: 1.7312, Train Acc: 0.9082, Val Loss: 1.7356, Val Acc: 0.9045\n",
            "Epoch [278/4000], Train Loss: 1.7332, Train Acc: 0.9074, Val Loss: 1.7426, Val Acc: 0.8968\n",
            "Epoch [279/4000], Train Loss: 1.7321, Train Acc: 0.9080, Val Loss: 1.7353, Val Acc: 0.8962\n",
            "Epoch [280/4000], Train Loss: 1.7342, Train Acc: 0.9064, Val Loss: 1.7308, Val Acc: 0.9096\n",
            "Epoch [281/4000], Train Loss: 1.7318, Train Acc: 0.9080, Val Loss: 1.7294, Val Acc: 0.9103\n",
            "Epoch [282/4000], Train Loss: 1.7313, Train Acc: 0.9095, Val Loss: 1.7264, Val Acc: 0.9160\n",
            "Epoch [283/4000], Train Loss: 1.7291, Train Acc: 0.9099, Val Loss: 1.7386, Val Acc: 0.9019\n",
            "Epoch [284/4000], Train Loss: 1.7305, Train Acc: 0.9077, Val Loss: 1.7529, Val Acc: 0.8756\n",
            "Epoch [285/4000], Train Loss: 1.7304, Train Acc: 0.9098, Val Loss: 1.7283, Val Acc: 0.9103\n",
            "Epoch [286/4000], Train Loss: 1.7296, Train Acc: 0.9101, Val Loss: 1.7362, Val Acc: 0.9013\n",
            "Epoch [287/4000], Train Loss: 1.7307, Train Acc: 0.9080, Val Loss: 1.7326, Val Acc: 0.9064\n",
            "Epoch [288/4000], Train Loss: 1.7285, Train Acc: 0.9120, Val Loss: 1.7290, Val Acc: 0.9135\n",
            "Epoch [289/4000], Train Loss: 1.7336, Train Acc: 0.9066, Val Loss: 1.7375, Val Acc: 0.9045\n",
            "Epoch [290/4000], Train Loss: 1.7299, Train Acc: 0.9101, Val Loss: 1.7316, Val Acc: 0.9051\n",
            "Epoch [291/4000], Train Loss: 1.7323, Train Acc: 0.9075, Val Loss: 1.7248, Val Acc: 0.9128\n",
            "Epoch [292/4000], Train Loss: 1.7299, Train Acc: 0.9109, Val Loss: 1.7375, Val Acc: 0.9103\n",
            "Epoch [293/4000], Train Loss: 1.7285, Train Acc: 0.9101, Val Loss: 1.7275, Val Acc: 0.9096\n",
            "Epoch [294/4000], Train Loss: 1.7376, Train Acc: 0.9027, Val Loss: 1.7332, Val Acc: 0.9096\n",
            "Epoch [295/4000], Train Loss: 1.7282, Train Acc: 0.9119, Val Loss: 1.7339, Val Acc: 0.9032\n",
            "Epoch [296/4000], Train Loss: 1.7286, Train Acc: 0.9111, Val Loss: 1.7303, Val Acc: 0.9083\n",
            "Epoch [297/4000], Train Loss: 1.7319, Train Acc: 0.9101, Val Loss: 1.7430, Val Acc: 0.9013\n",
            "Epoch [298/4000], Train Loss: 1.7295, Train Acc: 0.9091, Val Loss: 1.7328, Val Acc: 0.9083\n",
            "Epoch [299/4000], Train Loss: 1.7335, Train Acc: 0.9053, Val Loss: 1.7345, Val Acc: 0.9096\n",
            "Epoch [300/4000], Train Loss: 1.7324, Train Acc: 0.9075, Val Loss: 1.7308, Val Acc: 0.9109\n",
            "Epoch [301/4000], Train Loss: 1.7292, Train Acc: 0.9109, Val Loss: 1.7314, Val Acc: 0.9115\n",
            "Epoch [302/4000], Train Loss: 1.7284, Train Acc: 0.9107, Val Loss: 1.7374, Val Acc: 0.8974\n",
            "Epoch [303/4000], Train Loss: 1.7289, Train Acc: 0.9101, Val Loss: 1.7302, Val Acc: 0.9103\n",
            "Epoch [304/4000], Train Loss: 1.7305, Train Acc: 0.9083, Val Loss: 1.7272, Val Acc: 0.9147\n",
            "Epoch [305/4000], Train Loss: 1.7284, Train Acc: 0.9109, Val Loss: 1.7353, Val Acc: 0.9032\n",
            "Epoch [306/4000], Train Loss: 1.7296, Train Acc: 0.9104, Val Loss: 1.7294, Val Acc: 0.9122\n",
            "Epoch [307/4000], Train Loss: 1.7296, Train Acc: 0.9122, Val Loss: 1.7260, Val Acc: 0.9083\n",
            "Epoch [308/4000], Train Loss: 1.7285, Train Acc: 0.9104, Val Loss: 1.7324, Val Acc: 0.9064\n",
            "Epoch [309/4000], Train Loss: 1.7286, Train Acc: 0.9111, Val Loss: 1.7275, Val Acc: 0.9109\n",
            "Epoch [310/4000], Train Loss: 1.7283, Train Acc: 0.9138, Val Loss: 1.7288, Val Acc: 0.9122\n",
            "Epoch [311/4000], Train Loss: 1.7307, Train Acc: 0.9093, Val Loss: 1.7447, Val Acc: 0.8962\n",
            "Epoch [312/4000], Train Loss: 1.7337, Train Acc: 0.9087, Val Loss: 1.7287, Val Acc: 0.9122\n",
            "Epoch [313/4000], Train Loss: 1.7289, Train Acc: 0.9122, Val Loss: 1.7292, Val Acc: 0.9128\n",
            "Epoch [314/4000], Train Loss: 1.7280, Train Acc: 0.9106, Val Loss: 1.7517, Val Acc: 0.8865\n",
            "Epoch [315/4000], Train Loss: 1.7298, Train Acc: 0.9098, Val Loss: 1.7255, Val Acc: 0.9122\n",
            "Epoch [316/4000], Train Loss: 1.7255, Train Acc: 0.9133, Val Loss: 1.7365, Val Acc: 0.9064\n",
            "Epoch [317/4000], Train Loss: 1.7287, Train Acc: 0.9117, Val Loss: 1.7251, Val Acc: 0.9160\n",
            "Epoch [318/4000], Train Loss: 1.7283, Train Acc: 0.9117, Val Loss: 1.7271, Val Acc: 0.9115\n",
            "Epoch [319/4000], Train Loss: 1.7288, Train Acc: 0.9123, Val Loss: 1.7291, Val Acc: 0.9090\n",
            "Epoch [320/4000], Train Loss: 1.7308, Train Acc: 0.9109, Val Loss: 1.7273, Val Acc: 0.9128\n",
            "Epoch [321/4000], Train Loss: 1.7270, Train Acc: 0.9122, Val Loss: 1.7294, Val Acc: 0.9128\n",
            "Epoch [322/4000], Train Loss: 1.7270, Train Acc: 0.9128, Val Loss: 1.7346, Val Acc: 0.9083\n",
            "Epoch [323/4000], Train Loss: 1.7270, Train Acc: 0.9143, Val Loss: 1.7234, Val Acc: 0.9154\n",
            "Epoch [324/4000], Train Loss: 1.7310, Train Acc: 0.9096, Val Loss: 1.7290, Val Acc: 0.9122\n",
            "Epoch [325/4000], Train Loss: 1.7273, Train Acc: 0.9119, Val Loss: 1.7276, Val Acc: 0.9109\n",
            "Epoch [326/4000], Train Loss: 1.7269, Train Acc: 0.9136, Val Loss: 1.7408, Val Acc: 0.9064\n",
            "Epoch [327/4000], Train Loss: 1.7287, Train Acc: 0.9114, Val Loss: 1.7336, Val Acc: 0.9006\n",
            "Epoch [328/4000], Train Loss: 1.7283, Train Acc: 0.9117, Val Loss: 1.7237, Val Acc: 0.9141\n",
            "Epoch [329/4000], Train Loss: 1.7269, Train Acc: 0.9120, Val Loss: 1.7283, Val Acc: 0.9128\n",
            "Epoch [330/4000], Train Loss: 1.7288, Train Acc: 0.9125, Val Loss: 1.7407, Val Acc: 0.9077\n",
            "Epoch [331/4000], Train Loss: 1.7288, Train Acc: 0.9122, Val Loss: 1.7231, Val Acc: 0.9160\n",
            "Epoch [332/4000], Train Loss: 1.7252, Train Acc: 0.9147, Val Loss: 1.7325, Val Acc: 0.9013\n",
            "Epoch [333/4000], Train Loss: 1.7279, Train Acc: 0.9128, Val Loss: 1.7335, Val Acc: 0.9058\n",
            "Epoch [334/4000], Train Loss: 1.7262, Train Acc: 0.9138, Val Loss: 1.7304, Val Acc: 0.9077\n",
            "Epoch [335/4000], Train Loss: 1.7263, Train Acc: 0.9123, Val Loss: 1.7325, Val Acc: 0.9045\n",
            "Epoch [336/4000], Train Loss: 1.7284, Train Acc: 0.9157, Val Loss: 1.7280, Val Acc: 0.9167\n",
            "Epoch [337/4000], Train Loss: 1.7279, Train Acc: 0.9112, Val Loss: 1.7298, Val Acc: 0.9109\n",
            "Epoch [338/4000], Train Loss: 1.7280, Train Acc: 0.9107, Val Loss: 1.7291, Val Acc: 0.9109\n",
            "Epoch [339/4000], Train Loss: 1.7268, Train Acc: 0.9131, Val Loss: 1.7329, Val Acc: 0.9051\n",
            "Epoch [340/4000], Train Loss: 1.7284, Train Acc: 0.9103, Val Loss: 1.7329, Val Acc: 0.9090\n",
            "Epoch [341/4000], Train Loss: 1.7286, Train Acc: 0.9122, Val Loss: 1.7243, Val Acc: 0.9147\n",
            "Epoch [342/4000], Train Loss: 1.7271, Train Acc: 0.9135, Val Loss: 1.7300, Val Acc: 0.9160\n",
            "Epoch [343/4000], Train Loss: 1.7318, Train Acc: 0.9103, Val Loss: 1.7247, Val Acc: 0.9128\n",
            "Epoch [344/4000], Train Loss: 1.7276, Train Acc: 0.9122, Val Loss: 1.7272, Val Acc: 0.9135\n",
            "Epoch [345/4000], Train Loss: 1.7303, Train Acc: 0.9106, Val Loss: 1.7285, Val Acc: 0.9103\n",
            "Epoch [346/4000], Train Loss: 1.7317, Train Acc: 0.9085, Val Loss: 1.7296, Val Acc: 0.9141\n",
            "Epoch [347/4000], Train Loss: 1.7270, Train Acc: 0.9128, Val Loss: 1.7269, Val Acc: 0.9141\n",
            "Epoch [348/4000], Train Loss: 1.7270, Train Acc: 0.9125, Val Loss: 1.7259, Val Acc: 0.9141\n",
            "Epoch [349/4000], Train Loss: 1.7276, Train Acc: 0.9117, Val Loss: 1.7290, Val Acc: 0.9141\n",
            "Epoch [350/4000], Train Loss: 1.7246, Train Acc: 0.9151, Val Loss: 1.7278, Val Acc: 0.9154\n",
            "Epoch [351/4000], Train Loss: 1.7260, Train Acc: 0.9128, Val Loss: 1.7234, Val Acc: 0.9147\n",
            "Epoch [352/4000], Train Loss: 1.7329, Train Acc: 0.9079, Val Loss: 1.7273, Val Acc: 0.9128\n",
            "Epoch [353/4000], Train Loss: 1.7258, Train Acc: 0.9139, Val Loss: 1.7307, Val Acc: 0.9077\n",
            "Epoch [354/4000], Train Loss: 1.7285, Train Acc: 0.9107, Val Loss: 1.7294, Val Acc: 0.9135\n",
            "Epoch [355/4000], Train Loss: 1.7276, Train Acc: 0.9111, Val Loss: 1.7264, Val Acc: 0.9109\n",
            "Epoch [356/4000], Train Loss: 1.7260, Train Acc: 0.9136, Val Loss: 1.7243, Val Acc: 0.9115\n",
            "Epoch [357/4000], Train Loss: 1.7277, Train Acc: 0.9119, Val Loss: 1.7255, Val Acc: 0.9141\n",
            "Epoch [358/4000], Train Loss: 1.7254, Train Acc: 0.9131, Val Loss: 1.7248, Val Acc: 0.9128\n",
            "Epoch [359/4000], Train Loss: 1.7262, Train Acc: 0.9128, Val Loss: 1.7259, Val Acc: 0.9115\n",
            "Epoch [360/4000], Train Loss: 1.7293, Train Acc: 0.9107, Val Loss: 1.7322, Val Acc: 0.9077\n",
            "Epoch [361/4000], Train Loss: 1.7281, Train Acc: 0.9120, Val Loss: 1.7347, Val Acc: 0.9077\n",
            "Epoch [362/4000], Train Loss: 1.7267, Train Acc: 0.9115, Val Loss: 1.7272, Val Acc: 0.9071\n",
            "Epoch [363/4000], Train Loss: 1.7267, Train Acc: 0.9114, Val Loss: 1.7286, Val Acc: 0.9147\n",
            "Epoch [364/4000], Train Loss: 1.7253, Train Acc: 0.9147, Val Loss: 1.7388, Val Acc: 0.9026\n",
            "Epoch [365/4000], Train Loss: 1.7312, Train Acc: 0.9083, Val Loss: 1.7316, Val Acc: 0.9090\n",
            "Epoch [366/4000], Train Loss: 1.7287, Train Acc: 0.9106, Val Loss: 1.7317, Val Acc: 0.9115\n",
            "Epoch [367/4000], Train Loss: 1.7250, Train Acc: 0.9143, Val Loss: 1.7275, Val Acc: 0.9103\n",
            "Epoch [368/4000], Train Loss: 1.7270, Train Acc: 0.9130, Val Loss: 1.7378, Val Acc: 0.9071\n",
            "Epoch [369/4000], Train Loss: 1.7282, Train Acc: 0.9104, Val Loss: 1.7260, Val Acc: 0.9141\n",
            "Epoch [370/4000], Train Loss: 1.7262, Train Acc: 0.9136, Val Loss: 1.7286, Val Acc: 0.9147\n",
            "Epoch [371/4000], Train Loss: 1.7252, Train Acc: 0.9138, Val Loss: 1.7219, Val Acc: 0.9160\n",
            "Epoch [372/4000], Train Loss: 1.7279, Train Acc: 0.9112, Val Loss: 1.7248, Val Acc: 0.9141\n",
            "Epoch [373/4000], Train Loss: 1.7257, Train Acc: 0.9143, Val Loss: 1.7266, Val Acc: 0.9167\n",
            "Epoch [374/4000], Train Loss: 1.7257, Train Acc: 0.9146, Val Loss: 1.7393, Val Acc: 0.9026\n",
            "Epoch [375/4000], Train Loss: 1.7296, Train Acc: 0.9114, Val Loss: 1.7276, Val Acc: 0.9109\n",
            "Epoch [376/4000], Train Loss: 1.7248, Train Acc: 0.9146, Val Loss: 1.7280, Val Acc: 0.9122\n",
            "Epoch [377/4000], Train Loss: 1.7272, Train Acc: 0.9120, Val Loss: 1.7292, Val Acc: 0.9147\n",
            "Epoch [378/4000], Train Loss: 1.7253, Train Acc: 0.9141, Val Loss: 1.7254, Val Acc: 0.9135\n",
            "Epoch [379/4000], Train Loss: 1.7268, Train Acc: 0.9143, Val Loss: 1.7256, Val Acc: 0.9115\n",
            "Epoch [380/4000], Train Loss: 1.7289, Train Acc: 0.9120, Val Loss: 1.7277, Val Acc: 0.9128\n",
            "Epoch [381/4000], Train Loss: 1.7255, Train Acc: 0.9135, Val Loss: 1.7259, Val Acc: 0.9167\n",
            "Epoch [382/4000], Train Loss: 1.7297, Train Acc: 0.9096, Val Loss: 1.7319, Val Acc: 0.9122\n",
            "Epoch [383/4000], Train Loss: 1.7264, Train Acc: 0.9114, Val Loss: 1.7242, Val Acc: 0.9167\n",
            "Epoch [384/4000], Train Loss: 1.7246, Train Acc: 0.9141, Val Loss: 1.7212, Val Acc: 0.9128\n",
            "Epoch [385/4000], Train Loss: 1.7254, Train Acc: 0.9146, Val Loss: 1.7285, Val Acc: 0.9115\n",
            "Epoch [386/4000], Train Loss: 1.7253, Train Acc: 0.9146, Val Loss: 1.7284, Val Acc: 0.9103\n",
            "Epoch [387/4000], Train Loss: 1.7258, Train Acc: 0.9131, Val Loss: 1.7249, Val Acc: 0.9135\n",
            "Epoch [388/4000], Train Loss: 1.7262, Train Acc: 0.9144, Val Loss: 1.7308, Val Acc: 0.9115\n",
            "Epoch [389/4000], Train Loss: 1.7268, Train Acc: 0.9133, Val Loss: 1.7306, Val Acc: 0.9096\n",
            "Epoch [390/4000], Train Loss: 1.7241, Train Acc: 0.9144, Val Loss: 1.7224, Val Acc: 0.9147\n",
            "Epoch [391/4000], Train Loss: 1.7263, Train Acc: 0.9157, Val Loss: 1.7252, Val Acc: 0.9167\n",
            "Epoch [392/4000], Train Loss: 1.7245, Train Acc: 0.9143, Val Loss: 1.7255, Val Acc: 0.9160\n",
            "Epoch [393/4000], Train Loss: 1.7278, Train Acc: 0.9112, Val Loss: 1.7241, Val Acc: 0.9160\n",
            "Epoch [394/4000], Train Loss: 1.7269, Train Acc: 0.9122, Val Loss: 1.7393, Val Acc: 0.8968\n",
            "Epoch [395/4000], Train Loss: 1.7332, Train Acc: 0.9069, Val Loss: 1.7248, Val Acc: 0.9160\n",
            "Epoch [396/4000], Train Loss: 1.7272, Train Acc: 0.9130, Val Loss: 1.7227, Val Acc: 0.9135\n",
            "Epoch [397/4000], Train Loss: 1.7242, Train Acc: 0.9160, Val Loss: 1.7333, Val Acc: 0.9077\n",
            "Epoch [398/4000], Train Loss: 1.7280, Train Acc: 0.9120, Val Loss: 1.7294, Val Acc: 0.9109\n",
            "Epoch [399/4000], Train Loss: 1.7266, Train Acc: 0.9131, Val Loss: 1.7250, Val Acc: 0.9160\n",
            "Epoch [400/4000], Train Loss: 1.7257, Train Acc: 0.9128, Val Loss: 1.7327, Val Acc: 0.9051\n",
            "Epoch [401/4000], Train Loss: 1.7262, Train Acc: 0.9138, Val Loss: 1.7251, Val Acc: 0.9122\n",
            "Epoch [402/4000], Train Loss: 1.7246, Train Acc: 0.9144, Val Loss: 1.7284, Val Acc: 0.9103\n",
            "Epoch [403/4000], Train Loss: 1.7248, Train Acc: 0.9157, Val Loss: 1.7341, Val Acc: 0.9071\n",
            "Epoch [404/4000], Train Loss: 1.7299, Train Acc: 0.9095, Val Loss: 1.7325, Val Acc: 0.9109\n",
            "Epoch [405/4000], Train Loss: 1.7267, Train Acc: 0.9125, Val Loss: 1.7224, Val Acc: 0.9122\n",
            "Epoch [406/4000], Train Loss: 1.7222, Train Acc: 0.9157, Val Loss: 1.7225, Val Acc: 0.9173\n",
            "Epoch [407/4000], Train Loss: 1.7258, Train Acc: 0.9136, Val Loss: 1.7278, Val Acc: 0.9083\n",
            "Epoch [408/4000], Train Loss: 1.7248, Train Acc: 0.9135, Val Loss: 1.7408, Val Acc: 0.9115\n",
            "Epoch [409/4000], Train Loss: 1.7258, Train Acc: 0.9141, Val Loss: 1.7324, Val Acc: 0.9122\n",
            "Epoch [410/4000], Train Loss: 1.7271, Train Acc: 0.9138, Val Loss: 1.7249, Val Acc: 0.9160\n",
            "Epoch [411/4000], Train Loss: 1.7248, Train Acc: 0.9144, Val Loss: 1.7239, Val Acc: 0.9147\n",
            "Epoch [412/4000], Train Loss: 1.7276, Train Acc: 0.9115, Val Loss: 1.7225, Val Acc: 0.9173\n",
            "Epoch [413/4000], Train Loss: 1.7250, Train Acc: 0.9141, Val Loss: 1.7209, Val Acc: 0.9160\n",
            "Epoch [414/4000], Train Loss: 1.7245, Train Acc: 0.9125, Val Loss: 1.7212, Val Acc: 0.9186\n",
            "Epoch [415/4000], Train Loss: 1.7265, Train Acc: 0.9143, Val Loss: 1.7225, Val Acc: 0.9115\n",
            "Epoch [416/4000], Train Loss: 1.7258, Train Acc: 0.9146, Val Loss: 1.7340, Val Acc: 0.9045\n",
            "Epoch [417/4000], Train Loss: 1.7224, Train Acc: 0.9173, Val Loss: 1.7343, Val Acc: 0.9090\n",
            "Epoch [418/4000], Train Loss: 1.7231, Train Acc: 0.9157, Val Loss: 1.7230, Val Acc: 0.9167\n",
            "Epoch [419/4000], Train Loss: 1.7242, Train Acc: 0.9139, Val Loss: 1.7233, Val Acc: 0.9147\n",
            "Epoch [420/4000], Train Loss: 1.7234, Train Acc: 0.9163, Val Loss: 1.7331, Val Acc: 0.9115\n",
            "Epoch [421/4000], Train Loss: 1.7261, Train Acc: 0.9139, Val Loss: 1.7211, Val Acc: 0.9141\n",
            "Epoch [422/4000], Train Loss: 1.7240, Train Acc: 0.9146, Val Loss: 1.7249, Val Acc: 0.9154\n",
            "Epoch [423/4000], Train Loss: 1.7233, Train Acc: 0.9176, Val Loss: 1.7218, Val Acc: 0.9192\n",
            "Epoch [424/4000], Train Loss: 1.7235, Train Acc: 0.9163, Val Loss: 1.7298, Val Acc: 0.9115\n",
            "Epoch [425/4000], Train Loss: 1.7243, Train Acc: 0.9162, Val Loss: 1.7192, Val Acc: 0.9173\n",
            "Epoch [426/4000], Train Loss: 1.7262, Train Acc: 0.9159, Val Loss: 1.7301, Val Acc: 0.9154\n",
            "Epoch [427/4000], Train Loss: 1.7240, Train Acc: 0.9149, Val Loss: 1.7295, Val Acc: 0.9160\n",
            "Epoch [428/4000], Train Loss: 1.7277, Train Acc: 0.9144, Val Loss: 1.7344, Val Acc: 0.9115\n",
            "Epoch [429/4000], Train Loss: 1.7291, Train Acc: 0.9093, Val Loss: 1.7273, Val Acc: 0.9122\n",
            "Epoch [430/4000], Train Loss: 1.7233, Train Acc: 0.9152, Val Loss: 1.7375, Val Acc: 0.9058\n",
            "Epoch [431/4000], Train Loss: 1.7251, Train Acc: 0.9155, Val Loss: 1.7266, Val Acc: 0.9147\n",
            "Epoch [432/4000], Train Loss: 1.7246, Train Acc: 0.9155, Val Loss: 1.7278, Val Acc: 0.9077\n",
            "Epoch [433/4000], Train Loss: 1.7230, Train Acc: 0.9162, Val Loss: 1.7255, Val Acc: 0.9122\n",
            "Epoch [434/4000], Train Loss: 1.7264, Train Acc: 0.9133, Val Loss: 1.7223, Val Acc: 0.9179\n",
            "Epoch [435/4000], Train Loss: 1.7254, Train Acc: 0.9149, Val Loss: 1.7245, Val Acc: 0.9115\n",
            "Epoch [436/4000], Train Loss: 1.7239, Train Acc: 0.9157, Val Loss: 1.7283, Val Acc: 0.9128\n",
            "Epoch [437/4000], Train Loss: 1.7249, Train Acc: 0.9143, Val Loss: 1.7217, Val Acc: 0.9147\n",
            "Epoch [438/4000], Train Loss: 1.7248, Train Acc: 0.9152, Val Loss: 1.7287, Val Acc: 0.9090\n",
            "Epoch [439/4000], Train Loss: 1.7263, Train Acc: 0.9123, Val Loss: 1.7260, Val Acc: 0.9141\n",
            "Epoch [440/4000], Train Loss: 1.7245, Train Acc: 0.9149, Val Loss: 1.7235, Val Acc: 0.9128\n",
            "Epoch [441/4000], Train Loss: 1.7236, Train Acc: 0.9152, Val Loss: 1.7268, Val Acc: 0.9103\n",
            "Epoch [442/4000], Train Loss: 1.7240, Train Acc: 0.9147, Val Loss: 1.7323, Val Acc: 0.9141\n",
            "Epoch [443/4000], Train Loss: 1.7249, Train Acc: 0.9151, Val Loss: 1.7237, Val Acc: 0.9154\n",
            "Epoch [444/4000], Train Loss: 1.7238, Train Acc: 0.9143, Val Loss: 1.7230, Val Acc: 0.9141\n",
            "Epoch [445/4000], Train Loss: 1.7242, Train Acc: 0.9152, Val Loss: 1.7265, Val Acc: 0.9103\n",
            "Epoch [446/4000], Train Loss: 1.7256, Train Acc: 0.9135, Val Loss: 1.7266, Val Acc: 0.9064\n",
            "Epoch [447/4000], Train Loss: 1.7238, Train Acc: 0.9160, Val Loss: 1.7255, Val Acc: 0.9154\n",
            "Epoch [448/4000], Train Loss: 1.7260, Train Acc: 0.9147, Val Loss: 1.7244, Val Acc: 0.9154\n",
            "Epoch [449/4000], Train Loss: 1.7229, Train Acc: 0.9162, Val Loss: 1.7226, Val Acc: 0.9135\n",
            "Epoch [450/4000], Train Loss: 1.7243, Train Acc: 0.9154, Val Loss: 1.7235, Val Acc: 0.9154\n",
            "Epoch [451/4000], Train Loss: 1.7250, Train Acc: 0.9138, Val Loss: 1.7245, Val Acc: 0.9179\n",
            "Epoch [452/4000], Train Loss: 1.7243, Train Acc: 0.9133, Val Loss: 1.7209, Val Acc: 0.9160\n",
            "Epoch [453/4000], Train Loss: 1.7234, Train Acc: 0.9165, Val Loss: 1.7379, Val Acc: 0.8981\n",
            "Epoch [454/4000], Train Loss: 1.7244, Train Acc: 0.9152, Val Loss: 1.7212, Val Acc: 0.9179\n",
            "Epoch [455/4000], Train Loss: 1.7242, Train Acc: 0.9155, Val Loss: 1.7221, Val Acc: 0.9192\n",
            "Epoch [456/4000], Train Loss: 1.7233, Train Acc: 0.9151, Val Loss: 1.7212, Val Acc: 0.9179\n",
            "Epoch [457/4000], Train Loss: 1.7217, Train Acc: 0.9179, Val Loss: 1.7309, Val Acc: 0.9103\n",
            "Epoch [458/4000], Train Loss: 1.7244, Train Acc: 0.9152, Val Loss: 1.7323, Val Acc: 0.9122\n",
            "Epoch [459/4000], Train Loss: 1.7250, Train Acc: 0.9141, Val Loss: 1.7239, Val Acc: 0.9128\n",
            "Epoch [460/4000], Train Loss: 1.7238, Train Acc: 0.9163, Val Loss: 1.7234, Val Acc: 0.9147\n",
            "Epoch [461/4000], Train Loss: 1.7262, Train Acc: 0.9139, Val Loss: 1.7289, Val Acc: 0.9096\n",
            "Epoch [462/4000], Train Loss: 1.7226, Train Acc: 0.9154, Val Loss: 1.7203, Val Acc: 0.9141\n",
            "Epoch [463/4000], Train Loss: 1.7235, Train Acc: 0.9157, Val Loss: 1.7323, Val Acc: 0.9090\n",
            "Epoch [464/4000], Train Loss: 1.7280, Train Acc: 0.9154, Val Loss: 1.7216, Val Acc: 0.9154\n",
            "Epoch [465/4000], Train Loss: 1.7237, Train Acc: 0.9143, Val Loss: 1.7336, Val Acc: 0.9026\n",
            "Epoch [466/4000], Train Loss: 1.7248, Train Acc: 0.9143, Val Loss: 1.7228, Val Acc: 0.9160\n",
            "Epoch [467/4000], Train Loss: 1.7217, Train Acc: 0.9179, Val Loss: 1.7275, Val Acc: 0.9147\n",
            "Epoch [468/4000], Train Loss: 1.7259, Train Acc: 0.9154, Val Loss: 1.7283, Val Acc: 0.9064\n",
            "Epoch [469/4000], Train Loss: 1.7219, Train Acc: 0.9176, Val Loss: 1.7249, Val Acc: 0.9115\n",
            "Epoch [470/4000], Train Loss: 1.7250, Train Acc: 0.9136, Val Loss: 1.7223, Val Acc: 0.9160\n",
            "Epoch [471/4000], Train Loss: 1.7255, Train Acc: 0.9133, Val Loss: 1.7292, Val Acc: 0.9128\n",
            "Epoch [472/4000], Train Loss: 1.7236, Train Acc: 0.9135, Val Loss: 1.7236, Val Acc: 0.9154\n",
            "Epoch [473/4000], Train Loss: 1.7231, Train Acc: 0.9159, Val Loss: 1.7228, Val Acc: 0.9167\n",
            "Epoch [474/4000], Train Loss: 1.7250, Train Acc: 0.9149, Val Loss: 1.7243, Val Acc: 0.9135\n",
            "Epoch [475/4000], Train Loss: 1.7221, Train Acc: 0.9147, Val Loss: 1.7188, Val Acc: 0.9186\n",
            "Epoch [476/4000], Train Loss: 1.7228, Train Acc: 0.9192, Val Loss: 1.7228, Val Acc: 0.9141\n",
            "Epoch [477/4000], Train Loss: 1.7259, Train Acc: 0.9151, Val Loss: 1.7307, Val Acc: 0.9135\n",
            "Epoch [478/4000], Train Loss: 1.7232, Train Acc: 0.9163, Val Loss: 1.7248, Val Acc: 0.9154\n",
            "Epoch [479/4000], Train Loss: 1.7221, Train Acc: 0.9181, Val Loss: 1.7317, Val Acc: 0.9122\n",
            "Epoch [480/4000], Train Loss: 1.7250, Train Acc: 0.9135, Val Loss: 1.7261, Val Acc: 0.9173\n",
            "Epoch [481/4000], Train Loss: 1.7248, Train Acc: 0.9151, Val Loss: 1.7213, Val Acc: 0.9167\n",
            "Epoch [482/4000], Train Loss: 1.7216, Train Acc: 0.9173, Val Loss: 1.7218, Val Acc: 0.9212\n",
            "Epoch [483/4000], Train Loss: 1.7237, Train Acc: 0.9159, Val Loss: 1.7360, Val Acc: 0.9006\n",
            "Epoch [484/4000], Train Loss: 1.7230, Train Acc: 0.9176, Val Loss: 1.7275, Val Acc: 0.9135\n",
            "Epoch [485/4000], Train Loss: 1.7233, Train Acc: 0.9170, Val Loss: 1.7275, Val Acc: 0.9090\n",
            "Epoch [486/4000], Train Loss: 1.7250, Train Acc: 0.9143, Val Loss: 1.7396, Val Acc: 0.9077\n",
            "Epoch [487/4000], Train Loss: 1.7234, Train Acc: 0.9176, Val Loss: 1.7184, Val Acc: 0.9192\n",
            "Epoch [488/4000], Train Loss: 1.7229, Train Acc: 0.9160, Val Loss: 1.7239, Val Acc: 0.9173\n",
            "Epoch [489/4000], Train Loss: 1.7230, Train Acc: 0.9178, Val Loss: 1.7232, Val Acc: 0.9179\n",
            "Epoch [490/4000], Train Loss: 1.7220, Train Acc: 0.9176, Val Loss: 1.7228, Val Acc: 0.9179\n",
            "Epoch [491/4000], Train Loss: 1.7239, Train Acc: 0.9165, Val Loss: 1.7208, Val Acc: 0.9154\n",
            "Epoch [492/4000], Train Loss: 1.7256, Train Acc: 0.9135, Val Loss: 1.7340, Val Acc: 0.9038\n",
            "Epoch [493/4000], Train Loss: 1.7255, Train Acc: 0.9136, Val Loss: 1.7276, Val Acc: 0.9109\n",
            "Epoch [494/4000], Train Loss: 1.7245, Train Acc: 0.9143, Val Loss: 1.7257, Val Acc: 0.9154\n",
            "Epoch [495/4000], Train Loss: 1.7244, Train Acc: 0.9144, Val Loss: 1.7242, Val Acc: 0.9141\n",
            "Epoch [496/4000], Train Loss: 1.7248, Train Acc: 0.9139, Val Loss: 1.7202, Val Acc: 0.9192\n",
            "Epoch [497/4000], Train Loss: 1.7221, Train Acc: 0.9170, Val Loss: 1.7232, Val Acc: 0.9160\n",
            "Epoch [498/4000], Train Loss: 1.7217, Train Acc: 0.9183, Val Loss: 1.7356, Val Acc: 0.9122\n",
            "Epoch [499/4000], Train Loss: 1.7234, Train Acc: 0.9165, Val Loss: 1.7338, Val Acc: 0.9083\n",
            "Epoch [500/4000], Train Loss: 1.7235, Train Acc: 0.9179, Val Loss: 1.7302, Val Acc: 0.9167\n",
            "Epoch [501/4000], Train Loss: 1.7218, Train Acc: 0.9171, Val Loss: 1.7215, Val Acc: 0.9173\n",
            "Epoch [502/4000], Train Loss: 1.7215, Train Acc: 0.9179, Val Loss: 1.7213, Val Acc: 0.9179\n",
            "Epoch [503/4000], Train Loss: 1.7261, Train Acc: 0.9133, Val Loss: 1.7216, Val Acc: 0.9141\n",
            "Epoch [504/4000], Train Loss: 1.7227, Train Acc: 0.9157, Val Loss: 1.7199, Val Acc: 0.9192\n",
            "Epoch [505/4000], Train Loss: 1.7210, Train Acc: 0.9184, Val Loss: 1.7257, Val Acc: 0.9147\n",
            "Epoch [506/4000], Train Loss: 1.7233, Train Acc: 0.9173, Val Loss: 1.7252, Val Acc: 0.9186\n",
            "Epoch [507/4000], Train Loss: 1.7218, Train Acc: 0.9175, Val Loss: 1.7259, Val Acc: 0.9115\n",
            "Epoch [508/4000], Train Loss: 1.7233, Train Acc: 0.9165, Val Loss: 1.7204, Val Acc: 0.9160\n",
            "Epoch [509/4000], Train Loss: 1.7234, Train Acc: 0.9167, Val Loss: 1.7257, Val Acc: 0.9186\n",
            "Epoch [510/4000], Train Loss: 1.7231, Train Acc: 0.9157, Val Loss: 1.7237, Val Acc: 0.9167\n",
            "Epoch [511/4000], Train Loss: 1.7209, Train Acc: 0.9191, Val Loss: 1.7283, Val Acc: 0.9147\n",
            "Epoch [512/4000], Train Loss: 1.7223, Train Acc: 0.9173, Val Loss: 1.7230, Val Acc: 0.9141\n",
            "Epoch [513/4000], Train Loss: 1.7238, Train Acc: 0.9157, Val Loss: 1.7233, Val Acc: 0.9173\n",
            "Epoch [514/4000], Train Loss: 1.7220, Train Acc: 0.9187, Val Loss: 1.7316, Val Acc: 0.9077\n",
            "Epoch [515/4000], Train Loss: 1.7238, Train Acc: 0.9162, Val Loss: 1.7226, Val Acc: 0.9147\n",
            "Epoch [516/4000], Train Loss: 1.7244, Train Acc: 0.9155, Val Loss: 1.7192, Val Acc: 0.9167\n",
            "Epoch [517/4000], Train Loss: 1.7250, Train Acc: 0.9151, Val Loss: 1.7302, Val Acc: 0.9096\n",
            "Epoch [518/4000], Train Loss: 1.7224, Train Acc: 0.9179, Val Loss: 1.7243, Val Acc: 0.9167\n",
            "Epoch [519/4000], Train Loss: 1.7248, Train Acc: 0.9151, Val Loss: 1.7297, Val Acc: 0.9141\n",
            "Epoch [520/4000], Train Loss: 1.7212, Train Acc: 0.9171, Val Loss: 1.7320, Val Acc: 0.9109\n",
            "Epoch [521/4000], Train Loss: 1.7260, Train Acc: 0.9139, Val Loss: 1.7273, Val Acc: 0.9122\n",
            "Epoch [522/4000], Train Loss: 1.7236, Train Acc: 0.9151, Val Loss: 1.7218, Val Acc: 0.9141\n",
            "Epoch [523/4000], Train Loss: 1.7219, Train Acc: 0.9171, Val Loss: 1.7225, Val Acc: 0.9167\n",
            "Epoch [524/4000], Train Loss: 1.7216, Train Acc: 0.9178, Val Loss: 1.7203, Val Acc: 0.9192\n",
            "Epoch [525/4000], Train Loss: 1.7214, Train Acc: 0.9165, Val Loss: 1.7213, Val Acc: 0.9192\n",
            "Epoch [526/4000], Train Loss: 1.7208, Train Acc: 0.9186, Val Loss: 1.7267, Val Acc: 0.9154\n",
            "Epoch [527/4000], Train Loss: 1.7218, Train Acc: 0.9181, Val Loss: 1.7230, Val Acc: 0.9192\n",
            "Epoch [528/4000], Train Loss: 1.7204, Train Acc: 0.9199, Val Loss: 1.7307, Val Acc: 0.9103\n",
            "Epoch [529/4000], Train Loss: 1.7237, Train Acc: 0.9157, Val Loss: 1.7253, Val Acc: 0.9135\n",
            "Epoch [530/4000], Train Loss: 1.7215, Train Acc: 0.9186, Val Loss: 1.7329, Val Acc: 0.9038\n",
            "Epoch [531/4000], Train Loss: 1.7252, Train Acc: 0.9147, Val Loss: 1.7228, Val Acc: 0.9122\n",
            "Epoch [532/4000], Train Loss: 1.7218, Train Acc: 0.9175, Val Loss: 1.7186, Val Acc: 0.9167\n",
            "Epoch [533/4000], Train Loss: 1.7217, Train Acc: 0.9170, Val Loss: 1.7253, Val Acc: 0.9154\n",
            "Epoch [534/4000], Train Loss: 1.7226, Train Acc: 0.9175, Val Loss: 1.7273, Val Acc: 0.9160\n",
            "Epoch [535/4000], Train Loss: 1.7223, Train Acc: 0.9168, Val Loss: 1.7209, Val Acc: 0.9205\n",
            "Epoch [536/4000], Train Loss: 1.7201, Train Acc: 0.9184, Val Loss: 1.7240, Val Acc: 0.9154\n",
            "Epoch [537/4000], Train Loss: 1.7246, Train Acc: 0.9149, Val Loss: 1.7180, Val Acc: 0.9186\n",
            "Epoch [538/4000], Train Loss: 1.7241, Train Acc: 0.9135, Val Loss: 1.7256, Val Acc: 0.9115\n",
            "Epoch [539/4000], Train Loss: 1.7244, Train Acc: 0.9146, Val Loss: 1.7210, Val Acc: 0.9167\n",
            "Epoch [540/4000], Train Loss: 1.7201, Train Acc: 0.9191, Val Loss: 1.7200, Val Acc: 0.9141\n",
            "Epoch [541/4000], Train Loss: 1.7210, Train Acc: 0.9181, Val Loss: 1.7211, Val Acc: 0.9231\n",
            "Epoch [542/4000], Train Loss: 1.7247, Train Acc: 0.9165, Val Loss: 1.7210, Val Acc: 0.9160\n",
            "Epoch [543/4000], Train Loss: 1.7243, Train Acc: 0.9149, Val Loss: 1.7242, Val Acc: 0.9186\n",
            "Epoch [544/4000], Train Loss: 1.7228, Train Acc: 0.9171, Val Loss: 1.7252, Val Acc: 0.9109\n",
            "Epoch [545/4000], Train Loss: 1.7268, Train Acc: 0.9147, Val Loss: 1.7196, Val Acc: 0.9218\n",
            "Epoch [546/4000], Train Loss: 1.7248, Train Acc: 0.9154, Val Loss: 1.7343, Val Acc: 0.9109\n",
            "Epoch [547/4000], Train Loss: 1.7218, Train Acc: 0.9175, Val Loss: 1.7184, Val Acc: 0.9199\n",
            "Epoch [548/4000], Train Loss: 1.7211, Train Acc: 0.9181, Val Loss: 1.7258, Val Acc: 0.9128\n",
            "Epoch [549/4000], Train Loss: 1.7221, Train Acc: 0.9163, Val Loss: 1.7190, Val Acc: 0.9192\n",
            "Epoch [550/4000], Train Loss: 1.7193, Train Acc: 0.9205, Val Loss: 1.7200, Val Acc: 0.9218\n",
            "Epoch [551/4000], Train Loss: 1.7206, Train Acc: 0.9192, Val Loss: 1.7214, Val Acc: 0.9192\n",
            "Epoch [552/4000], Train Loss: 1.7233, Train Acc: 0.9168, Val Loss: 1.7217, Val Acc: 0.9224\n",
            "Epoch [553/4000], Train Loss: 1.7234, Train Acc: 0.9170, Val Loss: 1.7189, Val Acc: 0.9173\n",
            "Epoch [554/4000], Train Loss: 1.7205, Train Acc: 0.9197, Val Loss: 1.7261, Val Acc: 0.9109\n",
            "Epoch [555/4000], Train Loss: 1.7219, Train Acc: 0.9171, Val Loss: 1.7348, Val Acc: 0.9006\n",
            "Epoch [556/4000], Train Loss: 1.7223, Train Acc: 0.9165, Val Loss: 1.7206, Val Acc: 0.9199\n",
            "Epoch [557/4000], Train Loss: 1.7212, Train Acc: 0.9192, Val Loss: 1.7353, Val Acc: 0.9032\n",
            "Epoch [558/4000], Train Loss: 1.7211, Train Acc: 0.9183, Val Loss: 1.7263, Val Acc: 0.9147\n",
            "Epoch [559/4000], Train Loss: 1.7219, Train Acc: 0.9175, Val Loss: 1.7263, Val Acc: 0.9135\n",
            "Epoch [560/4000], Train Loss: 1.7216, Train Acc: 0.9159, Val Loss: 1.7299, Val Acc: 0.9115\n",
            "Epoch [561/4000], Train Loss: 1.7277, Train Acc: 0.9131, Val Loss: 1.7257, Val Acc: 0.9167\n",
            "Epoch [562/4000], Train Loss: 1.7202, Train Acc: 0.9178, Val Loss: 1.7283, Val Acc: 0.9141\n",
            "Epoch [563/4000], Train Loss: 1.7205, Train Acc: 0.9187, Val Loss: 1.7229, Val Acc: 0.9186\n",
            "Epoch [564/4000], Train Loss: 1.7225, Train Acc: 0.9159, Val Loss: 1.7180, Val Acc: 0.9205\n",
            "Epoch [565/4000], Train Loss: 1.7229, Train Acc: 0.9173, Val Loss: 1.7187, Val Acc: 0.9179\n",
            "Epoch [566/4000], Train Loss: 1.7198, Train Acc: 0.9194, Val Loss: 1.7204, Val Acc: 0.9205\n",
            "Epoch [567/4000], Train Loss: 1.7221, Train Acc: 0.9173, Val Loss: 1.7182, Val Acc: 0.9186\n",
            "Epoch [568/4000], Train Loss: 1.7192, Train Acc: 0.9197, Val Loss: 1.7265, Val Acc: 0.9186\n",
            "Epoch [569/4000], Train Loss: 1.7219, Train Acc: 0.9170, Val Loss: 1.7262, Val Acc: 0.9128\n",
            "Epoch [570/4000], Train Loss: 1.7225, Train Acc: 0.9157, Val Loss: 1.7222, Val Acc: 0.9154\n",
            "Epoch [571/4000], Train Loss: 1.7225, Train Acc: 0.9160, Val Loss: 1.7279, Val Acc: 0.9109\n",
            "Epoch [572/4000], Train Loss: 1.7280, Train Acc: 0.9130, Val Loss: 1.7225, Val Acc: 0.9205\n",
            "Epoch [573/4000], Train Loss: 1.7256, Train Acc: 0.9154, Val Loss: 1.7181, Val Acc: 0.9218\n",
            "Epoch [574/4000], Train Loss: 1.7201, Train Acc: 0.9170, Val Loss: 1.7243, Val Acc: 0.9192\n",
            "Epoch [575/4000], Train Loss: 1.7217, Train Acc: 0.9183, Val Loss: 1.7285, Val Acc: 0.9077\n",
            "Epoch [576/4000], Train Loss: 1.7235, Train Acc: 0.9162, Val Loss: 1.7380, Val Acc: 0.9096\n",
            "Epoch [577/4000], Train Loss: 1.7217, Train Acc: 0.9189, Val Loss: 1.7260, Val Acc: 0.9186\n",
            "Epoch [578/4000], Train Loss: 1.7201, Train Acc: 0.9197, Val Loss: 1.7276, Val Acc: 0.9154\n",
            "Epoch [579/4000], Train Loss: 1.7207, Train Acc: 0.9178, Val Loss: 1.7203, Val Acc: 0.9192\n",
            "Epoch [580/4000], Train Loss: 1.7237, Train Acc: 0.9171, Val Loss: 1.7229, Val Acc: 0.9160\n",
            "Epoch [581/4000], Train Loss: 1.7214, Train Acc: 0.9184, Val Loss: 1.7325, Val Acc: 0.9064\n",
            "Epoch [582/4000], Train Loss: 1.7239, Train Acc: 0.9152, Val Loss: 1.7217, Val Acc: 0.9160\n",
            "Epoch [583/4000], Train Loss: 1.7201, Train Acc: 0.9194, Val Loss: 1.7220, Val Acc: 0.9218\n",
            "Epoch [584/4000], Train Loss: 1.7203, Train Acc: 0.9199, Val Loss: 1.7209, Val Acc: 0.9199\n",
            "Epoch [585/4000], Train Loss: 1.7203, Train Acc: 0.9199, Val Loss: 1.7263, Val Acc: 0.9160\n",
            "Epoch [586/4000], Train Loss: 1.7226, Train Acc: 0.9160, Val Loss: 1.7327, Val Acc: 0.9122\n",
            "Epoch [587/4000], Train Loss: 1.7215, Train Acc: 0.9176, Val Loss: 1.7230, Val Acc: 0.9128\n",
            "Epoch [588/4000], Train Loss: 1.7218, Train Acc: 0.9176, Val Loss: 1.7205, Val Acc: 0.9186\n",
            "Epoch [589/4000], Train Loss: 1.7235, Train Acc: 0.9162, Val Loss: 1.7275, Val Acc: 0.9122\n",
            "Epoch [590/4000], Train Loss: 1.7215, Train Acc: 0.9187, Val Loss: 1.7223, Val Acc: 0.9141\n",
            "Epoch [591/4000], Train Loss: 1.7245, Train Acc: 0.9144, Val Loss: 1.7214, Val Acc: 0.9128\n",
            "Epoch [592/4000], Train Loss: 1.7204, Train Acc: 0.9178, Val Loss: 1.7207, Val Acc: 0.9192\n",
            "Epoch [593/4000], Train Loss: 1.7202, Train Acc: 0.9191, Val Loss: 1.7206, Val Acc: 0.9199\n",
            "Epoch [594/4000], Train Loss: 1.7206, Train Acc: 0.9192, Val Loss: 1.7200, Val Acc: 0.9199\n",
            "Epoch [595/4000], Train Loss: 1.7196, Train Acc: 0.9200, Val Loss: 1.7225, Val Acc: 0.9231\n",
            "Epoch [596/4000], Train Loss: 1.7218, Train Acc: 0.9207, Val Loss: 1.7309, Val Acc: 0.9147\n",
            "Epoch [597/4000], Train Loss: 1.7232, Train Acc: 0.9171, Val Loss: 1.7202, Val Acc: 0.9199\n",
            "Epoch [598/4000], Train Loss: 1.7185, Train Acc: 0.9210, Val Loss: 1.7199, Val Acc: 0.9179\n",
            "Epoch [599/4000], Train Loss: 1.7211, Train Acc: 0.9192, Val Loss: 1.7244, Val Acc: 0.9115\n",
            "Epoch [600/4000], Train Loss: 1.7203, Train Acc: 0.9171, Val Loss: 1.7255, Val Acc: 0.9224\n",
            "Epoch [601/4000], Train Loss: 1.7230, Train Acc: 0.9162, Val Loss: 1.7236, Val Acc: 0.9135\n",
            "Epoch [602/4000], Train Loss: 1.7247, Train Acc: 0.9133, Val Loss: 1.7233, Val Acc: 0.9160\n",
            "Epoch [603/4000], Train Loss: 1.7211, Train Acc: 0.9184, Val Loss: 1.7210, Val Acc: 0.9192\n",
            "Epoch [604/4000], Train Loss: 1.7217, Train Acc: 0.9171, Val Loss: 1.7184, Val Acc: 0.9186\n",
            "Epoch [605/4000], Train Loss: 1.7187, Train Acc: 0.9199, Val Loss: 1.7254, Val Acc: 0.9103\n",
            "Epoch [606/4000], Train Loss: 1.7240, Train Acc: 0.9167, Val Loss: 1.7215, Val Acc: 0.9160\n",
            "Epoch [607/4000], Train Loss: 1.7234, Train Acc: 0.9162, Val Loss: 1.7189, Val Acc: 0.9231\n",
            "Epoch [608/4000], Train Loss: 1.7202, Train Acc: 0.9197, Val Loss: 1.7228, Val Acc: 0.9192\n",
            "Epoch [609/4000], Train Loss: 1.7225, Train Acc: 0.9173, Val Loss: 1.7193, Val Acc: 0.9154\n",
            "Epoch [610/4000], Train Loss: 1.7200, Train Acc: 0.9183, Val Loss: 1.7309, Val Acc: 0.9154\n",
            "Epoch [611/4000], Train Loss: 1.7204, Train Acc: 0.9213, Val Loss: 1.7356, Val Acc: 0.9058\n",
            "Epoch [612/4000], Train Loss: 1.7221, Train Acc: 0.9162, Val Loss: 1.7249, Val Acc: 0.9179\n",
            "Epoch [613/4000], Train Loss: 1.7213, Train Acc: 0.9192, Val Loss: 1.7223, Val Acc: 0.9186\n",
            "Epoch [614/4000], Train Loss: 1.7240, Train Acc: 0.9170, Val Loss: 1.7308, Val Acc: 0.9083\n",
            "Epoch [615/4000], Train Loss: 1.7225, Train Acc: 0.9163, Val Loss: 1.7262, Val Acc: 0.9173\n",
            "Epoch [616/4000], Train Loss: 1.7222, Train Acc: 0.9183, Val Loss: 1.7278, Val Acc: 0.9128\n",
            "Epoch [617/4000], Train Loss: 1.7212, Train Acc: 0.9183, Val Loss: 1.7255, Val Acc: 0.9141\n",
            "Epoch [618/4000], Train Loss: 1.7199, Train Acc: 0.9187, Val Loss: 1.7169, Val Acc: 0.9212\n",
            "Epoch [619/4000], Train Loss: 1.7216, Train Acc: 0.9187, Val Loss: 1.7269, Val Acc: 0.9160\n",
            "Epoch [620/4000], Train Loss: 1.7206, Train Acc: 0.9197, Val Loss: 1.7257, Val Acc: 0.9090\n",
            "Epoch [621/4000], Train Loss: 1.7205, Train Acc: 0.9204, Val Loss: 1.7216, Val Acc: 0.9135\n",
            "Epoch [622/4000], Train Loss: 1.7204, Train Acc: 0.9183, Val Loss: 1.7248, Val Acc: 0.9147\n",
            "Epoch [623/4000], Train Loss: 1.7212, Train Acc: 0.9173, Val Loss: 1.7199, Val Acc: 0.9128\n",
            "Epoch [624/4000], Train Loss: 1.7229, Train Acc: 0.9176, Val Loss: 1.7333, Val Acc: 0.9032\n",
            "Epoch [625/4000], Train Loss: 1.7203, Train Acc: 0.9191, Val Loss: 1.7335, Val Acc: 0.9128\n",
            "Epoch [626/4000], Train Loss: 1.7251, Train Acc: 0.9155, Val Loss: 1.7252, Val Acc: 0.9141\n",
            "Epoch [627/4000], Train Loss: 1.7185, Train Acc: 0.9197, Val Loss: 1.7202, Val Acc: 0.9199\n",
            "Epoch [628/4000], Train Loss: 1.7208, Train Acc: 0.9207, Val Loss: 1.7186, Val Acc: 0.9192\n",
            "Epoch [629/4000], Train Loss: 1.7210, Train Acc: 0.9186, Val Loss: 1.7243, Val Acc: 0.9135\n",
            "Epoch [630/4000], Train Loss: 1.7195, Train Acc: 0.9194, Val Loss: 1.7263, Val Acc: 0.9154\n",
            "Epoch [631/4000], Train Loss: 1.7207, Train Acc: 0.9194, Val Loss: 1.7214, Val Acc: 0.9179\n",
            "Epoch [632/4000], Train Loss: 1.7206, Train Acc: 0.9170, Val Loss: 1.7247, Val Acc: 0.9179\n",
            "Epoch [633/4000], Train Loss: 1.7211, Train Acc: 0.9178, Val Loss: 1.7194, Val Acc: 0.9224\n",
            "Epoch [634/4000], Train Loss: 1.7204, Train Acc: 0.9192, Val Loss: 1.7279, Val Acc: 0.9141\n",
            "Epoch [635/4000], Train Loss: 1.7231, Train Acc: 0.9163, Val Loss: 1.7314, Val Acc: 0.9058\n",
            "Epoch [636/4000], Train Loss: 1.7233, Train Acc: 0.9146, Val Loss: 1.7199, Val Acc: 0.9173\n",
            "Epoch [637/4000], Train Loss: 1.7195, Train Acc: 0.9191, Val Loss: 1.7263, Val Acc: 0.9083\n",
            "Epoch [638/4000], Train Loss: 1.7205, Train Acc: 0.9181, Val Loss: 1.7247, Val Acc: 0.9199\n",
            "Epoch [639/4000], Train Loss: 1.7191, Train Acc: 0.9202, Val Loss: 1.7217, Val Acc: 0.9147\n",
            "Epoch [640/4000], Train Loss: 1.7210, Train Acc: 0.9178, Val Loss: 1.7232, Val Acc: 0.9115\n",
            "Epoch [641/4000], Train Loss: 1.7243, Train Acc: 0.9155, Val Loss: 1.7203, Val Acc: 0.9167\n",
            "Epoch [642/4000], Train Loss: 1.7246, Train Acc: 0.9151, Val Loss: 1.7253, Val Acc: 0.9096\n",
            "Epoch [643/4000], Train Loss: 1.7217, Train Acc: 0.9165, Val Loss: 1.7231, Val Acc: 0.9173\n",
            "Epoch [644/4000], Train Loss: 1.7203, Train Acc: 0.9191, Val Loss: 1.7244, Val Acc: 0.9154\n",
            "Epoch [645/4000], Train Loss: 1.7207, Train Acc: 0.9197, Val Loss: 1.7223, Val Acc: 0.9141\n",
            "Epoch [646/4000], Train Loss: 1.7226, Train Acc: 0.9173, Val Loss: 1.7233, Val Acc: 0.9205\n",
            "Epoch [647/4000], Train Loss: 1.7199, Train Acc: 0.9199, Val Loss: 1.7208, Val Acc: 0.9192\n",
            "Epoch [648/4000], Train Loss: 1.7215, Train Acc: 0.9179, Val Loss: 1.7182, Val Acc: 0.9224\n",
            "Epoch [649/4000], Train Loss: 1.7191, Train Acc: 0.9210, Val Loss: 1.7296, Val Acc: 0.9038\n",
            "Epoch [650/4000], Train Loss: 1.7196, Train Acc: 0.9194, Val Loss: 1.7195, Val Acc: 0.9205\n",
            "Epoch [651/4000], Train Loss: 1.7203, Train Acc: 0.9200, Val Loss: 1.7226, Val Acc: 0.9135\n",
            "Epoch [652/4000], Train Loss: 1.7191, Train Acc: 0.9207, Val Loss: 1.7290, Val Acc: 0.9128\n",
            "Epoch [653/4000], Train Loss: 1.7212, Train Acc: 0.9187, Val Loss: 1.7252, Val Acc: 0.9199\n",
            "Epoch [654/4000], Train Loss: 1.7193, Train Acc: 0.9210, Val Loss: 1.7180, Val Acc: 0.9212\n",
            "Epoch [655/4000], Train Loss: 1.7205, Train Acc: 0.9196, Val Loss: 1.7375, Val Acc: 0.9096\n",
            "Epoch [656/4000], Train Loss: 1.7199, Train Acc: 0.9199, Val Loss: 1.7237, Val Acc: 0.9212\n",
            "Epoch [657/4000], Train Loss: 1.7222, Train Acc: 0.9178, Val Loss: 1.7256, Val Acc: 0.9147\n",
            "Epoch [658/4000], Train Loss: 1.7186, Train Acc: 0.9204, Val Loss: 1.7219, Val Acc: 0.9205\n",
            "Epoch [659/4000], Train Loss: 1.7202, Train Acc: 0.9191, Val Loss: 1.7219, Val Acc: 0.9179\n",
            "Epoch [660/4000], Train Loss: 1.7188, Train Acc: 0.9208, Val Loss: 1.7255, Val Acc: 0.9064\n",
            "Epoch [661/4000], Train Loss: 1.7219, Train Acc: 0.9178, Val Loss: 1.7261, Val Acc: 0.9128\n",
            "Epoch [662/4000], Train Loss: 1.7190, Train Acc: 0.9197, Val Loss: 1.7224, Val Acc: 0.9147\n",
            "Epoch [663/4000], Train Loss: 1.7206, Train Acc: 0.9184, Val Loss: 1.7247, Val Acc: 0.9179\n",
            "Epoch [664/4000], Train Loss: 1.7198, Train Acc: 0.9205, Val Loss: 1.7187, Val Acc: 0.9224\n",
            "Epoch [665/4000], Train Loss: 1.7193, Train Acc: 0.9199, Val Loss: 1.7198, Val Acc: 0.9154\n",
            "Epoch [666/4000], Train Loss: 1.7216, Train Acc: 0.9186, Val Loss: 1.7199, Val Acc: 0.9192\n",
            "Epoch [667/4000], Train Loss: 1.7217, Train Acc: 0.9183, Val Loss: 1.7256, Val Acc: 0.9160\n",
            "Epoch [668/4000], Train Loss: 1.7195, Train Acc: 0.9189, Val Loss: 1.7189, Val Acc: 0.9199\n",
            "Epoch [669/4000], Train Loss: 1.7209, Train Acc: 0.9183, Val Loss: 1.7214, Val Acc: 0.9135\n",
            "Epoch [670/4000], Train Loss: 1.7212, Train Acc: 0.9194, Val Loss: 1.7218, Val Acc: 0.9205\n",
            "Epoch [671/4000], Train Loss: 1.7196, Train Acc: 0.9212, Val Loss: 1.7195, Val Acc: 0.9218\n",
            "Epoch [672/4000], Train Loss: 1.7198, Train Acc: 0.9207, Val Loss: 1.7202, Val Acc: 0.9160\n",
            "Epoch [673/4000], Train Loss: 1.7216, Train Acc: 0.9176, Val Loss: 1.7199, Val Acc: 0.9218\n",
            "Epoch [674/4000], Train Loss: 1.7208, Train Acc: 0.9192, Val Loss: 1.7196, Val Acc: 0.9205\n",
            "Epoch [675/4000], Train Loss: 1.7209, Train Acc: 0.9183, Val Loss: 1.7189, Val Acc: 0.9224\n",
            "Epoch [676/4000], Train Loss: 1.7191, Train Acc: 0.9199, Val Loss: 1.7190, Val Acc: 0.9224\n",
            "Epoch [677/4000], Train Loss: 1.7189, Train Acc: 0.9200, Val Loss: 1.7250, Val Acc: 0.9160\n",
            "Epoch [678/4000], Train Loss: 1.7193, Train Acc: 0.9189, Val Loss: 1.7247, Val Acc: 0.9141\n",
            "Epoch [679/4000], Train Loss: 1.7211, Train Acc: 0.9191, Val Loss: 1.7238, Val Acc: 0.9173\n",
            "Epoch [680/4000], Train Loss: 1.7218, Train Acc: 0.9176, Val Loss: 1.7215, Val Acc: 0.9167\n",
            "Epoch [681/4000], Train Loss: 1.7182, Train Acc: 0.9216, Val Loss: 1.7234, Val Acc: 0.9160\n",
            "Epoch [682/4000], Train Loss: 1.7188, Train Acc: 0.9212, Val Loss: 1.7286, Val Acc: 0.9154\n",
            "Epoch [683/4000], Train Loss: 1.7211, Train Acc: 0.9191, Val Loss: 1.7206, Val Acc: 0.9141\n",
            "Epoch [684/4000], Train Loss: 1.7203, Train Acc: 0.9179, Val Loss: 1.7219, Val Acc: 0.9167\n",
            "Epoch [685/4000], Train Loss: 1.7201, Train Acc: 0.9202, Val Loss: 1.7240, Val Acc: 0.9186\n",
            "Epoch [686/4000], Train Loss: 1.7203, Train Acc: 0.9187, Val Loss: 1.7202, Val Acc: 0.9218\n",
            "Epoch [687/4000], Train Loss: 1.7189, Train Acc: 0.9215, Val Loss: 1.7266, Val Acc: 0.9141\n",
            "Epoch [688/4000], Train Loss: 1.7196, Train Acc: 0.9210, Val Loss: 1.7222, Val Acc: 0.9173\n",
            "Epoch [689/4000], Train Loss: 1.7199, Train Acc: 0.9196, Val Loss: 1.7212, Val Acc: 0.9186\n",
            "Epoch [690/4000], Train Loss: 1.7194, Train Acc: 0.9200, Val Loss: 1.7218, Val Acc: 0.9192\n",
            "Epoch [691/4000], Train Loss: 1.7220, Train Acc: 0.9179, Val Loss: 1.7253, Val Acc: 0.9192\n",
            "Epoch [692/4000], Train Loss: 1.7190, Train Acc: 0.9200, Val Loss: 1.7211, Val Acc: 0.9212\n",
            "Epoch [693/4000], Train Loss: 1.7204, Train Acc: 0.9194, Val Loss: 1.7214, Val Acc: 0.9186\n",
            "Epoch [694/4000], Train Loss: 1.7194, Train Acc: 0.9191, Val Loss: 1.7202, Val Acc: 0.9154\n",
            "Epoch [695/4000], Train Loss: 1.7184, Train Acc: 0.9184, Val Loss: 1.7194, Val Acc: 0.9141\n",
            "Epoch [696/4000], Train Loss: 1.7225, Train Acc: 0.9160, Val Loss: 1.7240, Val Acc: 0.9128\n",
            "Epoch [697/4000], Train Loss: 1.7202, Train Acc: 0.9170, Val Loss: 1.7204, Val Acc: 0.9205\n",
            "Epoch [698/4000], Train Loss: 1.7185, Train Acc: 0.9210, Val Loss: 1.7188, Val Acc: 0.9212\n",
            "Epoch [699/4000], Train Loss: 1.7209, Train Acc: 0.9183, Val Loss: 1.7247, Val Acc: 0.9122\n",
            "Epoch [700/4000], Train Loss: 1.7197, Train Acc: 0.9210, Val Loss: 1.7220, Val Acc: 0.9167\n",
            "Epoch [701/4000], Train Loss: 1.7216, Train Acc: 0.9196, Val Loss: 1.7213, Val Acc: 0.9199\n",
            "Epoch [702/4000], Train Loss: 1.7201, Train Acc: 0.9196, Val Loss: 1.7258, Val Acc: 0.9205\n",
            "Epoch [703/4000], Train Loss: 1.7203, Train Acc: 0.9204, Val Loss: 1.7181, Val Acc: 0.9250\n",
            "Epoch [704/4000], Train Loss: 1.7191, Train Acc: 0.9194, Val Loss: 1.7314, Val Acc: 0.9115\n",
            "Epoch [705/4000], Train Loss: 1.7212, Train Acc: 0.9192, Val Loss: 1.7291, Val Acc: 0.9122\n",
            "Epoch [706/4000], Train Loss: 1.7195, Train Acc: 0.9191, Val Loss: 1.7224, Val Acc: 0.9205\n",
            "Epoch [707/4000], Train Loss: 1.7183, Train Acc: 0.9204, Val Loss: 1.7190, Val Acc: 0.9192\n",
            "Epoch [708/4000], Train Loss: 1.7196, Train Acc: 0.9205, Val Loss: 1.7174, Val Acc: 0.9192\n",
            "Epoch [709/4000], Train Loss: 1.7212, Train Acc: 0.9196, Val Loss: 1.7318, Val Acc: 0.9141\n",
            "Epoch [710/4000], Train Loss: 1.7231, Train Acc: 0.9163, Val Loss: 1.7221, Val Acc: 0.9231\n",
            "Epoch [711/4000], Train Loss: 1.7180, Train Acc: 0.9207, Val Loss: 1.7169, Val Acc: 0.9231\n",
            "Epoch [712/4000], Train Loss: 1.7195, Train Acc: 0.9208, Val Loss: 1.7271, Val Acc: 0.9147\n",
            "Epoch [713/4000], Train Loss: 1.7246, Train Acc: 0.9151, Val Loss: 1.7184, Val Acc: 0.9186\n",
            "Epoch [714/4000], Train Loss: 1.7190, Train Acc: 0.9205, Val Loss: 1.7256, Val Acc: 0.9186\n",
            "Epoch [715/4000], Train Loss: 1.7198, Train Acc: 0.9207, Val Loss: 1.7173, Val Acc: 0.9224\n",
            "Epoch [716/4000], Train Loss: 1.7193, Train Acc: 0.9199, Val Loss: 1.7294, Val Acc: 0.9096\n",
            "Epoch [717/4000], Train Loss: 1.7188, Train Acc: 0.9207, Val Loss: 1.7163, Val Acc: 0.9231\n",
            "Epoch [718/4000], Train Loss: 1.7215, Train Acc: 0.9186, Val Loss: 1.7226, Val Acc: 0.9173\n",
            "Epoch [719/4000], Train Loss: 1.7215, Train Acc: 0.9173, Val Loss: 1.7172, Val Acc: 0.9192\n",
            "Epoch [720/4000], Train Loss: 1.7183, Train Acc: 0.9204, Val Loss: 1.7175, Val Acc: 0.9173\n",
            "Epoch [721/4000], Train Loss: 1.7172, Train Acc: 0.9223, Val Loss: 1.7157, Val Acc: 0.9231\n",
            "Epoch [722/4000], Train Loss: 1.7193, Train Acc: 0.9194, Val Loss: 1.7177, Val Acc: 0.9179\n",
            "Epoch [723/4000], Train Loss: 1.7177, Train Acc: 0.9202, Val Loss: 1.7180, Val Acc: 0.9192\n",
            "Epoch [724/4000], Train Loss: 1.7175, Train Acc: 0.9223, Val Loss: 1.7255, Val Acc: 0.9205\n",
            "Epoch [725/4000], Train Loss: 1.7231, Train Acc: 0.9173, Val Loss: 1.7212, Val Acc: 0.9160\n",
            "Epoch [726/4000], Train Loss: 1.7208, Train Acc: 0.9179, Val Loss: 1.7179, Val Acc: 0.9218\n",
            "Epoch [727/4000], Train Loss: 1.7193, Train Acc: 0.9197, Val Loss: 1.7279, Val Acc: 0.9167\n",
            "Epoch [728/4000], Train Loss: 1.7183, Train Acc: 0.9210, Val Loss: 1.7159, Val Acc: 0.9263\n",
            "Epoch [729/4000], Train Loss: 1.7177, Train Acc: 0.9207, Val Loss: 1.7214, Val Acc: 0.9212\n",
            "Epoch [730/4000], Train Loss: 1.7192, Train Acc: 0.9208, Val Loss: 1.7190, Val Acc: 0.9199\n",
            "Epoch [731/4000], Train Loss: 1.7189, Train Acc: 0.9202, Val Loss: 1.7160, Val Acc: 0.9237\n",
            "Epoch [732/4000], Train Loss: 1.7220, Train Acc: 0.9173, Val Loss: 1.7174, Val Acc: 0.9218\n",
            "Epoch [733/4000], Train Loss: 1.7183, Train Acc: 0.9228, Val Loss: 1.7234, Val Acc: 0.9154\n",
            "Epoch [734/4000], Train Loss: 1.7199, Train Acc: 0.9212, Val Loss: 1.7184, Val Acc: 0.9199\n",
            "Epoch [735/4000], Train Loss: 1.7181, Train Acc: 0.9213, Val Loss: 1.7221, Val Acc: 0.9154\n",
            "Epoch [736/4000], Train Loss: 1.7195, Train Acc: 0.9200, Val Loss: 1.7150, Val Acc: 0.9224\n",
            "Epoch [737/4000], Train Loss: 1.7185, Train Acc: 0.9208, Val Loss: 1.7206, Val Acc: 0.9237\n",
            "Epoch [738/4000], Train Loss: 1.7194, Train Acc: 0.9212, Val Loss: 1.7176, Val Acc: 0.9250\n",
            "Epoch [739/4000], Train Loss: 1.7186, Train Acc: 0.9197, Val Loss: 1.7185, Val Acc: 0.9237\n",
            "Epoch [740/4000], Train Loss: 1.7199, Train Acc: 0.9173, Val Loss: 1.7198, Val Acc: 0.9231\n",
            "Epoch [741/4000], Train Loss: 1.7207, Train Acc: 0.9191, Val Loss: 1.7208, Val Acc: 0.9212\n",
            "Epoch [742/4000], Train Loss: 1.7193, Train Acc: 0.9210, Val Loss: 1.7164, Val Acc: 0.9244\n",
            "Epoch [743/4000], Train Loss: 1.7170, Train Acc: 0.9220, Val Loss: 1.7176, Val Acc: 0.9237\n",
            "Epoch [744/4000], Train Loss: 1.7190, Train Acc: 0.9204, Val Loss: 1.7210, Val Acc: 0.9212\n",
            "Epoch [745/4000], Train Loss: 1.7176, Train Acc: 0.9212, Val Loss: 1.7242, Val Acc: 0.9179\n",
            "Epoch [746/4000], Train Loss: 1.7206, Train Acc: 0.9186, Val Loss: 1.7241, Val Acc: 0.9179\n",
            "Epoch [747/4000], Train Loss: 1.7177, Train Acc: 0.9202, Val Loss: 1.7197, Val Acc: 0.9250\n",
            "Epoch [748/4000], Train Loss: 1.7173, Train Acc: 0.9208, Val Loss: 1.7195, Val Acc: 0.9205\n",
            "Epoch [749/4000], Train Loss: 1.7193, Train Acc: 0.9197, Val Loss: 1.7253, Val Acc: 0.9186\n",
            "Epoch [750/4000], Train Loss: 1.7203, Train Acc: 0.9196, Val Loss: 1.7195, Val Acc: 0.9212\n",
            "Epoch [751/4000], Train Loss: 1.7218, Train Acc: 0.9189, Val Loss: 1.7192, Val Acc: 0.9205\n",
            "Epoch [752/4000], Train Loss: 1.7197, Train Acc: 0.9207, Val Loss: 1.7265, Val Acc: 0.9160\n",
            "Epoch [753/4000], Train Loss: 1.7188, Train Acc: 0.9213, Val Loss: 1.7325, Val Acc: 0.9038\n",
            "Epoch [754/4000], Train Loss: 1.7187, Train Acc: 0.9208, Val Loss: 1.7143, Val Acc: 0.9212\n",
            "Epoch [755/4000], Train Loss: 1.7172, Train Acc: 0.9220, Val Loss: 1.7213, Val Acc: 0.9173\n",
            "Epoch [756/4000], Train Loss: 1.7184, Train Acc: 0.9194, Val Loss: 1.7160, Val Acc: 0.9231\n",
            "Epoch [757/4000], Train Loss: 1.7169, Train Acc: 0.9210, Val Loss: 1.7229, Val Acc: 0.9167\n",
            "Epoch [758/4000], Train Loss: 1.7187, Train Acc: 0.9205, Val Loss: 1.7193, Val Acc: 0.9199\n",
            "Epoch [759/4000], Train Loss: 1.7181, Train Acc: 0.9212, Val Loss: 1.7193, Val Acc: 0.9218\n",
            "Epoch [760/4000], Train Loss: 1.7181, Train Acc: 0.9208, Val Loss: 1.7251, Val Acc: 0.9147\n",
            "Epoch [761/4000], Train Loss: 1.7179, Train Acc: 0.9212, Val Loss: 1.7218, Val Acc: 0.9186\n",
            "Epoch [762/4000], Train Loss: 1.7181, Train Acc: 0.9213, Val Loss: 1.7174, Val Acc: 0.9276\n",
            "Epoch [763/4000], Train Loss: 1.7176, Train Acc: 0.9216, Val Loss: 1.7236, Val Acc: 0.9173\n",
            "Epoch [764/4000], Train Loss: 1.7203, Train Acc: 0.9196, Val Loss: 1.7318, Val Acc: 0.9141\n",
            "Epoch [765/4000], Train Loss: 1.7199, Train Acc: 0.9192, Val Loss: 1.7203, Val Acc: 0.9212\n",
            "Epoch [766/4000], Train Loss: 1.7188, Train Acc: 0.9199, Val Loss: 1.7181, Val Acc: 0.9205\n",
            "Epoch [767/4000], Train Loss: 1.7186, Train Acc: 0.9194, Val Loss: 1.7230, Val Acc: 0.9173\n",
            "Epoch [768/4000], Train Loss: 1.7183, Train Acc: 0.9223, Val Loss: 1.7208, Val Acc: 0.9256\n",
            "Epoch [769/4000], Train Loss: 1.7183, Train Acc: 0.9205, Val Loss: 1.7223, Val Acc: 0.9135\n",
            "Epoch [770/4000], Train Loss: 1.7236, Train Acc: 0.9162, Val Loss: 1.7284, Val Acc: 0.9160\n",
            "Epoch [771/4000], Train Loss: 1.7214, Train Acc: 0.9184, Val Loss: 1.7194, Val Acc: 0.9224\n",
            "Epoch [772/4000], Train Loss: 1.7194, Train Acc: 0.9210, Val Loss: 1.7214, Val Acc: 0.9179\n",
            "Epoch [773/4000], Train Loss: 1.7184, Train Acc: 0.9207, Val Loss: 1.7191, Val Acc: 0.9269\n",
            "Epoch [774/4000], Train Loss: 1.7187, Train Acc: 0.9200, Val Loss: 1.7242, Val Acc: 0.9199\n",
            "Epoch [775/4000], Train Loss: 1.7183, Train Acc: 0.9212, Val Loss: 1.7223, Val Acc: 0.9173\n",
            "Epoch [776/4000], Train Loss: 1.7177, Train Acc: 0.9210, Val Loss: 1.7190, Val Acc: 0.9205\n",
            "Epoch [777/4000], Train Loss: 1.7206, Train Acc: 0.9187, Val Loss: 1.7172, Val Acc: 0.9199\n",
            "Epoch [778/4000], Train Loss: 1.7185, Train Acc: 0.9192, Val Loss: 1.7274, Val Acc: 0.9115\n",
            "Epoch [779/4000], Train Loss: 1.7182, Train Acc: 0.9197, Val Loss: 1.7163, Val Acc: 0.9186\n",
            "Epoch [780/4000], Train Loss: 1.7178, Train Acc: 0.9212, Val Loss: 1.7271, Val Acc: 0.9128\n",
            "Epoch [781/4000], Train Loss: 1.7194, Train Acc: 0.9207, Val Loss: 1.7257, Val Acc: 0.9160\n",
            "Epoch [782/4000], Train Loss: 1.7185, Train Acc: 0.9205, Val Loss: 1.7182, Val Acc: 0.9167\n",
            "Epoch [783/4000], Train Loss: 1.7185, Train Acc: 0.9208, Val Loss: 1.7241, Val Acc: 0.9173\n",
            "Epoch [784/4000], Train Loss: 1.7185, Train Acc: 0.9216, Val Loss: 1.7187, Val Acc: 0.9218\n",
            "Epoch [785/4000], Train Loss: 1.7173, Train Acc: 0.9205, Val Loss: 1.7196, Val Acc: 0.9147\n",
            "Epoch [786/4000], Train Loss: 1.7165, Train Acc: 0.9224, Val Loss: 1.7180, Val Acc: 0.9218\n",
            "Epoch [787/4000], Train Loss: 1.7175, Train Acc: 0.9231, Val Loss: 1.7229, Val Acc: 0.9167\n",
            "Epoch [788/4000], Train Loss: 1.7195, Train Acc: 0.9202, Val Loss: 1.7156, Val Acc: 0.9231\n",
            "Epoch [789/4000], Train Loss: 1.7204, Train Acc: 0.9192, Val Loss: 1.7192, Val Acc: 0.9212\n",
            "Epoch [790/4000], Train Loss: 1.7189, Train Acc: 0.9213, Val Loss: 1.7221, Val Acc: 0.9147\n",
            "Epoch [791/4000], Train Loss: 1.7187, Train Acc: 0.9197, Val Loss: 1.7190, Val Acc: 0.9212\n",
            "Epoch [792/4000], Train Loss: 1.7182, Train Acc: 0.9210, Val Loss: 1.7201, Val Acc: 0.9212\n",
            "Epoch [793/4000], Train Loss: 1.7258, Train Acc: 0.9160, Val Loss: 1.7262, Val Acc: 0.9154\n",
            "Epoch [794/4000], Train Loss: 1.7233, Train Acc: 0.9171, Val Loss: 1.7242, Val Acc: 0.9154\n",
            "Epoch [795/4000], Train Loss: 1.7164, Train Acc: 0.9224, Val Loss: 1.7155, Val Acc: 0.9212\n",
            "Epoch [796/4000], Train Loss: 1.7164, Train Acc: 0.9224, Val Loss: 1.7229, Val Acc: 0.9128\n",
            "Epoch [797/4000], Train Loss: 1.7167, Train Acc: 0.9215, Val Loss: 1.7171, Val Acc: 0.9244\n",
            "Epoch [798/4000], Train Loss: 1.7178, Train Acc: 0.9215, Val Loss: 1.7241, Val Acc: 0.9192\n",
            "Epoch [799/4000], Train Loss: 1.7197, Train Acc: 0.9197, Val Loss: 1.7219, Val Acc: 0.9244\n",
            "Epoch [800/4000], Train Loss: 1.7183, Train Acc: 0.9205, Val Loss: 1.7197, Val Acc: 0.9205\n",
            "Epoch [801/4000], Train Loss: 1.7171, Train Acc: 0.9232, Val Loss: 1.7171, Val Acc: 0.9179\n",
            "Epoch [802/4000], Train Loss: 1.7184, Train Acc: 0.9197, Val Loss: 1.7247, Val Acc: 0.9186\n",
            "Epoch [803/4000], Train Loss: 1.7184, Train Acc: 0.9208, Val Loss: 1.7190, Val Acc: 0.9192\n",
            "Epoch [804/4000], Train Loss: 1.7222, Train Acc: 0.9175, Val Loss: 1.7227, Val Acc: 0.9205\n",
            "Epoch [805/4000], Train Loss: 1.7185, Train Acc: 0.9212, Val Loss: 1.7139, Val Acc: 0.9250\n",
            "Epoch [806/4000], Train Loss: 1.7174, Train Acc: 0.9210, Val Loss: 1.7216, Val Acc: 0.9173\n",
            "Epoch [807/4000], Train Loss: 1.7173, Train Acc: 0.9212, Val Loss: 1.7205, Val Acc: 0.9250\n",
            "Epoch [808/4000], Train Loss: 1.7169, Train Acc: 0.9234, Val Loss: 1.7175, Val Acc: 0.9237\n",
            "Epoch [809/4000], Train Loss: 1.7213, Train Acc: 0.9173, Val Loss: 1.7281, Val Acc: 0.9128\n",
            "Epoch [810/4000], Train Loss: 1.7181, Train Acc: 0.9207, Val Loss: 1.7173, Val Acc: 0.9179\n",
            "Epoch [811/4000], Train Loss: 1.7176, Train Acc: 0.9216, Val Loss: 1.7175, Val Acc: 0.9192\n",
            "Epoch [812/4000], Train Loss: 1.7166, Train Acc: 0.9244, Val Loss: 1.7252, Val Acc: 0.9128\n",
            "Epoch [813/4000], Train Loss: 1.7177, Train Acc: 0.9220, Val Loss: 1.7282, Val Acc: 0.9205\n",
            "Epoch [814/4000], Train Loss: 1.7199, Train Acc: 0.9191, Val Loss: 1.7174, Val Acc: 0.9199\n",
            "Epoch [815/4000], Train Loss: 1.7207, Train Acc: 0.9207, Val Loss: 1.7153, Val Acc: 0.9250\n",
            "Epoch [816/4000], Train Loss: 1.7189, Train Acc: 0.9210, Val Loss: 1.7615, Val Acc: 0.8885\n",
            "Epoch [817/4000], Train Loss: 1.7196, Train Acc: 0.9200, Val Loss: 1.7171, Val Acc: 0.9237\n",
            "Epoch [818/4000], Train Loss: 1.7170, Train Acc: 0.9220, Val Loss: 1.7188, Val Acc: 0.9192\n",
            "Epoch [819/4000], Train Loss: 1.7185, Train Acc: 0.9213, Val Loss: 1.7197, Val Acc: 0.9154\n",
            "Epoch [820/4000], Train Loss: 1.7180, Train Acc: 0.9200, Val Loss: 1.7168, Val Acc: 0.9218\n",
            "Epoch [821/4000], Train Loss: 1.7164, Train Acc: 0.9221, Val Loss: 1.7209, Val Acc: 0.9160\n",
            "Epoch [822/4000], Train Loss: 1.7181, Train Acc: 0.9204, Val Loss: 1.7188, Val Acc: 0.9186\n",
            "Epoch [823/4000], Train Loss: 1.7192, Train Acc: 0.9212, Val Loss: 1.7192, Val Acc: 0.9173\n",
            "Epoch [824/4000], Train Loss: 1.7179, Train Acc: 0.9224, Val Loss: 1.7167, Val Acc: 0.9179\n",
            "Epoch [825/4000], Train Loss: 1.7187, Train Acc: 0.9197, Val Loss: 1.7246, Val Acc: 0.9083\n",
            "Epoch [826/4000], Train Loss: 1.7185, Train Acc: 0.9218, Val Loss: 1.7226, Val Acc: 0.9186\n",
            "Epoch [827/4000], Train Loss: 1.7168, Train Acc: 0.9245, Val Loss: 1.7162, Val Acc: 0.9224\n",
            "Epoch [828/4000], Train Loss: 1.7179, Train Acc: 0.9210, Val Loss: 1.7212, Val Acc: 0.9218\n",
            "Epoch [829/4000], Train Loss: 1.7188, Train Acc: 0.9218, Val Loss: 1.7189, Val Acc: 0.9212\n",
            "Epoch [830/4000], Train Loss: 1.7177, Train Acc: 0.9220, Val Loss: 1.7202, Val Acc: 0.9173\n",
            "Epoch [831/4000], Train Loss: 1.7176, Train Acc: 0.9207, Val Loss: 1.7197, Val Acc: 0.9205\n",
            "Epoch [832/4000], Train Loss: 1.7167, Train Acc: 0.9223, Val Loss: 1.7181, Val Acc: 0.9212\n",
            "Epoch [833/4000], Train Loss: 1.7176, Train Acc: 0.9223, Val Loss: 1.7257, Val Acc: 0.9173\n",
            "Epoch [834/4000], Train Loss: 1.7169, Train Acc: 0.9229, Val Loss: 1.7172, Val Acc: 0.9224\n",
            "Epoch [835/4000], Train Loss: 1.7184, Train Acc: 0.9212, Val Loss: 1.7251, Val Acc: 0.9186\n",
            "Epoch [836/4000], Train Loss: 1.7163, Train Acc: 0.9226, Val Loss: 1.7160, Val Acc: 0.9256\n",
            "Epoch [837/4000], Train Loss: 1.7189, Train Acc: 0.9205, Val Loss: 1.7266, Val Acc: 0.9122\n",
            "Epoch [838/4000], Train Loss: 1.7204, Train Acc: 0.9215, Val Loss: 1.7182, Val Acc: 0.9231\n",
            "Epoch [839/4000], Train Loss: 1.7185, Train Acc: 0.9221, Val Loss: 1.7263, Val Acc: 0.9167\n",
            "Epoch [840/4000], Train Loss: 1.7199, Train Acc: 0.9199, Val Loss: 1.7134, Val Acc: 0.9237\n",
            "Epoch [841/4000], Train Loss: 1.7167, Train Acc: 0.9226, Val Loss: 1.7147, Val Acc: 0.9269\n",
            "Epoch [842/4000], Train Loss: 1.7180, Train Acc: 0.9213, Val Loss: 1.7214, Val Acc: 0.9192\n",
            "Epoch [843/4000], Train Loss: 1.7180, Train Acc: 0.9218, Val Loss: 1.7187, Val Acc: 0.9224\n",
            "Epoch [844/4000], Train Loss: 1.7196, Train Acc: 0.9205, Val Loss: 1.7170, Val Acc: 0.9199\n",
            "Epoch [845/4000], Train Loss: 1.7202, Train Acc: 0.9191, Val Loss: 1.7192, Val Acc: 0.9154\n",
            "Epoch [846/4000], Train Loss: 1.7182, Train Acc: 0.9236, Val Loss: 1.7214, Val Acc: 0.9224\n",
            "Epoch [847/4000], Train Loss: 1.7180, Train Acc: 0.9189, Val Loss: 1.7189, Val Acc: 0.9186\n",
            "Epoch [848/4000], Train Loss: 1.7172, Train Acc: 0.9208, Val Loss: 1.7185, Val Acc: 0.9224\n",
            "Epoch [849/4000], Train Loss: 1.7165, Train Acc: 0.9224, Val Loss: 1.7166, Val Acc: 0.9224\n",
            "Epoch [850/4000], Train Loss: 1.7176, Train Acc: 0.9218, Val Loss: 1.7166, Val Acc: 0.9212\n",
            "Epoch [851/4000], Train Loss: 1.7182, Train Acc: 0.9220, Val Loss: 1.7162, Val Acc: 0.9205\n",
            "Epoch [852/4000], Train Loss: 1.7176, Train Acc: 0.9220, Val Loss: 1.7221, Val Acc: 0.9205\n",
            "Epoch [853/4000], Train Loss: 1.7175, Train Acc: 0.9207, Val Loss: 1.7165, Val Acc: 0.9218\n",
            "Epoch [854/4000], Train Loss: 1.7168, Train Acc: 0.9229, Val Loss: 1.7171, Val Acc: 0.9237\n",
            "Epoch [855/4000], Train Loss: 1.7199, Train Acc: 0.9200, Val Loss: 1.7166, Val Acc: 0.9173\n",
            "Epoch [856/4000], Train Loss: 1.7175, Train Acc: 0.9229, Val Loss: 1.7218, Val Acc: 0.9179\n",
            "Epoch [857/4000], Train Loss: 1.7179, Train Acc: 0.9218, Val Loss: 1.7177, Val Acc: 0.9192\n",
            "Epoch [858/4000], Train Loss: 1.7186, Train Acc: 0.9200, Val Loss: 1.7169, Val Acc: 0.9224\n",
            "Epoch [859/4000], Train Loss: 1.7159, Train Acc: 0.9231, Val Loss: 1.7204, Val Acc: 0.9212\n",
            "Epoch [860/4000], Train Loss: 1.7178, Train Acc: 0.9213, Val Loss: 1.7175, Val Acc: 0.9218\n",
            "Epoch [861/4000], Train Loss: 1.7166, Train Acc: 0.9237, Val Loss: 1.7213, Val Acc: 0.9192\n",
            "Epoch [862/4000], Train Loss: 1.7181, Train Acc: 0.9210, Val Loss: 1.7188, Val Acc: 0.9231\n",
            "Epoch [863/4000], Train Loss: 1.7192, Train Acc: 0.9200, Val Loss: 1.7188, Val Acc: 0.9224\n",
            "Epoch [864/4000], Train Loss: 1.7186, Train Acc: 0.9200, Val Loss: 1.7181, Val Acc: 0.9212\n",
            "Epoch [865/4000], Train Loss: 1.7177, Train Acc: 0.9207, Val Loss: 1.7198, Val Acc: 0.9192\n",
            "Epoch [866/4000], Train Loss: 1.7180, Train Acc: 0.9231, Val Loss: 1.7203, Val Acc: 0.9192\n",
            "Epoch [867/4000], Train Loss: 1.7186, Train Acc: 0.9213, Val Loss: 1.7207, Val Acc: 0.9167\n",
            "Epoch [868/4000], Train Loss: 1.7182, Train Acc: 0.9215, Val Loss: 1.7148, Val Acc: 0.9224\n",
            "Epoch [869/4000], Train Loss: 1.7184, Train Acc: 0.9223, Val Loss: 1.7165, Val Acc: 0.9237\n",
            "Epoch [870/4000], Train Loss: 1.7168, Train Acc: 0.9234, Val Loss: 1.7217, Val Acc: 0.9135\n",
            "Epoch [871/4000], Train Loss: 1.7178, Train Acc: 0.9215, Val Loss: 1.7143, Val Acc: 0.9218\n",
            "Epoch [872/4000], Train Loss: 1.7187, Train Acc: 0.9202, Val Loss: 1.7237, Val Acc: 0.9192\n",
            "Epoch [873/4000], Train Loss: 1.7176, Train Acc: 0.9229, Val Loss: 1.7194, Val Acc: 0.9186\n",
            "Epoch [874/4000], Train Loss: 1.7219, Train Acc: 0.9178, Val Loss: 1.7151, Val Acc: 0.9244\n",
            "Epoch [875/4000], Train Loss: 1.7163, Train Acc: 0.9236, Val Loss: 1.7172, Val Acc: 0.9224\n",
            "Epoch [876/4000], Train Loss: 1.7173, Train Acc: 0.9242, Val Loss: 1.7160, Val Acc: 0.9256\n",
            "Epoch [877/4000], Train Loss: 1.7172, Train Acc: 0.9215, Val Loss: 1.7193, Val Acc: 0.9231\n",
            "Epoch [878/4000], Train Loss: 1.7183, Train Acc: 0.9199, Val Loss: 1.7175, Val Acc: 0.9224\n",
            "Epoch [879/4000], Train Loss: 1.7164, Train Acc: 0.9232, Val Loss: 1.7206, Val Acc: 0.9160\n",
            "Epoch [880/4000], Train Loss: 1.7160, Train Acc: 0.9226, Val Loss: 1.7152, Val Acc: 0.9186\n",
            "Epoch [881/4000], Train Loss: 1.7163, Train Acc: 0.9221, Val Loss: 1.7229, Val Acc: 0.9237\n",
            "Epoch [882/4000], Train Loss: 1.7196, Train Acc: 0.9208, Val Loss: 1.7212, Val Acc: 0.9244\n",
            "Epoch [883/4000], Train Loss: 1.7192, Train Acc: 0.9216, Val Loss: 1.7144, Val Acc: 0.9244\n",
            "Epoch [884/4000], Train Loss: 1.7160, Train Acc: 0.9224, Val Loss: 1.7177, Val Acc: 0.9212\n",
            "Epoch [885/4000], Train Loss: 1.7152, Train Acc: 0.9226, Val Loss: 1.7213, Val Acc: 0.9154\n",
            "Epoch [886/4000], Train Loss: 1.7167, Train Acc: 0.9212, Val Loss: 1.7169, Val Acc: 0.9192\n",
            "Epoch [887/4000], Train Loss: 1.7211, Train Acc: 0.9200, Val Loss: 1.7145, Val Acc: 0.9263\n",
            "Epoch [888/4000], Train Loss: 1.7213, Train Acc: 0.9196, Val Loss: 1.7136, Val Acc: 0.9250\n",
            "Epoch [889/4000], Train Loss: 1.7166, Train Acc: 0.9231, Val Loss: 1.7275, Val Acc: 0.9077\n",
            "Epoch [890/4000], Train Loss: 1.7161, Train Acc: 0.9231, Val Loss: 1.7112, Val Acc: 0.9282\n",
            "Epoch [891/4000], Train Loss: 1.7171, Train Acc: 0.9232, Val Loss: 1.7173, Val Acc: 0.9250\n",
            "Epoch [892/4000], Train Loss: 1.7176, Train Acc: 0.9223, Val Loss: 1.7175, Val Acc: 0.9237\n",
            "Epoch [893/4000], Train Loss: 1.7187, Train Acc: 0.9205, Val Loss: 1.7188, Val Acc: 0.9173\n",
            "Epoch [894/4000], Train Loss: 1.7190, Train Acc: 0.9226, Val Loss: 1.7205, Val Acc: 0.9186\n",
            "Epoch [895/4000], Train Loss: 1.7167, Train Acc: 0.9228, Val Loss: 1.7162, Val Acc: 0.9237\n",
            "Epoch [896/4000], Train Loss: 1.7191, Train Acc: 0.9210, Val Loss: 1.7152, Val Acc: 0.9250\n",
            "Epoch [897/4000], Train Loss: 1.7158, Train Acc: 0.9242, Val Loss: 1.7217, Val Acc: 0.9135\n",
            "Epoch [898/4000], Train Loss: 1.7169, Train Acc: 0.9224, Val Loss: 1.7249, Val Acc: 0.9109\n",
            "Epoch [899/4000], Train Loss: 1.7169, Train Acc: 0.9229, Val Loss: 1.7146, Val Acc: 0.9212\n",
            "Epoch [900/4000], Train Loss: 1.7182, Train Acc: 0.9210, Val Loss: 1.7176, Val Acc: 0.9192\n",
            "Epoch [901/4000], Train Loss: 1.7178, Train Acc: 0.9187, Val Loss: 1.7138, Val Acc: 0.9263\n",
            "Epoch [902/4000], Train Loss: 1.7160, Train Acc: 0.9226, Val Loss: 1.7202, Val Acc: 0.9231\n",
            "Epoch [903/4000], Train Loss: 1.7177, Train Acc: 0.9234, Val Loss: 1.7146, Val Acc: 0.9179\n",
            "Epoch [904/4000], Train Loss: 1.7184, Train Acc: 0.9213, Val Loss: 1.7144, Val Acc: 0.9250\n",
            "Epoch [905/4000], Train Loss: 1.7177, Train Acc: 0.9210, Val Loss: 1.7139, Val Acc: 0.9231\n",
            "Epoch [906/4000], Train Loss: 1.7159, Train Acc: 0.9237, Val Loss: 1.7216, Val Acc: 0.9250\n",
            "Epoch [907/4000], Train Loss: 1.7178, Train Acc: 0.9204, Val Loss: 1.7142, Val Acc: 0.9231\n",
            "Epoch [908/4000], Train Loss: 1.7176, Train Acc: 0.9229, Val Loss: 1.7163, Val Acc: 0.9218\n",
            "Epoch [909/4000], Train Loss: 1.7179, Train Acc: 0.9212, Val Loss: 1.7141, Val Acc: 0.9256\n",
            "Epoch [910/4000], Train Loss: 1.7178, Train Acc: 0.9223, Val Loss: 1.7193, Val Acc: 0.9186\n",
            "Epoch [911/4000], Train Loss: 1.7163, Train Acc: 0.9234, Val Loss: 1.7162, Val Acc: 0.9237\n",
            "Epoch [912/4000], Train Loss: 1.7154, Train Acc: 0.9248, Val Loss: 1.7179, Val Acc: 0.9205\n",
            "Epoch [913/4000], Train Loss: 1.7169, Train Acc: 0.9228, Val Loss: 1.7223, Val Acc: 0.9122\n",
            "Epoch [914/4000], Train Loss: 1.7172, Train Acc: 0.9213, Val Loss: 1.7222, Val Acc: 0.9250\n",
            "Epoch [915/4000], Train Loss: 1.7179, Train Acc: 0.9226, Val Loss: 1.7267, Val Acc: 0.9103\n",
            "Epoch [916/4000], Train Loss: 1.7172, Train Acc: 0.9212, Val Loss: 1.7202, Val Acc: 0.9212\n",
            "Epoch [917/4000], Train Loss: 1.7178, Train Acc: 0.9228, Val Loss: 1.7313, Val Acc: 0.9135\n",
            "Epoch [918/4000], Train Loss: 1.7176, Train Acc: 0.9228, Val Loss: 1.7187, Val Acc: 0.9205\n",
            "Epoch [919/4000], Train Loss: 1.7172, Train Acc: 0.9221, Val Loss: 1.7171, Val Acc: 0.9199\n",
            "Epoch [920/4000], Train Loss: 1.7180, Train Acc: 0.9216, Val Loss: 1.7179, Val Acc: 0.9237\n",
            "Epoch [921/4000], Train Loss: 1.7158, Train Acc: 0.9247, Val Loss: 1.7145, Val Acc: 0.9269\n",
            "Epoch [922/4000], Train Loss: 1.7171, Train Acc: 0.9224, Val Loss: 1.7155, Val Acc: 0.9231\n",
            "Epoch [923/4000], Train Loss: 1.7180, Train Acc: 0.9216, Val Loss: 1.7342, Val Acc: 0.9045\n",
            "Epoch [924/4000], Train Loss: 1.7176, Train Acc: 0.9226, Val Loss: 1.7204, Val Acc: 0.9167\n",
            "Epoch [925/4000], Train Loss: 1.7155, Train Acc: 0.9232, Val Loss: 1.7199, Val Acc: 0.9237\n",
            "Epoch [926/4000], Train Loss: 1.7174, Train Acc: 0.9215, Val Loss: 1.7178, Val Acc: 0.9224\n",
            "Epoch [927/4000], Train Loss: 1.7153, Train Acc: 0.9250, Val Loss: 1.7163, Val Acc: 0.9224\n",
            "Epoch [928/4000], Train Loss: 1.7181, Train Acc: 0.9218, Val Loss: 1.7205, Val Acc: 0.9179\n",
            "Epoch [929/4000], Train Loss: 1.7162, Train Acc: 0.9220, Val Loss: 1.7222, Val Acc: 0.9147\n",
            "Epoch [930/4000], Train Loss: 1.7195, Train Acc: 0.9192, Val Loss: 1.7320, Val Acc: 0.9141\n",
            "Epoch [931/4000], Train Loss: 1.7176, Train Acc: 0.9224, Val Loss: 1.7202, Val Acc: 0.9192\n",
            "Epoch [932/4000], Train Loss: 1.7162, Train Acc: 0.9224, Val Loss: 1.7157, Val Acc: 0.9237\n",
            "Epoch [933/4000], Train Loss: 1.7186, Train Acc: 0.9223, Val Loss: 1.7152, Val Acc: 0.9186\n",
            "Epoch [934/4000], Train Loss: 1.7182, Train Acc: 0.9223, Val Loss: 1.7186, Val Acc: 0.9160\n",
            "Epoch [935/4000], Train Loss: 1.7147, Train Acc: 0.9253, Val Loss: 1.7249, Val Acc: 0.9224\n",
            "Epoch [936/4000], Train Loss: 1.7202, Train Acc: 0.9196, Val Loss: 1.7266, Val Acc: 0.9090\n",
            "Epoch [937/4000], Train Loss: 1.7179, Train Acc: 0.9212, Val Loss: 1.7184, Val Acc: 0.9192\n",
            "Epoch [938/4000], Train Loss: 1.7171, Train Acc: 0.9218, Val Loss: 1.7176, Val Acc: 0.9231\n",
            "Epoch [939/4000], Train Loss: 1.7179, Train Acc: 0.9236, Val Loss: 1.7136, Val Acc: 0.9288\n",
            "Epoch [940/4000], Train Loss: 1.7149, Train Acc: 0.9260, Val Loss: 1.7219, Val Acc: 0.9218\n",
            "Epoch [941/4000], Train Loss: 1.7152, Train Acc: 0.9229, Val Loss: 1.7169, Val Acc: 0.9218\n",
            "Epoch [942/4000], Train Loss: 1.7169, Train Acc: 0.9242, Val Loss: 1.7156, Val Acc: 0.9282\n",
            "Epoch [943/4000], Train Loss: 1.7169, Train Acc: 0.9242, Val Loss: 1.7175, Val Acc: 0.9186\n",
            "Epoch [944/4000], Train Loss: 1.7182, Train Acc: 0.9220, Val Loss: 1.7169, Val Acc: 0.9244\n",
            "Epoch [945/4000], Train Loss: 1.7167, Train Acc: 0.9228, Val Loss: 1.7221, Val Acc: 0.9212\n",
            "Epoch [946/4000], Train Loss: 1.7167, Train Acc: 0.9232, Val Loss: 1.7214, Val Acc: 0.9212\n",
            "Epoch [947/4000], Train Loss: 1.7178, Train Acc: 0.9210, Val Loss: 1.7212, Val Acc: 0.9186\n",
            "Epoch [948/4000], Train Loss: 1.7171, Train Acc: 0.9232, Val Loss: 1.7137, Val Acc: 0.9237\n",
            "Epoch [949/4000], Train Loss: 1.7183, Train Acc: 0.9212, Val Loss: 1.7137, Val Acc: 0.9263\n",
            "Epoch [950/4000], Train Loss: 1.7163, Train Acc: 0.9224, Val Loss: 1.7208, Val Acc: 0.9173\n",
            "Epoch [951/4000], Train Loss: 1.7176, Train Acc: 0.9213, Val Loss: 1.7154, Val Acc: 0.9218\n",
            "Epoch [952/4000], Train Loss: 1.7153, Train Acc: 0.9231, Val Loss: 1.7163, Val Acc: 0.9244\n",
            "Epoch [953/4000], Train Loss: 1.7186, Train Acc: 0.9207, Val Loss: 1.7168, Val Acc: 0.9250\n",
            "Epoch [954/4000], Train Loss: 1.7167, Train Acc: 0.9228, Val Loss: 1.7140, Val Acc: 0.9231\n",
            "Epoch [955/4000], Train Loss: 1.7163, Train Acc: 0.9234, Val Loss: 1.7141, Val Acc: 0.9244\n",
            "Epoch [956/4000], Train Loss: 1.7150, Train Acc: 0.9232, Val Loss: 1.7277, Val Acc: 0.9103\n",
            "Epoch [957/4000], Train Loss: 1.7179, Train Acc: 0.9216, Val Loss: 1.7269, Val Acc: 0.9173\n",
            "Epoch [958/4000], Train Loss: 1.7202, Train Acc: 0.9197, Val Loss: 1.7198, Val Acc: 0.9173\n",
            "Epoch [959/4000], Train Loss: 1.7180, Train Acc: 0.9234, Val Loss: 1.7185, Val Acc: 0.9212\n",
            "Epoch [960/4000], Train Loss: 1.7167, Train Acc: 0.9236, Val Loss: 1.7220, Val Acc: 0.9224\n",
            "Epoch [961/4000], Train Loss: 1.7157, Train Acc: 0.9224, Val Loss: 1.7176, Val Acc: 0.9173\n",
            "Epoch [962/4000], Train Loss: 1.7154, Train Acc: 0.9237, Val Loss: 1.7165, Val Acc: 0.9192\n",
            "Epoch [963/4000], Train Loss: 1.7152, Train Acc: 0.9250, Val Loss: 1.7219, Val Acc: 0.9212\n",
            "Epoch [964/4000], Train Loss: 1.7162, Train Acc: 0.9242, Val Loss: 1.7140, Val Acc: 0.9237\n",
            "Epoch [965/4000], Train Loss: 1.7181, Train Acc: 0.9208, Val Loss: 1.7153, Val Acc: 0.9256\n",
            "Epoch [966/4000], Train Loss: 1.7164, Train Acc: 0.9228, Val Loss: 1.7184, Val Acc: 0.9231\n",
            "Epoch [967/4000], Train Loss: 1.7163, Train Acc: 0.9221, Val Loss: 1.7186, Val Acc: 0.9231\n",
            "Epoch [968/4000], Train Loss: 1.7207, Train Acc: 0.9212, Val Loss: 1.7254, Val Acc: 0.9115\n",
            "Epoch [969/4000], Train Loss: 1.7186, Train Acc: 0.9213, Val Loss: 1.7201, Val Acc: 0.9205\n",
            "Epoch [970/4000], Train Loss: 1.7158, Train Acc: 0.9224, Val Loss: 1.7156, Val Acc: 0.9250\n",
            "Epoch [971/4000], Train Loss: 1.7151, Train Acc: 0.9237, Val Loss: 1.7260, Val Acc: 0.9141\n",
            "Epoch [972/4000], Train Loss: 1.7146, Train Acc: 0.9252, Val Loss: 1.7176, Val Acc: 0.9256\n",
            "Epoch [973/4000], Train Loss: 1.7173, Train Acc: 0.9236, Val Loss: 1.7173, Val Acc: 0.9218\n",
            "Epoch [974/4000], Train Loss: 1.7191, Train Acc: 0.9213, Val Loss: 1.7120, Val Acc: 0.9256\n",
            "Epoch [975/4000], Train Loss: 1.7165, Train Acc: 0.9237, Val Loss: 1.7167, Val Acc: 0.9192\n",
            "Epoch [976/4000], Train Loss: 1.7167, Train Acc: 0.9207, Val Loss: 1.7141, Val Acc: 0.9231\n",
            "Epoch [977/4000], Train Loss: 1.7163, Train Acc: 0.9234, Val Loss: 1.7140, Val Acc: 0.9231\n",
            "Epoch [978/4000], Train Loss: 1.7159, Train Acc: 0.9234, Val Loss: 1.7175, Val Acc: 0.9218\n",
            "Epoch [979/4000], Train Loss: 1.7166, Train Acc: 0.9232, Val Loss: 1.7167, Val Acc: 0.9237\n",
            "Epoch [980/4000], Train Loss: 1.7173, Train Acc: 0.9218, Val Loss: 1.7221, Val Acc: 0.9244\n",
            "Epoch [981/4000], Train Loss: 1.7187, Train Acc: 0.9205, Val Loss: 1.7124, Val Acc: 0.9250\n",
            "Epoch [982/4000], Train Loss: 1.7161, Train Acc: 0.9239, Val Loss: 1.7198, Val Acc: 0.9212\n",
            "Epoch [983/4000], Train Loss: 1.7178, Train Acc: 0.9205, Val Loss: 1.7190, Val Acc: 0.9192\n",
            "Epoch [984/4000], Train Loss: 1.7160, Train Acc: 0.9226, Val Loss: 1.7186, Val Acc: 0.9173\n",
            "Epoch [985/4000], Train Loss: 1.7180, Train Acc: 0.9215, Val Loss: 1.7165, Val Acc: 0.9231\n",
            "Epoch [986/4000], Train Loss: 1.7177, Train Acc: 0.9232, Val Loss: 1.7290, Val Acc: 0.9077\n",
            "Epoch [987/4000], Train Loss: 1.7151, Train Acc: 0.9223, Val Loss: 1.7177, Val Acc: 0.9224\n",
            "Epoch [988/4000], Train Loss: 1.7179, Train Acc: 0.9221, Val Loss: 1.7214, Val Acc: 0.9237\n",
            "Epoch [989/4000], Train Loss: 1.7163, Train Acc: 0.9231, Val Loss: 1.7152, Val Acc: 0.9263\n",
            "Epoch [990/4000], Train Loss: 1.7177, Train Acc: 0.9221, Val Loss: 1.7289, Val Acc: 0.9071\n",
            "Epoch [991/4000], Train Loss: 1.7155, Train Acc: 0.9240, Val Loss: 1.7123, Val Acc: 0.9250\n",
            "Epoch [992/4000], Train Loss: 1.7171, Train Acc: 0.9220, Val Loss: 1.7199, Val Acc: 0.9237\n",
            "Epoch [993/4000], Train Loss: 1.7178, Train Acc: 0.9187, Val Loss: 1.7192, Val Acc: 0.9167\n",
            "Epoch [994/4000], Train Loss: 1.7169, Train Acc: 0.9232, Val Loss: 1.7131, Val Acc: 0.9244\n",
            "Epoch [995/4000], Train Loss: 1.7153, Train Acc: 0.9234, Val Loss: 1.7256, Val Acc: 0.9173\n",
            "Epoch [996/4000], Train Loss: 1.7174, Train Acc: 0.9232, Val Loss: 1.7159, Val Acc: 0.9269\n",
            "Epoch [997/4000], Train Loss: 1.7156, Train Acc: 0.9245, Val Loss: 1.7194, Val Acc: 0.9218\n",
            "Epoch [998/4000], Train Loss: 1.7174, Train Acc: 0.9220, Val Loss: 1.7270, Val Acc: 0.9167\n",
            "Epoch [999/4000], Train Loss: 1.7165, Train Acc: 0.9234, Val Loss: 1.7189, Val Acc: 0.9250\n",
            "Epoch [1000/4000], Train Loss: 1.7157, Train Acc: 0.9234, Val Loss: 1.7136, Val Acc: 0.9276\n",
            "Epoch [1001/4000], Train Loss: 1.7159, Train Acc: 0.9231, Val Loss: 1.7147, Val Acc: 0.9256\n",
            "Epoch [1002/4000], Train Loss: 1.7160, Train Acc: 0.9248, Val Loss: 1.7107, Val Acc: 0.9269\n",
            "Epoch [1003/4000], Train Loss: 1.7168, Train Acc: 0.9229, Val Loss: 1.7178, Val Acc: 0.9218\n",
            "Epoch [1004/4000], Train Loss: 1.7174, Train Acc: 0.9220, Val Loss: 1.7149, Val Acc: 0.9282\n",
            "Epoch [1005/4000], Train Loss: 1.7181, Train Acc: 0.9218, Val Loss: 1.7143, Val Acc: 0.9237\n",
            "Epoch [1006/4000], Train Loss: 1.7182, Train Acc: 0.9205, Val Loss: 1.7328, Val Acc: 0.9122\n",
            "Epoch [1007/4000], Train Loss: 1.7179, Train Acc: 0.9224, Val Loss: 1.7130, Val Acc: 0.9231\n",
            "Epoch [1008/4000], Train Loss: 1.7155, Train Acc: 0.9248, Val Loss: 1.7148, Val Acc: 0.9276\n",
            "Epoch [1009/4000], Train Loss: 1.7148, Train Acc: 0.9250, Val Loss: 1.7158, Val Acc: 0.9263\n",
            "Epoch [1010/4000], Train Loss: 1.7175, Train Acc: 0.9220, Val Loss: 1.7238, Val Acc: 0.9179\n",
            "Epoch [1011/4000], Train Loss: 1.7173, Train Acc: 0.9223, Val Loss: 1.7216, Val Acc: 0.9160\n",
            "Epoch [1012/4000], Train Loss: 1.7175, Train Acc: 0.9216, Val Loss: 1.7247, Val Acc: 0.9218\n",
            "Epoch [1013/4000], Train Loss: 1.7157, Train Acc: 0.9240, Val Loss: 1.7148, Val Acc: 0.9244\n",
            "Epoch [1014/4000], Train Loss: 1.7182, Train Acc: 0.9212, Val Loss: 1.7222, Val Acc: 0.9199\n",
            "Epoch [1015/4000], Train Loss: 1.7185, Train Acc: 0.9221, Val Loss: 1.7134, Val Acc: 0.9224\n",
            "Epoch [1016/4000], Train Loss: 1.7150, Train Acc: 0.9236, Val Loss: 1.7145, Val Acc: 0.9269\n",
            "Epoch [1017/4000], Train Loss: 1.7171, Train Acc: 0.9242, Val Loss: 1.7148, Val Acc: 0.9212\n",
            "Epoch [1018/4000], Train Loss: 1.7181, Train Acc: 0.9220, Val Loss: 1.7118, Val Acc: 0.9231\n",
            "Epoch [1019/4000], Train Loss: 1.7170, Train Acc: 0.9237, Val Loss: 1.7136, Val Acc: 0.9244\n",
            "Epoch [1020/4000], Train Loss: 1.7173, Train Acc: 0.9234, Val Loss: 1.7231, Val Acc: 0.9218\n",
            "Epoch [1021/4000], Train Loss: 1.7170, Train Acc: 0.9239, Val Loss: 1.7192, Val Acc: 0.9218\n",
            "Epoch [1022/4000], Train Loss: 1.7172, Train Acc: 0.9226, Val Loss: 1.7237, Val Acc: 0.9147\n",
            "Epoch [1023/4000], Train Loss: 1.7162, Train Acc: 0.9224, Val Loss: 1.7166, Val Acc: 0.9212\n",
            "Epoch [1024/4000], Train Loss: 1.7168, Train Acc: 0.9232, Val Loss: 1.7162, Val Acc: 0.9250\n",
            "Epoch [1025/4000], Train Loss: 1.7159, Train Acc: 0.9234, Val Loss: 1.7152, Val Acc: 0.9256\n",
            "Epoch [1026/4000], Train Loss: 1.7153, Train Acc: 0.9237, Val Loss: 1.7188, Val Acc: 0.9224\n",
            "Epoch [1027/4000], Train Loss: 1.7159, Train Acc: 0.9232, Val Loss: 1.7130, Val Acc: 0.9269\n",
            "Epoch [1028/4000], Train Loss: 1.7174, Train Acc: 0.9221, Val Loss: 1.7193, Val Acc: 0.9218\n",
            "Epoch [1029/4000], Train Loss: 1.7186, Train Acc: 0.9213, Val Loss: 1.7183, Val Acc: 0.9205\n",
            "Epoch [1030/4000], Train Loss: 1.7173, Train Acc: 0.9231, Val Loss: 1.7173, Val Acc: 0.9237\n",
            "Epoch [1031/4000], Train Loss: 1.7164, Train Acc: 0.9229, Val Loss: 1.7168, Val Acc: 0.9205\n",
            "Epoch [1032/4000], Train Loss: 1.7170, Train Acc: 0.9237, Val Loss: 1.7147, Val Acc: 0.9224\n",
            "Epoch [1033/4000], Train Loss: 1.7165, Train Acc: 0.9234, Val Loss: 1.7214, Val Acc: 0.9199\n",
            "Epoch [1034/4000], Train Loss: 1.7170, Train Acc: 0.9223, Val Loss: 1.7174, Val Acc: 0.9250\n",
            "Epoch [1035/4000], Train Loss: 1.7157, Train Acc: 0.9232, Val Loss: 1.7137, Val Acc: 0.9269\n",
            "Epoch [1036/4000], Train Loss: 1.7159, Train Acc: 0.9229, Val Loss: 1.7261, Val Acc: 0.9090\n",
            "Epoch [1037/4000], Train Loss: 1.7204, Train Acc: 0.9194, Val Loss: 1.7168, Val Acc: 0.9192\n",
            "Epoch [1038/4000], Train Loss: 1.7155, Train Acc: 0.9226, Val Loss: 1.7192, Val Acc: 0.9167\n",
            "Epoch [1039/4000], Train Loss: 1.7171, Train Acc: 0.9224, Val Loss: 1.7164, Val Acc: 0.9250\n",
            "Epoch [1040/4000], Train Loss: 1.7165, Train Acc: 0.9224, Val Loss: 1.7176, Val Acc: 0.9256\n",
            "Epoch [1041/4000], Train Loss: 1.7185, Train Acc: 0.9215, Val Loss: 1.7184, Val Acc: 0.9212\n",
            "Epoch [1042/4000], Train Loss: 1.7156, Train Acc: 0.9229, Val Loss: 1.7164, Val Acc: 0.9231\n",
            "Epoch [1043/4000], Train Loss: 1.7151, Train Acc: 0.9242, Val Loss: 1.7163, Val Acc: 0.9231\n",
            "Epoch [1044/4000], Train Loss: 1.7166, Train Acc: 0.9229, Val Loss: 1.7280, Val Acc: 0.9115\n",
            "Epoch [1045/4000], Train Loss: 1.7153, Train Acc: 0.9226, Val Loss: 1.7188, Val Acc: 0.9237\n",
            "Epoch [1046/4000], Train Loss: 1.7176, Train Acc: 0.9226, Val Loss: 1.7229, Val Acc: 0.9135\n",
            "Epoch [1047/4000], Train Loss: 1.7163, Train Acc: 0.9229, Val Loss: 1.7148, Val Acc: 0.9276\n",
            "Epoch [1048/4000], Train Loss: 1.7150, Train Acc: 0.9234, Val Loss: 1.7149, Val Acc: 0.9224\n",
            "Epoch [1049/4000], Train Loss: 1.7169, Train Acc: 0.9232, Val Loss: 1.7155, Val Acc: 0.9212\n",
            "Epoch [1050/4000], Train Loss: 1.7159, Train Acc: 0.9240, Val Loss: 1.7135, Val Acc: 0.9263\n",
            "Epoch [1051/4000], Train Loss: 1.7169, Train Acc: 0.9215, Val Loss: 1.7183, Val Acc: 0.9218\n",
            "Epoch [1052/4000], Train Loss: 1.7152, Train Acc: 0.9229, Val Loss: 1.7178, Val Acc: 0.9192\n",
            "Epoch [1053/4000], Train Loss: 1.7196, Train Acc: 0.9202, Val Loss: 1.7208, Val Acc: 0.9250\n",
            "Epoch [1054/4000], Train Loss: 1.7139, Train Acc: 0.9256, Val Loss: 1.7150, Val Acc: 0.9269\n",
            "Epoch [1055/4000], Train Loss: 1.7152, Train Acc: 0.9245, Val Loss: 1.7239, Val Acc: 0.9173\n",
            "Epoch [1056/4000], Train Loss: 1.7167, Train Acc: 0.9220, Val Loss: 1.7159, Val Acc: 0.9231\n",
            "Epoch [1057/4000], Train Loss: 1.7183, Train Acc: 0.9207, Val Loss: 1.7159, Val Acc: 0.9231\n",
            "Epoch [1058/4000], Train Loss: 1.7163, Train Acc: 0.9226, Val Loss: 1.7225, Val Acc: 0.9192\n",
            "Epoch [1059/4000], Train Loss: 1.7158, Train Acc: 0.9239, Val Loss: 1.7145, Val Acc: 0.9263\n",
            "Epoch [1060/4000], Train Loss: 1.7154, Train Acc: 0.9255, Val Loss: 1.7110, Val Acc: 0.9250\n",
            "Epoch [1061/4000], Train Loss: 1.7149, Train Acc: 0.9237, Val Loss: 1.7141, Val Acc: 0.9295\n",
            "Epoch [1062/4000], Train Loss: 1.7145, Train Acc: 0.9253, Val Loss: 1.7130, Val Acc: 0.9282\n",
            "Epoch [1063/4000], Train Loss: 1.7150, Train Acc: 0.9242, Val Loss: 1.7167, Val Acc: 0.9256\n",
            "Epoch [1064/4000], Train Loss: 1.7153, Train Acc: 0.9242, Val Loss: 1.7118, Val Acc: 0.9269\n",
            "Epoch [1065/4000], Train Loss: 1.7158, Train Acc: 0.9226, Val Loss: 1.7151, Val Acc: 0.9269\n",
            "Epoch [1066/4000], Train Loss: 1.7165, Train Acc: 0.9221, Val Loss: 1.7183, Val Acc: 0.9218\n",
            "Epoch [1067/4000], Train Loss: 1.7166, Train Acc: 0.9237, Val Loss: 1.7255, Val Acc: 0.9263\n",
            "Epoch [1068/4000], Train Loss: 1.7158, Train Acc: 0.9237, Val Loss: 1.7190, Val Acc: 0.9205\n",
            "Epoch [1069/4000], Train Loss: 1.7152, Train Acc: 0.9239, Val Loss: 1.7322, Val Acc: 0.9154\n",
            "Epoch [1070/4000], Train Loss: 1.7164, Train Acc: 0.9226, Val Loss: 1.7118, Val Acc: 0.9269\n",
            "Epoch [1071/4000], Train Loss: 1.7154, Train Acc: 0.9234, Val Loss: 1.7175, Val Acc: 0.9167\n",
            "Epoch [1072/4000], Train Loss: 1.7173, Train Acc: 0.9232, Val Loss: 1.7275, Val Acc: 0.9147\n",
            "Epoch [1073/4000], Train Loss: 1.7160, Train Acc: 0.9242, Val Loss: 1.7197, Val Acc: 0.9199\n",
            "Epoch [1074/4000], Train Loss: 1.7152, Train Acc: 0.9245, Val Loss: 1.7173, Val Acc: 0.9231\n",
            "Epoch [1075/4000], Train Loss: 1.7164, Train Acc: 0.9231, Val Loss: 1.7158, Val Acc: 0.9237\n",
            "Epoch [1076/4000], Train Loss: 1.7156, Train Acc: 0.9245, Val Loss: 1.7193, Val Acc: 0.9231\n",
            "Epoch [1077/4000], Train Loss: 1.7159, Train Acc: 0.9223, Val Loss: 1.7148, Val Acc: 0.9263\n",
            "Epoch [1078/4000], Train Loss: 1.7157, Train Acc: 0.9229, Val Loss: 1.7144, Val Acc: 0.9224\n",
            "Epoch [1079/4000], Train Loss: 1.7155, Train Acc: 0.9239, Val Loss: 1.7160, Val Acc: 0.9256\n",
            "Epoch [1080/4000], Train Loss: 1.7156, Train Acc: 0.9247, Val Loss: 1.7201, Val Acc: 0.9231\n",
            "Epoch [1081/4000], Train Loss: 1.7159, Train Acc: 0.9229, Val Loss: 1.7167, Val Acc: 0.9250\n",
            "Epoch [1082/4000], Train Loss: 1.7166, Train Acc: 0.9224, Val Loss: 1.7117, Val Acc: 0.9256\n",
            "Epoch [1083/4000], Train Loss: 1.7145, Train Acc: 0.9245, Val Loss: 1.7219, Val Acc: 0.9199\n",
            "Epoch [1084/4000], Train Loss: 1.7219, Train Acc: 0.9179, Val Loss: 1.7200, Val Acc: 0.9237\n",
            "Epoch [1085/4000], Train Loss: 1.7154, Train Acc: 0.9247, Val Loss: 1.7120, Val Acc: 0.9250\n",
            "Epoch [1086/4000], Train Loss: 1.7145, Train Acc: 0.9236, Val Loss: 1.7183, Val Acc: 0.9173\n",
            "Epoch [1087/4000], Train Loss: 1.7170, Train Acc: 0.9218, Val Loss: 1.7122, Val Acc: 0.9224\n",
            "Epoch [1088/4000], Train Loss: 1.7158, Train Acc: 0.9231, Val Loss: 1.7188, Val Acc: 0.9231\n",
            "Epoch [1089/4000], Train Loss: 1.7161, Train Acc: 0.9242, Val Loss: 1.7141, Val Acc: 0.9269\n",
            "Epoch [1090/4000], Train Loss: 1.7154, Train Acc: 0.9237, Val Loss: 1.7185, Val Acc: 0.9237\n",
            "Epoch [1091/4000], Train Loss: 1.7165, Train Acc: 0.9236, Val Loss: 1.7217, Val Acc: 0.9179\n",
            "Epoch [1092/4000], Train Loss: 1.7155, Train Acc: 0.9226, Val Loss: 1.7179, Val Acc: 0.9212\n",
            "Epoch [1093/4000], Train Loss: 1.7183, Train Acc: 0.9224, Val Loss: 1.7217, Val Acc: 0.9186\n",
            "Epoch [1094/4000], Train Loss: 1.7177, Train Acc: 0.9213, Val Loss: 1.7150, Val Acc: 0.9276\n",
            "Epoch [1095/4000], Train Loss: 1.7154, Train Acc: 0.9231, Val Loss: 1.7164, Val Acc: 0.9231\n",
            "Epoch [1096/4000], Train Loss: 1.7158, Train Acc: 0.9242, Val Loss: 1.7164, Val Acc: 0.9212\n",
            "Epoch [1097/4000], Train Loss: 1.7151, Train Acc: 0.9226, Val Loss: 1.7157, Val Acc: 0.9269\n",
            "Epoch [1098/4000], Train Loss: 1.7161, Train Acc: 0.9240, Val Loss: 1.7154, Val Acc: 0.9263\n",
            "Epoch [1099/4000], Train Loss: 1.7169, Train Acc: 0.9236, Val Loss: 1.7278, Val Acc: 0.9173\n",
            "Epoch [1100/4000], Train Loss: 1.7198, Train Acc: 0.9194, Val Loss: 1.7183, Val Acc: 0.9269\n",
            "Epoch [1101/4000], Train Loss: 1.7156, Train Acc: 0.9244, Val Loss: 1.7145, Val Acc: 0.9212\n",
            "Epoch [1102/4000], Train Loss: 1.7136, Train Acc: 0.9261, Val Loss: 1.7149, Val Acc: 0.9212\n",
            "Epoch [1103/4000], Train Loss: 1.7147, Train Acc: 0.9240, Val Loss: 1.7224, Val Acc: 0.9179\n",
            "Epoch [1104/4000], Train Loss: 1.7157, Train Acc: 0.9244, Val Loss: 1.7135, Val Acc: 0.9269\n",
            "Epoch [1105/4000], Train Loss: 1.7163, Train Acc: 0.9224, Val Loss: 1.7152, Val Acc: 0.9231\n",
            "Epoch [1106/4000], Train Loss: 1.7149, Train Acc: 0.9237, Val Loss: 1.7141, Val Acc: 0.9263\n",
            "Epoch [1107/4000], Train Loss: 1.7164, Train Acc: 0.9234, Val Loss: 1.7126, Val Acc: 0.9288\n",
            "Epoch [1108/4000], Train Loss: 1.7146, Train Acc: 0.9237, Val Loss: 1.7200, Val Acc: 0.9224\n",
            "Epoch [1109/4000], Train Loss: 1.7161, Train Acc: 0.9231, Val Loss: 1.7147, Val Acc: 0.9244\n",
            "Epoch [1110/4000], Train Loss: 1.7150, Train Acc: 0.9239, Val Loss: 1.7171, Val Acc: 0.9237\n",
            "Epoch [1111/4000], Train Loss: 1.7164, Train Acc: 0.9236, Val Loss: 1.7222, Val Acc: 0.9186\n",
            "Epoch [1112/4000], Train Loss: 1.7151, Train Acc: 0.9234, Val Loss: 1.7142, Val Acc: 0.9237\n",
            "Epoch [1113/4000], Train Loss: 1.7151, Train Acc: 0.9250, Val Loss: 1.7182, Val Acc: 0.9256\n",
            "Epoch [1114/4000], Train Loss: 1.7160, Train Acc: 0.9248, Val Loss: 1.7175, Val Acc: 0.9231\n",
            "Epoch [1115/4000], Train Loss: 1.7149, Train Acc: 0.9250, Val Loss: 1.7181, Val Acc: 0.9237\n",
            "Epoch [1116/4000], Train Loss: 1.7176, Train Acc: 0.9231, Val Loss: 1.7152, Val Acc: 0.9263\n",
            "Epoch [1117/4000], Train Loss: 1.7181, Train Acc: 0.9228, Val Loss: 1.7169, Val Acc: 0.9224\n",
            "Epoch [1118/4000], Train Loss: 1.7168, Train Acc: 0.9223, Val Loss: 1.7161, Val Acc: 0.9199\n",
            "Epoch [1119/4000], Train Loss: 1.7157, Train Acc: 0.9239, Val Loss: 1.7138, Val Acc: 0.9231\n",
            "Epoch [1120/4000], Train Loss: 1.7157, Train Acc: 0.9239, Val Loss: 1.7146, Val Acc: 0.9231\n",
            "Epoch [1121/4000], Train Loss: 1.7165, Train Acc: 0.9218, Val Loss: 1.7283, Val Acc: 0.9154\n",
            "Epoch [1122/4000], Train Loss: 1.7152, Train Acc: 0.9240, Val Loss: 1.7238, Val Acc: 0.9199\n",
            "Epoch [1123/4000], Train Loss: 1.7159, Train Acc: 0.9223, Val Loss: 1.7121, Val Acc: 0.9256\n",
            "Epoch [1124/4000], Train Loss: 1.7175, Train Acc: 0.9221, Val Loss: 1.7149, Val Acc: 0.9205\n",
            "Epoch [1125/4000], Train Loss: 1.7156, Train Acc: 0.9244, Val Loss: 1.7129, Val Acc: 0.9301\n",
            "Epoch [1126/4000], Train Loss: 1.7164, Train Acc: 0.9215, Val Loss: 1.7127, Val Acc: 0.9256\n",
            "Epoch [1127/4000], Train Loss: 1.7173, Train Acc: 0.9240, Val Loss: 1.7264, Val Acc: 0.9167\n",
            "Epoch [1128/4000], Train Loss: 1.7161, Train Acc: 0.9234, Val Loss: 1.7201, Val Acc: 0.9224\n",
            "Epoch [1129/4000], Train Loss: 1.7170, Train Acc: 0.9240, Val Loss: 1.7157, Val Acc: 0.9212\n",
            "Epoch [1130/4000], Train Loss: 1.7154, Train Acc: 0.9236, Val Loss: 1.7126, Val Acc: 0.9237\n",
            "Epoch [1131/4000], Train Loss: 1.7152, Train Acc: 0.9229, Val Loss: 1.7226, Val Acc: 0.9224\n",
            "Epoch [1132/4000], Train Loss: 1.7164, Train Acc: 0.9248, Val Loss: 1.7195, Val Acc: 0.9231\n",
            "Epoch [1133/4000], Train Loss: 1.7160, Train Acc: 0.9228, Val Loss: 1.7106, Val Acc: 0.9263\n",
            "Epoch [1134/4000], Train Loss: 1.7152, Train Acc: 0.9244, Val Loss: 1.7165, Val Acc: 0.9199\n",
            "Epoch [1135/4000], Train Loss: 1.7150, Train Acc: 0.9240, Val Loss: 1.7156, Val Acc: 0.9263\n",
            "Epoch [1136/4000], Train Loss: 1.7148, Train Acc: 0.9236, Val Loss: 1.7206, Val Acc: 0.9218\n",
            "Epoch [1137/4000], Train Loss: 1.7171, Train Acc: 0.9224, Val Loss: 1.7174, Val Acc: 0.9231\n",
            "Epoch [1138/4000], Train Loss: 1.7152, Train Acc: 0.9245, Val Loss: 1.7147, Val Acc: 0.9244\n",
            "Epoch [1139/4000], Train Loss: 1.7149, Train Acc: 0.9248, Val Loss: 1.7146, Val Acc: 0.9250\n",
            "Epoch [1140/4000], Train Loss: 1.7156, Train Acc: 0.9224, Val Loss: 1.7161, Val Acc: 0.9244\n",
            "Epoch [1141/4000], Train Loss: 1.7148, Train Acc: 0.9240, Val Loss: 1.7155, Val Acc: 0.9224\n",
            "Epoch [1142/4000], Train Loss: 1.7174, Train Acc: 0.9229, Val Loss: 1.7162, Val Acc: 0.9218\n",
            "Epoch [1143/4000], Train Loss: 1.7157, Train Acc: 0.9234, Val Loss: 1.7207, Val Acc: 0.9167\n",
            "Epoch [1144/4000], Train Loss: 1.7189, Train Acc: 0.9208, Val Loss: 1.7137, Val Acc: 0.9288\n",
            "Epoch [1145/4000], Train Loss: 1.7151, Train Acc: 0.9244, Val Loss: 1.7189, Val Acc: 0.9205\n",
            "Epoch [1146/4000], Train Loss: 1.7142, Train Acc: 0.9242, Val Loss: 1.7124, Val Acc: 0.9224\n",
            "Epoch [1147/4000], Train Loss: 1.7181, Train Acc: 0.9228, Val Loss: 1.7112, Val Acc: 0.9237\n",
            "Epoch [1148/4000], Train Loss: 1.7155, Train Acc: 0.9228, Val Loss: 1.7235, Val Acc: 0.9250\n",
            "Epoch [1149/4000], Train Loss: 1.7176, Train Acc: 0.9223, Val Loss: 1.7211, Val Acc: 0.9199\n",
            "Epoch [1150/4000], Train Loss: 1.7162, Train Acc: 0.9240, Val Loss: 1.7114, Val Acc: 0.9282\n",
            "Epoch [1151/4000], Train Loss: 1.7153, Train Acc: 0.9255, Val Loss: 1.7138, Val Acc: 0.9244\n",
            "Epoch [1152/4000], Train Loss: 1.7165, Train Acc: 0.9232, Val Loss: 1.7115, Val Acc: 0.9276\n",
            "Epoch [1153/4000], Train Loss: 1.7157, Train Acc: 0.9240, Val Loss: 1.7131, Val Acc: 0.9218\n",
            "Epoch [1154/4000], Train Loss: 1.7183, Train Acc: 0.9221, Val Loss: 1.7109, Val Acc: 0.9224\n",
            "Epoch [1155/4000], Train Loss: 1.7158, Train Acc: 0.9232, Val Loss: 1.7113, Val Acc: 0.9256\n",
            "Epoch [1156/4000], Train Loss: 1.7140, Train Acc: 0.9250, Val Loss: 1.7136, Val Acc: 0.9250\n",
            "Epoch [1157/4000], Train Loss: 1.7146, Train Acc: 0.9253, Val Loss: 1.7203, Val Acc: 0.9199\n",
            "Epoch [1158/4000], Train Loss: 1.7168, Train Acc: 0.9236, Val Loss: 1.7181, Val Acc: 0.9224\n",
            "Epoch [1159/4000], Train Loss: 1.7166, Train Acc: 0.9237, Val Loss: 1.7137, Val Acc: 0.9250\n",
            "Epoch [1160/4000], Train Loss: 1.7149, Train Acc: 0.9239, Val Loss: 1.7139, Val Acc: 0.9276\n",
            "Epoch [1161/4000], Train Loss: 1.7156, Train Acc: 0.9224, Val Loss: 1.7134, Val Acc: 0.9250\n",
            "Epoch [1162/4000], Train Loss: 1.7146, Train Acc: 0.9242, Val Loss: 1.7105, Val Acc: 0.9256\n",
            "Epoch [1163/4000], Train Loss: 1.7154, Train Acc: 0.9244, Val Loss: 1.7158, Val Acc: 0.9224\n",
            "Epoch [1164/4000], Train Loss: 1.7158, Train Acc: 0.9228, Val Loss: 1.7155, Val Acc: 0.9269\n",
            "Epoch [1165/4000], Train Loss: 1.7153, Train Acc: 0.9250, Val Loss: 1.7215, Val Acc: 0.9212\n",
            "Epoch [1166/4000], Train Loss: 1.7153, Train Acc: 0.9231, Val Loss: 1.7243, Val Acc: 0.9192\n",
            "Epoch [1167/4000], Train Loss: 1.7153, Train Acc: 0.9236, Val Loss: 1.7193, Val Acc: 0.9205\n",
            "Epoch [1168/4000], Train Loss: 1.7141, Train Acc: 0.9264, Val Loss: 1.7145, Val Acc: 0.9250\n",
            "Epoch [1169/4000], Train Loss: 1.7141, Train Acc: 0.9250, Val Loss: 1.7172, Val Acc: 0.9218\n",
            "Epoch [1170/4000], Train Loss: 1.7158, Train Acc: 0.9237, Val Loss: 1.7151, Val Acc: 0.9282\n",
            "Epoch [1171/4000], Train Loss: 1.7166, Train Acc: 0.9247, Val Loss: 1.7221, Val Acc: 0.9192\n",
            "Epoch [1172/4000], Train Loss: 1.7165, Train Acc: 0.9237, Val Loss: 1.7229, Val Acc: 0.9186\n",
            "Epoch [1173/4000], Train Loss: 1.7172, Train Acc: 0.9231, Val Loss: 1.7124, Val Acc: 0.9288\n",
            "Epoch [1174/4000], Train Loss: 1.7139, Train Acc: 0.9253, Val Loss: 1.7149, Val Acc: 0.9276\n",
            "Epoch [1175/4000], Train Loss: 1.7150, Train Acc: 0.9239, Val Loss: 1.7141, Val Acc: 0.9256\n",
            "Epoch [1176/4000], Train Loss: 1.7148, Train Acc: 0.9261, Val Loss: 1.7143, Val Acc: 0.9218\n",
            "Epoch [1177/4000], Train Loss: 1.7138, Train Acc: 0.9247, Val Loss: 1.7172, Val Acc: 0.9218\n",
            "Epoch [1178/4000], Train Loss: 1.7163, Train Acc: 0.9232, Val Loss: 1.7161, Val Acc: 0.9256\n",
            "Epoch [1179/4000], Train Loss: 1.7167, Train Acc: 0.9234, Val Loss: 1.7133, Val Acc: 0.9269\n",
            "Epoch [1180/4000], Train Loss: 1.7157, Train Acc: 0.9237, Val Loss: 1.7151, Val Acc: 0.9269\n",
            "Epoch [1181/4000], Train Loss: 1.7154, Train Acc: 0.9250, Val Loss: 1.7143, Val Acc: 0.9256\n",
            "Epoch [1182/4000], Train Loss: 1.7167, Train Acc: 0.9237, Val Loss: 1.7151, Val Acc: 0.9250\n",
            "Epoch [1183/4000], Train Loss: 1.7180, Train Acc: 0.9239, Val Loss: 1.7211, Val Acc: 0.9205\n",
            "Epoch [1184/4000], Train Loss: 1.7143, Train Acc: 0.9237, Val Loss: 1.7133, Val Acc: 0.9237\n",
            "Epoch [1185/4000], Train Loss: 1.7178, Train Acc: 0.9236, Val Loss: 1.7216, Val Acc: 0.9173\n",
            "Epoch [1186/4000], Train Loss: 1.7175, Train Acc: 0.9215, Val Loss: 1.7150, Val Acc: 0.9256\n",
            "Epoch [1187/4000], Train Loss: 1.7141, Train Acc: 0.9245, Val Loss: 1.7159, Val Acc: 0.9192\n",
            "Epoch [1188/4000], Train Loss: 1.7182, Train Acc: 0.9204, Val Loss: 1.7147, Val Acc: 0.9237\n",
            "Epoch [1189/4000], Train Loss: 1.7152, Train Acc: 0.9236, Val Loss: 1.7150, Val Acc: 0.9212\n",
            "Epoch [1190/4000], Train Loss: 1.7150, Train Acc: 0.9253, Val Loss: 1.7149, Val Acc: 0.9250\n",
            "Epoch [1191/4000], Train Loss: 1.7138, Train Acc: 0.9258, Val Loss: 1.7178, Val Acc: 0.9295\n",
            "Epoch [1192/4000], Train Loss: 1.7153, Train Acc: 0.9244, Val Loss: 1.7142, Val Acc: 0.9250\n",
            "Epoch [1193/4000], Train Loss: 1.7156, Train Acc: 0.9228, Val Loss: 1.7133, Val Acc: 0.9276\n",
            "Epoch [1194/4000], Train Loss: 1.7161, Train Acc: 0.9236, Val Loss: 1.7156, Val Acc: 0.9244\n",
            "Epoch [1195/4000], Train Loss: 1.7161, Train Acc: 0.9232, Val Loss: 1.7248, Val Acc: 0.9154\n",
            "Epoch [1196/4000], Train Loss: 1.7148, Train Acc: 0.9258, Val Loss: 1.7151, Val Acc: 0.9256\n",
            "Epoch [1197/4000], Train Loss: 1.7142, Train Acc: 0.9253, Val Loss: 1.7221, Val Acc: 0.9173\n",
            "Epoch [1198/4000], Train Loss: 1.7154, Train Acc: 0.9245, Val Loss: 1.7145, Val Acc: 0.9250\n",
            "Epoch [1199/4000], Train Loss: 1.7153, Train Acc: 0.9244, Val Loss: 1.7129, Val Acc: 0.9276\n",
            "Epoch [1200/4000], Train Loss: 1.7140, Train Acc: 0.9242, Val Loss: 1.7149, Val Acc: 0.9256\n",
            "Epoch [1201/4000], Train Loss: 1.7152, Train Acc: 0.9236, Val Loss: 1.7205, Val Acc: 0.9173\n",
            "Epoch [1202/4000], Train Loss: 1.7174, Train Acc: 0.9221, Val Loss: 1.7259, Val Acc: 0.9199\n",
            "Epoch [1203/4000], Train Loss: 1.7160, Train Acc: 0.9234, Val Loss: 1.7133, Val Acc: 0.9218\n",
            "Epoch [1204/4000], Train Loss: 1.7154, Train Acc: 0.9240, Val Loss: 1.7123, Val Acc: 0.9288\n",
            "Epoch [1205/4000], Train Loss: 1.7156, Train Acc: 0.9239, Val Loss: 1.7195, Val Acc: 0.9231\n",
            "Epoch [1206/4000], Train Loss: 1.7175, Train Acc: 0.9216, Val Loss: 1.7146, Val Acc: 0.9244\n",
            "Epoch [1207/4000], Train Loss: 1.7154, Train Acc: 0.9250, Val Loss: 1.7153, Val Acc: 0.9244\n",
            "Epoch [1208/4000], Train Loss: 1.7156, Train Acc: 0.9232, Val Loss: 1.7145, Val Acc: 0.9244\n",
            "Epoch [1209/4000], Train Loss: 1.7143, Train Acc: 0.9236, Val Loss: 1.7163, Val Acc: 0.9282\n",
            "Epoch [1210/4000], Train Loss: 1.7148, Train Acc: 0.9255, Val Loss: 1.7197, Val Acc: 0.9199\n",
            "Epoch [1211/4000], Train Loss: 1.7160, Train Acc: 0.9231, Val Loss: 1.7151, Val Acc: 0.9212\n",
            "Epoch [1212/4000], Train Loss: 1.7153, Train Acc: 0.9239, Val Loss: 1.7126, Val Acc: 0.9263\n",
            "Epoch [1213/4000], Train Loss: 1.7146, Train Acc: 0.9248, Val Loss: 1.7224, Val Acc: 0.9186\n",
            "Epoch [1214/4000], Train Loss: 1.7140, Train Acc: 0.9244, Val Loss: 1.7166, Val Acc: 0.9263\n",
            "Epoch [1215/4000], Train Loss: 1.7150, Train Acc: 0.9258, Val Loss: 1.7125, Val Acc: 0.9269\n",
            "Epoch [1216/4000], Train Loss: 1.7176, Train Acc: 0.9231, Val Loss: 1.7156, Val Acc: 0.9244\n",
            "Epoch [1217/4000], Train Loss: 1.7168, Train Acc: 0.9228, Val Loss: 1.7215, Val Acc: 0.9167\n",
            "Epoch [1218/4000], Train Loss: 1.7180, Train Acc: 0.9210, Val Loss: 1.7145, Val Acc: 0.9256\n",
            "Epoch [1219/4000], Train Loss: 1.7160, Train Acc: 0.9231, Val Loss: 1.7157, Val Acc: 0.9212\n",
            "Epoch [1220/4000], Train Loss: 1.7136, Train Acc: 0.9242, Val Loss: 1.7167, Val Acc: 0.9250\n",
            "Epoch [1221/4000], Train Loss: 1.7154, Train Acc: 0.9228, Val Loss: 1.7133, Val Acc: 0.9263\n",
            "Epoch [1222/4000], Train Loss: 1.7152, Train Acc: 0.9247, Val Loss: 1.7246, Val Acc: 0.9224\n",
            "Epoch [1223/4000], Train Loss: 1.7156, Train Acc: 0.9215, Val Loss: 1.7167, Val Acc: 0.9224\n",
            "Epoch [1224/4000], Train Loss: 1.7151, Train Acc: 0.9236, Val Loss: 1.7131, Val Acc: 0.9276\n",
            "Epoch [1225/4000], Train Loss: 1.7160, Train Acc: 0.9242, Val Loss: 1.7121, Val Acc: 0.9263\n",
            "Epoch [1226/4000], Train Loss: 1.7179, Train Acc: 0.9220, Val Loss: 1.7142, Val Acc: 0.9276\n",
            "Epoch [1227/4000], Train Loss: 1.7148, Train Acc: 0.9237, Val Loss: 1.7139, Val Acc: 0.9231\n",
            "Epoch [1228/4000], Train Loss: 1.7155, Train Acc: 0.9231, Val Loss: 1.7142, Val Acc: 0.9256\n",
            "Epoch [1229/4000], Train Loss: 1.7152, Train Acc: 0.9248, Val Loss: 1.7141, Val Acc: 0.9295\n",
            "Epoch [1230/4000], Train Loss: 1.7156, Train Acc: 0.9240, Val Loss: 1.7138, Val Acc: 0.9256\n",
            "Epoch [1231/4000], Train Loss: 1.7163, Train Acc: 0.9237, Val Loss: 1.7134, Val Acc: 0.9256\n",
            "Epoch [1232/4000], Train Loss: 1.7146, Train Acc: 0.9232, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [1233/4000], Train Loss: 1.7141, Train Acc: 0.9252, Val Loss: 1.7117, Val Acc: 0.9269\n",
            "Epoch [1234/4000], Train Loss: 1.7142, Train Acc: 0.9252, Val Loss: 1.7120, Val Acc: 0.9288\n",
            "Epoch [1235/4000], Train Loss: 1.7142, Train Acc: 0.9245, Val Loss: 1.7120, Val Acc: 0.9263\n",
            "Epoch [1236/4000], Train Loss: 1.7150, Train Acc: 0.9229, Val Loss: 1.7207, Val Acc: 0.9250\n",
            "Epoch [1237/4000], Train Loss: 1.7197, Train Acc: 0.9218, Val Loss: 1.7135, Val Acc: 0.9212\n",
            "Epoch [1238/4000], Train Loss: 1.7148, Train Acc: 0.9240, Val Loss: 1.7148, Val Acc: 0.9276\n",
            "Epoch [1239/4000], Train Loss: 1.7133, Train Acc: 0.9252, Val Loss: 1.7146, Val Acc: 0.9244\n",
            "Epoch [1240/4000], Train Loss: 1.7185, Train Acc: 0.9232, Val Loss: 1.7193, Val Acc: 0.9167\n",
            "Epoch [1241/4000], Train Loss: 1.7168, Train Acc: 0.9231, Val Loss: 1.7192, Val Acc: 0.9179\n",
            "Epoch [1242/4000], Train Loss: 1.7160, Train Acc: 0.9236, Val Loss: 1.7149, Val Acc: 0.9237\n",
            "Epoch [1243/4000], Train Loss: 1.7168, Train Acc: 0.9237, Val Loss: 1.7280, Val Acc: 0.9218\n",
            "Epoch [1244/4000], Train Loss: 1.7170, Train Acc: 0.9232, Val Loss: 1.7137, Val Acc: 0.9244\n",
            "Epoch [1245/4000], Train Loss: 1.7154, Train Acc: 0.9239, Val Loss: 1.7148, Val Acc: 0.9250\n",
            "Epoch [1246/4000], Train Loss: 1.7136, Train Acc: 0.9258, Val Loss: 1.7159, Val Acc: 0.9237\n",
            "Epoch [1247/4000], Train Loss: 1.7133, Train Acc: 0.9256, Val Loss: 1.7135, Val Acc: 0.9244\n",
            "Epoch [1248/4000], Train Loss: 1.7162, Train Acc: 0.9234, Val Loss: 1.7165, Val Acc: 0.9282\n",
            "Epoch [1249/4000], Train Loss: 1.7161, Train Acc: 0.9226, Val Loss: 1.7124, Val Acc: 0.9256\n",
            "Epoch [1250/4000], Train Loss: 1.7149, Train Acc: 0.9239, Val Loss: 1.7113, Val Acc: 0.9269\n",
            "Epoch [1251/4000], Train Loss: 1.7130, Train Acc: 0.9274, Val Loss: 1.7140, Val Acc: 0.9186\n",
            "Epoch [1252/4000], Train Loss: 1.7155, Train Acc: 0.9237, Val Loss: 1.7116, Val Acc: 0.9237\n",
            "Epoch [1253/4000], Train Loss: 1.7174, Train Acc: 0.9231, Val Loss: 1.7133, Val Acc: 0.9276\n",
            "Epoch [1254/4000], Train Loss: 1.7159, Train Acc: 0.9232, Val Loss: 1.7197, Val Acc: 0.9256\n",
            "Epoch [1255/4000], Train Loss: 1.7141, Train Acc: 0.9247, Val Loss: 1.7118, Val Acc: 0.9276\n",
            "Epoch [1256/4000], Train Loss: 1.7142, Train Acc: 0.9237, Val Loss: 1.7153, Val Acc: 0.9199\n",
            "Epoch [1257/4000], Train Loss: 1.7156, Train Acc: 0.9242, Val Loss: 1.7136, Val Acc: 0.9263\n",
            "Epoch [1258/4000], Train Loss: 1.7156, Train Acc: 0.9234, Val Loss: 1.7146, Val Acc: 0.9244\n",
            "Epoch [1259/4000], Train Loss: 1.7146, Train Acc: 0.9237, Val Loss: 1.7122, Val Acc: 0.9256\n",
            "Epoch [1260/4000], Train Loss: 1.7147, Train Acc: 0.9248, Val Loss: 1.7153, Val Acc: 0.9256\n",
            "Epoch [1261/4000], Train Loss: 1.7169, Train Acc: 0.9215, Val Loss: 1.7188, Val Acc: 0.9224\n",
            "Epoch [1262/4000], Train Loss: 1.7146, Train Acc: 0.9245, Val Loss: 1.7138, Val Acc: 0.9250\n",
            "Epoch [1263/4000], Train Loss: 1.7151, Train Acc: 0.9234, Val Loss: 1.7102, Val Acc: 0.9301\n",
            "Epoch [1264/4000], Train Loss: 1.7166, Train Acc: 0.9234, Val Loss: 1.7143, Val Acc: 0.9244\n",
            "Epoch [1265/4000], Train Loss: 1.7156, Train Acc: 0.9247, Val Loss: 1.7102, Val Acc: 0.9282\n",
            "Epoch [1266/4000], Train Loss: 1.7145, Train Acc: 0.9239, Val Loss: 1.7165, Val Acc: 0.9250\n",
            "Epoch [1267/4000], Train Loss: 1.7145, Train Acc: 0.9245, Val Loss: 1.7197, Val Acc: 0.9192\n",
            "Epoch [1268/4000], Train Loss: 1.7142, Train Acc: 0.9255, Val Loss: 1.7133, Val Acc: 0.9288\n",
            "Epoch [1269/4000], Train Loss: 1.7148, Train Acc: 0.9239, Val Loss: 1.7123, Val Acc: 0.9276\n",
            "Epoch [1270/4000], Train Loss: 1.7139, Train Acc: 0.9252, Val Loss: 1.7129, Val Acc: 0.9237\n",
            "Epoch [1271/4000], Train Loss: 1.7156, Train Acc: 0.9239, Val Loss: 1.7164, Val Acc: 0.9231\n",
            "Epoch [1272/4000], Train Loss: 1.7162, Train Acc: 0.9240, Val Loss: 1.7177, Val Acc: 0.9224\n",
            "Epoch [1273/4000], Train Loss: 1.7165, Train Acc: 0.9239, Val Loss: 1.7128, Val Acc: 0.9250\n",
            "Epoch [1274/4000], Train Loss: 1.7151, Train Acc: 0.9236, Val Loss: 1.7146, Val Acc: 0.9288\n",
            "Epoch [1275/4000], Train Loss: 1.7146, Train Acc: 0.9247, Val Loss: 1.7143, Val Acc: 0.9250\n",
            "Epoch [1276/4000], Train Loss: 1.7143, Train Acc: 0.9255, Val Loss: 1.7142, Val Acc: 0.9282\n",
            "Epoch [1277/4000], Train Loss: 1.7160, Train Acc: 0.9231, Val Loss: 1.7116, Val Acc: 0.9269\n",
            "Epoch [1278/4000], Train Loss: 1.7160, Train Acc: 0.9226, Val Loss: 1.7132, Val Acc: 0.9269\n",
            "Epoch [1279/4000], Train Loss: 1.7126, Train Acc: 0.9263, Val Loss: 1.7155, Val Acc: 0.9231\n",
            "Epoch [1280/4000], Train Loss: 1.7140, Train Acc: 0.9247, Val Loss: 1.7104, Val Acc: 0.9282\n",
            "Epoch [1281/4000], Train Loss: 1.7150, Train Acc: 0.9231, Val Loss: 1.7151, Val Acc: 0.9224\n",
            "Epoch [1282/4000], Train Loss: 1.7163, Train Acc: 0.9253, Val Loss: 1.7159, Val Acc: 0.9244\n",
            "Epoch [1283/4000], Train Loss: 1.7148, Train Acc: 0.9247, Val Loss: 1.7136, Val Acc: 0.9250\n",
            "Epoch [1284/4000], Train Loss: 1.7130, Train Acc: 0.9250, Val Loss: 1.7191, Val Acc: 0.9244\n",
            "Epoch [1285/4000], Train Loss: 1.7171, Train Acc: 0.9223, Val Loss: 1.7144, Val Acc: 0.9237\n",
            "Epoch [1286/4000], Train Loss: 1.7150, Train Acc: 0.9252, Val Loss: 1.7156, Val Acc: 0.9218\n",
            "Epoch [1287/4000], Train Loss: 1.7149, Train Acc: 0.9258, Val Loss: 1.7157, Val Acc: 0.9250\n",
            "Epoch [1288/4000], Train Loss: 1.7163, Train Acc: 0.9250, Val Loss: 1.7244, Val Acc: 0.9224\n",
            "Epoch [1289/4000], Train Loss: 1.7134, Train Acc: 0.9245, Val Loss: 1.7127, Val Acc: 0.9250\n",
            "Epoch [1290/4000], Train Loss: 1.7160, Train Acc: 0.9242, Val Loss: 1.7205, Val Acc: 0.9199\n",
            "Epoch [1291/4000], Train Loss: 1.7205, Train Acc: 0.9194, Val Loss: 1.7126, Val Acc: 0.9263\n",
            "Epoch [1292/4000], Train Loss: 1.7145, Train Acc: 0.9240, Val Loss: 1.7188, Val Acc: 0.9231\n",
            "Epoch [1293/4000], Train Loss: 1.7166, Train Acc: 0.9240, Val Loss: 1.7134, Val Acc: 0.9205\n",
            "Epoch [1294/4000], Train Loss: 1.7138, Train Acc: 0.9253, Val Loss: 1.7205, Val Acc: 0.9141\n",
            "Epoch [1295/4000], Train Loss: 1.7153, Train Acc: 0.9236, Val Loss: 1.7163, Val Acc: 0.9167\n",
            "Epoch [1296/4000], Train Loss: 1.7147, Train Acc: 0.9244, Val Loss: 1.7224, Val Acc: 0.9218\n",
            "Epoch [1297/4000], Train Loss: 1.7145, Train Acc: 0.9252, Val Loss: 1.7124, Val Acc: 0.9282\n",
            "Epoch [1298/4000], Train Loss: 1.7181, Train Acc: 0.9200, Val Loss: 1.7168, Val Acc: 0.9263\n",
            "Epoch [1299/4000], Train Loss: 1.7163, Train Acc: 0.9239, Val Loss: 1.7286, Val Acc: 0.9122\n",
            "Epoch [1300/4000], Train Loss: 1.7173, Train Acc: 0.9218, Val Loss: 1.7157, Val Acc: 0.9263\n",
            "Epoch [1301/4000], Train Loss: 1.7126, Train Acc: 0.9260, Val Loss: 1.7145, Val Acc: 0.9218\n",
            "Epoch [1302/4000], Train Loss: 1.7138, Train Acc: 0.9253, Val Loss: 1.7128, Val Acc: 0.9282\n",
            "Epoch [1303/4000], Train Loss: 1.7149, Train Acc: 0.9253, Val Loss: 1.7188, Val Acc: 0.9218\n",
            "Epoch [1304/4000], Train Loss: 1.7166, Train Acc: 0.9208, Val Loss: 1.7129, Val Acc: 0.9282\n",
            "Epoch [1305/4000], Train Loss: 1.7142, Train Acc: 0.9250, Val Loss: 1.7130, Val Acc: 0.9295\n",
            "Epoch [1306/4000], Train Loss: 1.7176, Train Acc: 0.9218, Val Loss: 1.7151, Val Acc: 0.9256\n",
            "Epoch [1307/4000], Train Loss: 1.7166, Train Acc: 0.9224, Val Loss: 1.7179, Val Acc: 0.9256\n",
            "Epoch [1308/4000], Train Loss: 1.7138, Train Acc: 0.9244, Val Loss: 1.7147, Val Acc: 0.9244\n",
            "Epoch [1309/4000], Train Loss: 1.7149, Train Acc: 0.9247, Val Loss: 1.7155, Val Acc: 0.9218\n",
            "Epoch [1310/4000], Train Loss: 1.7157, Train Acc: 0.9234, Val Loss: 1.7122, Val Acc: 0.9282\n",
            "Epoch [1311/4000], Train Loss: 1.7163, Train Acc: 0.9247, Val Loss: 1.7144, Val Acc: 0.9250\n",
            "Epoch [1312/4000], Train Loss: 1.7149, Train Acc: 0.9248, Val Loss: 1.7196, Val Acc: 0.9256\n",
            "Epoch [1313/4000], Train Loss: 1.7147, Train Acc: 0.9252, Val Loss: 1.7161, Val Acc: 0.9212\n",
            "Epoch [1314/4000], Train Loss: 1.7141, Train Acc: 0.9245, Val Loss: 1.7138, Val Acc: 0.9231\n",
            "Epoch [1315/4000], Train Loss: 1.7157, Train Acc: 0.9231, Val Loss: 1.7110, Val Acc: 0.9295\n",
            "Epoch [1316/4000], Train Loss: 1.7155, Train Acc: 0.9247, Val Loss: 1.7246, Val Acc: 0.9199\n",
            "Epoch [1317/4000], Train Loss: 1.7164, Train Acc: 0.9237, Val Loss: 1.7182, Val Acc: 0.9212\n",
            "Epoch [1318/4000], Train Loss: 1.7146, Train Acc: 0.9248, Val Loss: 1.7180, Val Acc: 0.9218\n",
            "Epoch [1319/4000], Train Loss: 1.7155, Train Acc: 0.9239, Val Loss: 1.7129, Val Acc: 0.9231\n",
            "Epoch [1320/4000], Train Loss: 1.7157, Train Acc: 0.9231, Val Loss: 1.7119, Val Acc: 0.9276\n",
            "Epoch [1321/4000], Train Loss: 1.7145, Train Acc: 0.9237, Val Loss: 1.7127, Val Acc: 0.9256\n",
            "Epoch [1322/4000], Train Loss: 1.7142, Train Acc: 0.9256, Val Loss: 1.7173, Val Acc: 0.9199\n",
            "Epoch [1323/4000], Train Loss: 1.7142, Train Acc: 0.9250, Val Loss: 1.7198, Val Acc: 0.9192\n",
            "Epoch [1324/4000], Train Loss: 1.7168, Train Acc: 0.9234, Val Loss: 1.7177, Val Acc: 0.9212\n",
            "Epoch [1325/4000], Train Loss: 1.7154, Train Acc: 0.9236, Val Loss: 1.7198, Val Acc: 0.9224\n",
            "Epoch [1326/4000], Train Loss: 1.7147, Train Acc: 0.9242, Val Loss: 1.7172, Val Acc: 0.9244\n",
            "Epoch [1327/4000], Train Loss: 1.7137, Train Acc: 0.9252, Val Loss: 1.7128, Val Acc: 0.9308\n",
            "Epoch [1328/4000], Train Loss: 1.7137, Train Acc: 0.9245, Val Loss: 1.7121, Val Acc: 0.9295\n",
            "Epoch [1329/4000], Train Loss: 1.7152, Train Acc: 0.9240, Val Loss: 1.7141, Val Acc: 0.9276\n",
            "Epoch [1330/4000], Train Loss: 1.7158, Train Acc: 0.9244, Val Loss: 1.7132, Val Acc: 0.9269\n",
            "Epoch [1331/4000], Train Loss: 1.7141, Train Acc: 0.9260, Val Loss: 1.7125, Val Acc: 0.9282\n",
            "Epoch [1332/4000], Train Loss: 1.7126, Train Acc: 0.9279, Val Loss: 1.7102, Val Acc: 0.9288\n",
            "Epoch [1333/4000], Train Loss: 1.7163, Train Acc: 0.9220, Val Loss: 1.7122, Val Acc: 0.9282\n",
            "Epoch [1334/4000], Train Loss: 1.7144, Train Acc: 0.9245, Val Loss: 1.7131, Val Acc: 0.9276\n",
            "Epoch [1335/4000], Train Loss: 1.7136, Train Acc: 0.9258, Val Loss: 1.7164, Val Acc: 0.9244\n",
            "Epoch [1336/4000], Train Loss: 1.7156, Train Acc: 0.9247, Val Loss: 1.7185, Val Acc: 0.9199\n",
            "Epoch [1337/4000], Train Loss: 1.7157, Train Acc: 0.9242, Val Loss: 1.7190, Val Acc: 0.9205\n",
            "Epoch [1338/4000], Train Loss: 1.7146, Train Acc: 0.9253, Val Loss: 1.7137, Val Acc: 0.9250\n",
            "Epoch [1339/4000], Train Loss: 1.7149, Train Acc: 0.9244, Val Loss: 1.7242, Val Acc: 0.9179\n",
            "Epoch [1340/4000], Train Loss: 1.7166, Train Acc: 0.9244, Val Loss: 1.7108, Val Acc: 0.9269\n",
            "Epoch [1341/4000], Train Loss: 1.7139, Train Acc: 0.9245, Val Loss: 1.7139, Val Acc: 0.9218\n",
            "Epoch [1342/4000], Train Loss: 1.7144, Train Acc: 0.9244, Val Loss: 1.7134, Val Acc: 0.9263\n",
            "Epoch [1343/4000], Train Loss: 1.7142, Train Acc: 0.9236, Val Loss: 1.7201, Val Acc: 0.9160\n",
            "Epoch [1344/4000], Train Loss: 1.7139, Train Acc: 0.9255, Val Loss: 1.7143, Val Acc: 0.9263\n",
            "Epoch [1345/4000], Train Loss: 1.7155, Train Acc: 0.9244, Val Loss: 1.7255, Val Acc: 0.9147\n",
            "Epoch [1346/4000], Train Loss: 1.7150, Train Acc: 0.9228, Val Loss: 1.7217, Val Acc: 0.9160\n",
            "Epoch [1347/4000], Train Loss: 1.7143, Train Acc: 0.9248, Val Loss: 1.7126, Val Acc: 0.9269\n",
            "Epoch [1348/4000], Train Loss: 1.7153, Train Acc: 0.9240, Val Loss: 1.7184, Val Acc: 0.9282\n",
            "Epoch [1349/4000], Train Loss: 1.7132, Train Acc: 0.9266, Val Loss: 1.7174, Val Acc: 0.9244\n",
            "Epoch [1350/4000], Train Loss: 1.7168, Train Acc: 0.9228, Val Loss: 1.7155, Val Acc: 0.9237\n",
            "Epoch [1351/4000], Train Loss: 1.7136, Train Acc: 0.9253, Val Loss: 1.7261, Val Acc: 0.9167\n",
            "Epoch [1352/4000], Train Loss: 1.7158, Train Acc: 0.9245, Val Loss: 1.7196, Val Acc: 0.9231\n",
            "Epoch [1353/4000], Train Loss: 1.7149, Train Acc: 0.9260, Val Loss: 1.7106, Val Acc: 0.9269\n",
            "Epoch [1354/4000], Train Loss: 1.7146, Train Acc: 0.9242, Val Loss: 1.7216, Val Acc: 0.9199\n",
            "Epoch [1355/4000], Train Loss: 1.7157, Train Acc: 0.9239, Val Loss: 1.7240, Val Acc: 0.9186\n",
            "Epoch [1356/4000], Train Loss: 1.7171, Train Acc: 0.9221, Val Loss: 1.7154, Val Acc: 0.9231\n",
            "Epoch [1357/4000], Train Loss: 1.7135, Train Acc: 0.9260, Val Loss: 1.7173, Val Acc: 0.9237\n",
            "Epoch [1358/4000], Train Loss: 1.7160, Train Acc: 0.9247, Val Loss: 1.7268, Val Acc: 0.9103\n",
            "Epoch [1359/4000], Train Loss: 1.7173, Train Acc: 0.9216, Val Loss: 1.7171, Val Acc: 0.9224\n",
            "Epoch [1360/4000], Train Loss: 1.7144, Train Acc: 0.9250, Val Loss: 1.7165, Val Acc: 0.9263\n",
            "Epoch [1361/4000], Train Loss: 1.7141, Train Acc: 0.9263, Val Loss: 1.7157, Val Acc: 0.9244\n",
            "Epoch [1362/4000], Train Loss: 1.7138, Train Acc: 0.9260, Val Loss: 1.7149, Val Acc: 0.9244\n",
            "Epoch [1363/4000], Train Loss: 1.7133, Train Acc: 0.9258, Val Loss: 1.7153, Val Acc: 0.9244\n",
            "Epoch [1364/4000], Train Loss: 1.7154, Train Acc: 0.9242, Val Loss: 1.7128, Val Acc: 0.9250\n",
            "Epoch [1365/4000], Train Loss: 1.7153, Train Acc: 0.9239, Val Loss: 1.7157, Val Acc: 0.9250\n",
            "Epoch [1366/4000], Train Loss: 1.7165, Train Acc: 0.9236, Val Loss: 1.7122, Val Acc: 0.9282\n",
            "Epoch [1367/4000], Train Loss: 1.7147, Train Acc: 0.9250, Val Loss: 1.7111, Val Acc: 0.9288\n",
            "Epoch [1368/4000], Train Loss: 1.7155, Train Acc: 0.9234, Val Loss: 1.7129, Val Acc: 0.9231\n",
            "Epoch [1369/4000], Train Loss: 1.7144, Train Acc: 0.9264, Val Loss: 1.7175, Val Acc: 0.9295\n",
            "Epoch [1370/4000], Train Loss: 1.7136, Train Acc: 0.9264, Val Loss: 1.7134, Val Acc: 0.9263\n",
            "Epoch [1371/4000], Train Loss: 1.7169, Train Acc: 0.9224, Val Loss: 1.7108, Val Acc: 0.9282\n",
            "Epoch [1372/4000], Train Loss: 1.7129, Train Acc: 0.9264, Val Loss: 1.7192, Val Acc: 0.9167\n",
            "Epoch [1373/4000], Train Loss: 1.7136, Train Acc: 0.9248, Val Loss: 1.7181, Val Acc: 0.9173\n",
            "Epoch [1374/4000], Train Loss: 1.7158, Train Acc: 0.9237, Val Loss: 1.7139, Val Acc: 0.9263\n",
            "Epoch [1375/4000], Train Loss: 1.7175, Train Acc: 0.9216, Val Loss: 1.7129, Val Acc: 0.9256\n",
            "Epoch [1376/4000], Train Loss: 1.7137, Train Acc: 0.9255, Val Loss: 1.7126, Val Acc: 0.9295\n",
            "Epoch [1377/4000], Train Loss: 1.7123, Train Acc: 0.9274, Val Loss: 1.7128, Val Acc: 0.9263\n",
            "Epoch [1378/4000], Train Loss: 1.7144, Train Acc: 0.9256, Val Loss: 1.7126, Val Acc: 0.9295\n",
            "Epoch [1379/4000], Train Loss: 1.7146, Train Acc: 0.9245, Val Loss: 1.7200, Val Acc: 0.9192\n",
            "Epoch [1380/4000], Train Loss: 1.7142, Train Acc: 0.9264, Val Loss: 1.7161, Val Acc: 0.9224\n",
            "Epoch [1381/4000], Train Loss: 1.7190, Train Acc: 0.9212, Val Loss: 1.7125, Val Acc: 0.9282\n",
            "Epoch [1382/4000], Train Loss: 1.7164, Train Acc: 0.9242, Val Loss: 1.7134, Val Acc: 0.9276\n",
            "Epoch [1383/4000], Train Loss: 1.7139, Train Acc: 0.9266, Val Loss: 1.7161, Val Acc: 0.9301\n",
            "Epoch [1384/4000], Train Loss: 1.7135, Train Acc: 0.9253, Val Loss: 1.7181, Val Acc: 0.9218\n",
            "Epoch [1385/4000], Train Loss: 1.7154, Train Acc: 0.9244, Val Loss: 1.7119, Val Acc: 0.9212\n",
            "Epoch [1386/4000], Train Loss: 1.7146, Train Acc: 0.9231, Val Loss: 1.7145, Val Acc: 0.9256\n",
            "Epoch [1387/4000], Train Loss: 1.7137, Train Acc: 0.9266, Val Loss: 1.7127, Val Acc: 0.9269\n",
            "Epoch [1388/4000], Train Loss: 1.7146, Train Acc: 0.9247, Val Loss: 1.7158, Val Acc: 0.9256\n",
            "Epoch [1389/4000], Train Loss: 1.7148, Train Acc: 0.9240, Val Loss: 1.7142, Val Acc: 0.9212\n",
            "Epoch [1390/4000], Train Loss: 1.7150, Train Acc: 0.9245, Val Loss: 1.7161, Val Acc: 0.9224\n",
            "Epoch [1391/4000], Train Loss: 1.7141, Train Acc: 0.9255, Val Loss: 1.7163, Val Acc: 0.9218\n",
            "Epoch [1392/4000], Train Loss: 1.7143, Train Acc: 0.9258, Val Loss: 1.7166, Val Acc: 0.9256\n",
            "Epoch [1393/4000], Train Loss: 1.7141, Train Acc: 0.9234, Val Loss: 1.7241, Val Acc: 0.9186\n",
            "Epoch [1394/4000], Train Loss: 1.7160, Train Acc: 0.9229, Val Loss: 1.7152, Val Acc: 0.9288\n",
            "Epoch [1395/4000], Train Loss: 1.7129, Train Acc: 0.9268, Val Loss: 1.7103, Val Acc: 0.9244\n",
            "Epoch [1396/4000], Train Loss: 1.7139, Train Acc: 0.9252, Val Loss: 1.7180, Val Acc: 0.9192\n",
            "Epoch [1397/4000], Train Loss: 1.7137, Train Acc: 0.9248, Val Loss: 1.7109, Val Acc: 0.9256\n",
            "Epoch [1398/4000], Train Loss: 1.7130, Train Acc: 0.9261, Val Loss: 1.7158, Val Acc: 0.9276\n",
            "Epoch [1399/4000], Train Loss: 1.7140, Train Acc: 0.9258, Val Loss: 1.7128, Val Acc: 0.9301\n",
            "Epoch [1400/4000], Train Loss: 1.7127, Train Acc: 0.9271, Val Loss: 1.7136, Val Acc: 0.9295\n",
            "Epoch [1401/4000], Train Loss: 1.7160, Train Acc: 0.9228, Val Loss: 1.7200, Val Acc: 0.9218\n",
            "Epoch [1402/4000], Train Loss: 1.7137, Train Acc: 0.9272, Val Loss: 1.7136, Val Acc: 0.9263\n",
            "Epoch [1403/4000], Train Loss: 1.7142, Train Acc: 0.9252, Val Loss: 1.7182, Val Acc: 0.9263\n",
            "Epoch [1404/4000], Train Loss: 1.7138, Train Acc: 0.9255, Val Loss: 1.7141, Val Acc: 0.9269\n",
            "Epoch [1405/4000], Train Loss: 1.7140, Train Acc: 0.9248, Val Loss: 1.7124, Val Acc: 0.9276\n",
            "Epoch [1406/4000], Train Loss: 1.7132, Train Acc: 0.9263, Val Loss: 1.7175, Val Acc: 0.9224\n",
            "Epoch [1407/4000], Train Loss: 1.7142, Train Acc: 0.9244, Val Loss: 1.7271, Val Acc: 0.9167\n",
            "Epoch [1408/4000], Train Loss: 1.7153, Train Acc: 0.9250, Val Loss: 1.7131, Val Acc: 0.9224\n",
            "Epoch [1409/4000], Train Loss: 1.7125, Train Acc: 0.9277, Val Loss: 1.7157, Val Acc: 0.9276\n",
            "Epoch [1410/4000], Train Loss: 1.7154, Train Acc: 0.9232, Val Loss: 1.7107, Val Acc: 0.9282\n",
            "Epoch [1411/4000], Train Loss: 1.7148, Train Acc: 0.9252, Val Loss: 1.7162, Val Acc: 0.9282\n",
            "Epoch [1412/4000], Train Loss: 1.7142, Train Acc: 0.9258, Val Loss: 1.7201, Val Acc: 0.9231\n",
            "Epoch [1413/4000], Train Loss: 1.7167, Train Acc: 0.9212, Val Loss: 1.7134, Val Acc: 0.9269\n",
            "Epoch [1414/4000], Train Loss: 1.7134, Train Acc: 0.9255, Val Loss: 1.7126, Val Acc: 0.9244\n",
            "Epoch [1415/4000], Train Loss: 1.7158, Train Acc: 0.9236, Val Loss: 1.7137, Val Acc: 0.9288\n",
            "Epoch [1416/4000], Train Loss: 1.7136, Train Acc: 0.9256, Val Loss: 1.7140, Val Acc: 0.9212\n",
            "Epoch [1417/4000], Train Loss: 1.7118, Train Acc: 0.9276, Val Loss: 1.7140, Val Acc: 0.9276\n",
            "Epoch [1418/4000], Train Loss: 1.7126, Train Acc: 0.9269, Val Loss: 1.7114, Val Acc: 0.9244\n",
            "Epoch [1419/4000], Train Loss: 1.7138, Train Acc: 0.9253, Val Loss: 1.7122, Val Acc: 0.9244\n",
            "Epoch [1420/4000], Train Loss: 1.7126, Train Acc: 0.9274, Val Loss: 1.7111, Val Acc: 0.9288\n",
            "Epoch [1421/4000], Train Loss: 1.7130, Train Acc: 0.9266, Val Loss: 1.7126, Val Acc: 0.9295\n",
            "Epoch [1422/4000], Train Loss: 1.7151, Train Acc: 0.9242, Val Loss: 1.7132, Val Acc: 0.9231\n",
            "Epoch [1423/4000], Train Loss: 1.7125, Train Acc: 0.9261, Val Loss: 1.7142, Val Acc: 0.9269\n",
            "Epoch [1424/4000], Train Loss: 1.7153, Train Acc: 0.9247, Val Loss: 1.7167, Val Acc: 0.9237\n",
            "Epoch [1425/4000], Train Loss: 1.7152, Train Acc: 0.9253, Val Loss: 1.7161, Val Acc: 0.9250\n",
            "Epoch [1426/4000], Train Loss: 1.7131, Train Acc: 0.9252, Val Loss: 1.7157, Val Acc: 0.9231\n",
            "Epoch [1427/4000], Train Loss: 1.7155, Train Acc: 0.9250, Val Loss: 1.7138, Val Acc: 0.9250\n",
            "Epoch [1428/4000], Train Loss: 1.7136, Train Acc: 0.9244, Val Loss: 1.7232, Val Acc: 0.9179\n",
            "Epoch [1429/4000], Train Loss: 1.7134, Train Acc: 0.9269, Val Loss: 1.7119, Val Acc: 0.9256\n",
            "Epoch [1430/4000], Train Loss: 1.7128, Train Acc: 0.9272, Val Loss: 1.7111, Val Acc: 0.9282\n",
            "Epoch [1431/4000], Train Loss: 1.7128, Train Acc: 0.9261, Val Loss: 1.7121, Val Acc: 0.9237\n",
            "Epoch [1432/4000], Train Loss: 1.7151, Train Acc: 0.9253, Val Loss: 1.7147, Val Acc: 0.9256\n",
            "Epoch [1433/4000], Train Loss: 1.7139, Train Acc: 0.9260, Val Loss: 1.7164, Val Acc: 0.9192\n",
            "Epoch [1434/4000], Train Loss: 1.7143, Train Acc: 0.9244, Val Loss: 1.7153, Val Acc: 0.9256\n",
            "Epoch [1435/4000], Train Loss: 1.7134, Train Acc: 0.9264, Val Loss: 1.7150, Val Acc: 0.9224\n",
            "Epoch [1436/4000], Train Loss: 1.7132, Train Acc: 0.9255, Val Loss: 1.7143, Val Acc: 0.9244\n",
            "Epoch [1437/4000], Train Loss: 1.7165, Train Acc: 0.9231, Val Loss: 1.7158, Val Acc: 0.9244\n",
            "Epoch [1438/4000], Train Loss: 1.7134, Train Acc: 0.9253, Val Loss: 1.7172, Val Acc: 0.9224\n",
            "Epoch [1439/4000], Train Loss: 1.7167, Train Acc: 0.9226, Val Loss: 1.7123, Val Acc: 0.9269\n",
            "Epoch [1440/4000], Train Loss: 1.7141, Train Acc: 0.9263, Val Loss: 1.7118, Val Acc: 0.9244\n",
            "Epoch [1441/4000], Train Loss: 1.7125, Train Acc: 0.9271, Val Loss: 1.7148, Val Acc: 0.9250\n",
            "Epoch [1442/4000], Train Loss: 1.7140, Train Acc: 0.9272, Val Loss: 1.7102, Val Acc: 0.9263\n",
            "Epoch [1443/4000], Train Loss: 1.7138, Train Acc: 0.9256, Val Loss: 1.7111, Val Acc: 0.9250\n",
            "Epoch [1444/4000], Train Loss: 1.7135, Train Acc: 0.9266, Val Loss: 1.7108, Val Acc: 0.9269\n",
            "Epoch [1445/4000], Train Loss: 1.7138, Train Acc: 0.9258, Val Loss: 1.7130, Val Acc: 0.9237\n",
            "Epoch [1446/4000], Train Loss: 1.7146, Train Acc: 0.9255, Val Loss: 1.7108, Val Acc: 0.9282\n",
            "Epoch [1447/4000], Train Loss: 1.7126, Train Acc: 0.9261, Val Loss: 1.7188, Val Acc: 0.9186\n",
            "Epoch [1448/4000], Train Loss: 1.7145, Train Acc: 0.9248, Val Loss: 1.7188, Val Acc: 0.9231\n",
            "Epoch [1449/4000], Train Loss: 1.7150, Train Acc: 0.9261, Val Loss: 1.7254, Val Acc: 0.9179\n",
            "Epoch [1450/4000], Train Loss: 1.7127, Train Acc: 0.9271, Val Loss: 1.7138, Val Acc: 0.9263\n",
            "Epoch [1451/4000], Train Loss: 1.7135, Train Acc: 0.9271, Val Loss: 1.7119, Val Acc: 0.9269\n",
            "Epoch [1452/4000], Train Loss: 1.7154, Train Acc: 0.9240, Val Loss: 1.7116, Val Acc: 0.9237\n",
            "Epoch [1453/4000], Train Loss: 1.7138, Train Acc: 0.9258, Val Loss: 1.7107, Val Acc: 0.9269\n",
            "Epoch [1454/4000], Train Loss: 1.7131, Train Acc: 0.9268, Val Loss: 1.7148, Val Acc: 0.9269\n",
            "Epoch [1455/4000], Train Loss: 1.7144, Train Acc: 0.9234, Val Loss: 1.7208, Val Acc: 0.9199\n",
            "Epoch [1456/4000], Train Loss: 1.7143, Train Acc: 0.9271, Val Loss: 1.7148, Val Acc: 0.9256\n",
            "Epoch [1457/4000], Train Loss: 1.7145, Train Acc: 0.9240, Val Loss: 1.7105, Val Acc: 0.9276\n",
            "Epoch [1458/4000], Train Loss: 1.7130, Train Acc: 0.9252, Val Loss: 1.7161, Val Acc: 0.9256\n",
            "Epoch [1459/4000], Train Loss: 1.7153, Train Acc: 0.9255, Val Loss: 1.7143, Val Acc: 0.9250\n",
            "Epoch [1460/4000], Train Loss: 1.7135, Train Acc: 0.9269, Val Loss: 1.7177, Val Acc: 0.9276\n",
            "Epoch [1461/4000], Train Loss: 1.7153, Train Acc: 0.9242, Val Loss: 1.7109, Val Acc: 0.9263\n",
            "Epoch [1462/4000], Train Loss: 1.7154, Train Acc: 0.9255, Val Loss: 1.7093, Val Acc: 0.9250\n",
            "Epoch [1463/4000], Train Loss: 1.7132, Train Acc: 0.9255, Val Loss: 1.7137, Val Acc: 0.9276\n",
            "Epoch [1464/4000], Train Loss: 1.7144, Train Acc: 0.9247, Val Loss: 1.7220, Val Acc: 0.9218\n",
            "Epoch [1465/4000], Train Loss: 1.7164, Train Acc: 0.9224, Val Loss: 1.7129, Val Acc: 0.9250\n",
            "Epoch [1466/4000], Train Loss: 1.7134, Train Acc: 0.9250, Val Loss: 1.7140, Val Acc: 0.9263\n",
            "Epoch [1467/4000], Train Loss: 1.7139, Train Acc: 0.9253, Val Loss: 1.7143, Val Acc: 0.9263\n",
            "Epoch [1468/4000], Train Loss: 1.7142, Train Acc: 0.9253, Val Loss: 1.7147, Val Acc: 0.9308\n",
            "Epoch [1469/4000], Train Loss: 1.7146, Train Acc: 0.9247, Val Loss: 1.7141, Val Acc: 0.9250\n",
            "Epoch [1470/4000], Train Loss: 1.7131, Train Acc: 0.9250, Val Loss: 1.7152, Val Acc: 0.9282\n",
            "Epoch [1471/4000], Train Loss: 1.7137, Train Acc: 0.9261, Val Loss: 1.7177, Val Acc: 0.9212\n",
            "Epoch [1472/4000], Train Loss: 1.7143, Train Acc: 0.9253, Val Loss: 1.7139, Val Acc: 0.9231\n",
            "Epoch [1473/4000], Train Loss: 1.7137, Train Acc: 0.9263, Val Loss: 1.7125, Val Acc: 0.9263\n",
            "Epoch [1474/4000], Train Loss: 1.7139, Train Acc: 0.9252, Val Loss: 1.7187, Val Acc: 0.9231\n",
            "Epoch [1475/4000], Train Loss: 1.7151, Train Acc: 0.9255, Val Loss: 1.7124, Val Acc: 0.9269\n",
            "Epoch [1476/4000], Train Loss: 1.7136, Train Acc: 0.9252, Val Loss: 1.7118, Val Acc: 0.9250\n",
            "Epoch [1477/4000], Train Loss: 1.7130, Train Acc: 0.9260, Val Loss: 1.7108, Val Acc: 0.9263\n",
            "Epoch [1478/4000], Train Loss: 1.7135, Train Acc: 0.9263, Val Loss: 1.7155, Val Acc: 0.9269\n",
            "Epoch [1479/4000], Train Loss: 1.7146, Train Acc: 0.9239, Val Loss: 1.7151, Val Acc: 0.9263\n",
            "Epoch [1480/4000], Train Loss: 1.7128, Train Acc: 0.9280, Val Loss: 1.7120, Val Acc: 0.9250\n",
            "Epoch [1481/4000], Train Loss: 1.7151, Train Acc: 0.9255, Val Loss: 1.7112, Val Acc: 0.9263\n",
            "Epoch [1482/4000], Train Loss: 1.7147, Train Acc: 0.9247, Val Loss: 1.7141, Val Acc: 0.9276\n",
            "Epoch [1483/4000], Train Loss: 1.7129, Train Acc: 0.9269, Val Loss: 1.7141, Val Acc: 0.9231\n",
            "Epoch [1484/4000], Train Loss: 1.7133, Train Acc: 0.9271, Val Loss: 1.7137, Val Acc: 0.9244\n",
            "Epoch [1485/4000], Train Loss: 1.7150, Train Acc: 0.9245, Val Loss: 1.7137, Val Acc: 0.9237\n",
            "Epoch [1486/4000], Train Loss: 1.7138, Train Acc: 0.9253, Val Loss: 1.7111, Val Acc: 0.9263\n",
            "Epoch [1487/4000], Train Loss: 1.7123, Train Acc: 0.9264, Val Loss: 1.7156, Val Acc: 0.9192\n",
            "Epoch [1488/4000], Train Loss: 1.7142, Train Acc: 0.9236, Val Loss: 1.7101, Val Acc: 0.9282\n",
            "Epoch [1489/4000], Train Loss: 1.7128, Train Acc: 0.9258, Val Loss: 1.7129, Val Acc: 0.9263\n",
            "Epoch [1490/4000], Train Loss: 1.7119, Train Acc: 0.9263, Val Loss: 1.7100, Val Acc: 0.9276\n",
            "Epoch [1491/4000], Train Loss: 1.7143, Train Acc: 0.9256, Val Loss: 1.7129, Val Acc: 0.9224\n",
            "Epoch [1492/4000], Train Loss: 1.7151, Train Acc: 0.9220, Val Loss: 1.7133, Val Acc: 0.9263\n",
            "Epoch [1493/4000], Train Loss: 1.7132, Train Acc: 0.9266, Val Loss: 1.7127, Val Acc: 0.9250\n",
            "Epoch [1494/4000], Train Loss: 1.7147, Train Acc: 0.9242, Val Loss: 1.7131, Val Acc: 0.9276\n",
            "Epoch [1495/4000], Train Loss: 1.7131, Train Acc: 0.9266, Val Loss: 1.7133, Val Acc: 0.9250\n",
            "Epoch [1496/4000], Train Loss: 1.7135, Train Acc: 0.9266, Val Loss: 1.7148, Val Acc: 0.9244\n",
            "Epoch [1497/4000], Train Loss: 1.7132, Train Acc: 0.9269, Val Loss: 1.7117, Val Acc: 0.9244\n",
            "Epoch [1498/4000], Train Loss: 1.7133, Train Acc: 0.9269, Val Loss: 1.7160, Val Acc: 0.9205\n",
            "Epoch [1499/4000], Train Loss: 1.7141, Train Acc: 0.9268, Val Loss: 1.7116, Val Acc: 0.9231\n",
            "Epoch [1500/4000], Train Loss: 1.7127, Train Acc: 0.9266, Val Loss: 1.7119, Val Acc: 0.9263\n",
            "Epoch [1501/4000], Train Loss: 1.7142, Train Acc: 0.9261, Val Loss: 1.7163, Val Acc: 0.9231\n",
            "Epoch [1502/4000], Train Loss: 1.7147, Train Acc: 0.9261, Val Loss: 1.7203, Val Acc: 0.9160\n",
            "Epoch [1503/4000], Train Loss: 1.7128, Train Acc: 0.9258, Val Loss: 1.7219, Val Acc: 0.9135\n",
            "Epoch [1504/4000], Train Loss: 1.7150, Train Acc: 0.9239, Val Loss: 1.7200, Val Acc: 0.9231\n",
            "Epoch [1505/4000], Train Loss: 1.7131, Train Acc: 0.9268, Val Loss: 1.7126, Val Acc: 0.9237\n",
            "Epoch [1506/4000], Train Loss: 1.7139, Train Acc: 0.9250, Val Loss: 1.7149, Val Acc: 0.9250\n",
            "Epoch [1507/4000], Train Loss: 1.7124, Train Acc: 0.9255, Val Loss: 1.7138, Val Acc: 0.9256\n",
            "Epoch [1508/4000], Train Loss: 1.7146, Train Acc: 0.9236, Val Loss: 1.7205, Val Acc: 0.9250\n",
            "Epoch [1509/4000], Train Loss: 1.7169, Train Acc: 0.9224, Val Loss: 1.7157, Val Acc: 0.9263\n",
            "Epoch [1510/4000], Train Loss: 1.7130, Train Acc: 0.9256, Val Loss: 1.7112, Val Acc: 0.9276\n",
            "Epoch [1511/4000], Train Loss: 1.7127, Train Acc: 0.9269, Val Loss: 1.7183, Val Acc: 0.9250\n",
            "Epoch [1512/4000], Train Loss: 1.7143, Train Acc: 0.9264, Val Loss: 1.7241, Val Acc: 0.9244\n",
            "Epoch [1513/4000], Train Loss: 1.7152, Train Acc: 0.9252, Val Loss: 1.7156, Val Acc: 0.9256\n",
            "Epoch [1514/4000], Train Loss: 1.7130, Train Acc: 0.9258, Val Loss: 1.7158, Val Acc: 0.9256\n",
            "Epoch [1515/4000], Train Loss: 1.7137, Train Acc: 0.9258, Val Loss: 1.7105, Val Acc: 0.9250\n",
            "Epoch [1516/4000], Train Loss: 1.7133, Train Acc: 0.9269, Val Loss: 1.7120, Val Acc: 0.9256\n",
            "Epoch [1517/4000], Train Loss: 1.7131, Train Acc: 0.9261, Val Loss: 1.7214, Val Acc: 0.9205\n",
            "Epoch [1518/4000], Train Loss: 1.7142, Train Acc: 0.9250, Val Loss: 1.7129, Val Acc: 0.9288\n",
            "Epoch [1519/4000], Train Loss: 1.7125, Train Acc: 0.9261, Val Loss: 1.7109, Val Acc: 0.9256\n",
            "Epoch [1520/4000], Train Loss: 1.7119, Train Acc: 0.9244, Val Loss: 1.7154, Val Acc: 0.9276\n",
            "Epoch [1521/4000], Train Loss: 1.7131, Train Acc: 0.9255, Val Loss: 1.7151, Val Acc: 0.9263\n",
            "Epoch [1522/4000], Train Loss: 1.7143, Train Acc: 0.9258, Val Loss: 1.7174, Val Acc: 0.9192\n",
            "Epoch [1523/4000], Train Loss: 1.7139, Train Acc: 0.9261, Val Loss: 1.7118, Val Acc: 0.9263\n",
            "Epoch [1524/4000], Train Loss: 1.7141, Train Acc: 0.9250, Val Loss: 1.7105, Val Acc: 0.9256\n",
            "Epoch [1525/4000], Train Loss: 1.7125, Train Acc: 0.9276, Val Loss: 1.7102, Val Acc: 0.9288\n",
            "Epoch [1526/4000], Train Loss: 1.7136, Train Acc: 0.9247, Val Loss: 1.7135, Val Acc: 0.9276\n",
            "Epoch [1527/4000], Train Loss: 1.7141, Train Acc: 0.9258, Val Loss: 1.7147, Val Acc: 0.9276\n",
            "Epoch [1528/4000], Train Loss: 1.7123, Train Acc: 0.9271, Val Loss: 1.7176, Val Acc: 0.9212\n",
            "Epoch [1529/4000], Train Loss: 1.7129, Train Acc: 0.9263, Val Loss: 1.7226, Val Acc: 0.9237\n",
            "Epoch [1530/4000], Train Loss: 1.7139, Train Acc: 0.9268, Val Loss: 1.7134, Val Acc: 0.9224\n",
            "Epoch [1531/4000], Train Loss: 1.7123, Train Acc: 0.9272, Val Loss: 1.7144, Val Acc: 0.9282\n",
            "Epoch [1532/4000], Train Loss: 1.7127, Train Acc: 0.9272, Val Loss: 1.7187, Val Acc: 0.9250\n",
            "Epoch [1533/4000], Train Loss: 1.7147, Train Acc: 0.9253, Val Loss: 1.7123, Val Acc: 0.9282\n",
            "Epoch [1534/4000], Train Loss: 1.7143, Train Acc: 0.9264, Val Loss: 1.7162, Val Acc: 0.9269\n",
            "Epoch [1535/4000], Train Loss: 1.7157, Train Acc: 0.9245, Val Loss: 1.7112, Val Acc: 0.9256\n",
            "Epoch [1536/4000], Train Loss: 1.7141, Train Acc: 0.9271, Val Loss: 1.7175, Val Acc: 0.9224\n",
            "Epoch [1537/4000], Train Loss: 1.7142, Train Acc: 0.9252, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [1538/4000], Train Loss: 1.7133, Train Acc: 0.9271, Val Loss: 1.7202, Val Acc: 0.9250\n",
            "Epoch [1539/4000], Train Loss: 1.7128, Train Acc: 0.9256, Val Loss: 1.7113, Val Acc: 0.9308\n",
            "Epoch [1540/4000], Train Loss: 1.7149, Train Acc: 0.9248, Val Loss: 1.7128, Val Acc: 0.9263\n",
            "Epoch [1541/4000], Train Loss: 1.7148, Train Acc: 0.9245, Val Loss: 1.7127, Val Acc: 0.9237\n",
            "Epoch [1542/4000], Train Loss: 1.7145, Train Acc: 0.9226, Val Loss: 1.7144, Val Acc: 0.9263\n",
            "Epoch [1543/4000], Train Loss: 1.7124, Train Acc: 0.9261, Val Loss: 1.7145, Val Acc: 0.9263\n",
            "Epoch [1544/4000], Train Loss: 1.7127, Train Acc: 0.9255, Val Loss: 1.7177, Val Acc: 0.9250\n",
            "Epoch [1545/4000], Train Loss: 1.7132, Train Acc: 0.9258, Val Loss: 1.7152, Val Acc: 0.9231\n",
            "Epoch [1546/4000], Train Loss: 1.7148, Train Acc: 0.9250, Val Loss: 1.7104, Val Acc: 0.9282\n",
            "Epoch [1547/4000], Train Loss: 1.7150, Train Acc: 0.9263, Val Loss: 1.7214, Val Acc: 0.9205\n",
            "Epoch [1548/4000], Train Loss: 1.7134, Train Acc: 0.9255, Val Loss: 1.7180, Val Acc: 0.9179\n",
            "Epoch [1549/4000], Train Loss: 1.7120, Train Acc: 0.9284, Val Loss: 1.7131, Val Acc: 0.9244\n",
            "Epoch [1550/4000], Train Loss: 1.7152, Train Acc: 0.9252, Val Loss: 1.7142, Val Acc: 0.9237\n",
            "Epoch [1551/4000], Train Loss: 1.7168, Train Acc: 0.9237, Val Loss: 1.7183, Val Acc: 0.9218\n",
            "Epoch [1552/4000], Train Loss: 1.7167, Train Acc: 0.9231, Val Loss: 1.7147, Val Acc: 0.9263\n",
            "Epoch [1553/4000], Train Loss: 1.7129, Train Acc: 0.9255, Val Loss: 1.7106, Val Acc: 0.9269\n",
            "Epoch [1554/4000], Train Loss: 1.7121, Train Acc: 0.9263, Val Loss: 1.7164, Val Acc: 0.9282\n",
            "Epoch [1555/4000], Train Loss: 1.7132, Train Acc: 0.9260, Val Loss: 1.7154, Val Acc: 0.9244\n",
            "Epoch [1556/4000], Train Loss: 1.7133, Train Acc: 0.9271, Val Loss: 1.7136, Val Acc: 0.9237\n",
            "Epoch [1557/4000], Train Loss: 1.7125, Train Acc: 0.9250, Val Loss: 1.7091, Val Acc: 0.9250\n",
            "Epoch [1558/4000], Train Loss: 1.7128, Train Acc: 0.9264, Val Loss: 1.7149, Val Acc: 0.9224\n",
            "Epoch [1559/4000], Train Loss: 1.7126, Train Acc: 0.9255, Val Loss: 1.7135, Val Acc: 0.9256\n",
            "Epoch [1560/4000], Train Loss: 1.7144, Train Acc: 0.9245, Val Loss: 1.7295, Val Acc: 0.9115\n",
            "Epoch [1561/4000], Train Loss: 1.7151, Train Acc: 0.9237, Val Loss: 1.7180, Val Acc: 0.9224\n",
            "Epoch [1562/4000], Train Loss: 1.7123, Train Acc: 0.9264, Val Loss: 1.7156, Val Acc: 0.9218\n",
            "Epoch [1563/4000], Train Loss: 1.7137, Train Acc: 0.9248, Val Loss: 1.7135, Val Acc: 0.9282\n",
            "Epoch [1564/4000], Train Loss: 1.7128, Train Acc: 0.9279, Val Loss: 1.7143, Val Acc: 0.9250\n",
            "Epoch [1565/4000], Train Loss: 1.7150, Train Acc: 0.9239, Val Loss: 1.7199, Val Acc: 0.9231\n",
            "Epoch [1566/4000], Train Loss: 1.7145, Train Acc: 0.9252, Val Loss: 1.7123, Val Acc: 0.9244\n",
            "Epoch [1567/4000], Train Loss: 1.7119, Train Acc: 0.9266, Val Loss: 1.7136, Val Acc: 0.9308\n",
            "Epoch [1568/4000], Train Loss: 1.7130, Train Acc: 0.9274, Val Loss: 1.7095, Val Acc: 0.9276\n",
            "Epoch [1569/4000], Train Loss: 1.7116, Train Acc: 0.9264, Val Loss: 1.7125, Val Acc: 0.9263\n",
            "Epoch [1570/4000], Train Loss: 1.7129, Train Acc: 0.9269, Val Loss: 1.7193, Val Acc: 0.9218\n",
            "Epoch [1571/4000], Train Loss: 1.7138, Train Acc: 0.9258, Val Loss: 1.7116, Val Acc: 0.9276\n",
            "Epoch [1572/4000], Train Loss: 1.7118, Train Acc: 0.9274, Val Loss: 1.7160, Val Acc: 0.9263\n",
            "Epoch [1573/4000], Train Loss: 1.7128, Train Acc: 0.9266, Val Loss: 1.7135, Val Acc: 0.9263\n",
            "Epoch [1574/4000], Train Loss: 1.7126, Train Acc: 0.9263, Val Loss: 1.7138, Val Acc: 0.9263\n",
            "Epoch [1575/4000], Train Loss: 1.7150, Train Acc: 0.9242, Val Loss: 1.7130, Val Acc: 0.9288\n",
            "Epoch [1576/4000], Train Loss: 1.7139, Train Acc: 0.9248, Val Loss: 1.7176, Val Acc: 0.9205\n",
            "Epoch [1577/4000], Train Loss: 1.7138, Train Acc: 0.9250, Val Loss: 1.7108, Val Acc: 0.9288\n",
            "Epoch [1578/4000], Train Loss: 1.7133, Train Acc: 0.9244, Val Loss: 1.7127, Val Acc: 0.9282\n",
            "Epoch [1579/4000], Train Loss: 1.7118, Train Acc: 0.9266, Val Loss: 1.7156, Val Acc: 0.9186\n",
            "Epoch [1580/4000], Train Loss: 1.7144, Train Acc: 0.9248, Val Loss: 1.7131, Val Acc: 0.9295\n",
            "Epoch [1581/4000], Train Loss: 1.7129, Train Acc: 0.9256, Val Loss: 1.7144, Val Acc: 0.9256\n",
            "Epoch [1582/4000], Train Loss: 1.7139, Train Acc: 0.9247, Val Loss: 1.7171, Val Acc: 0.9263\n",
            "Epoch [1583/4000], Train Loss: 1.7137, Train Acc: 0.9248, Val Loss: 1.7136, Val Acc: 0.9256\n",
            "Epoch [1584/4000], Train Loss: 1.7151, Train Acc: 0.9247, Val Loss: 1.7159, Val Acc: 0.9276\n",
            "Epoch [1585/4000], Train Loss: 1.7146, Train Acc: 0.9264, Val Loss: 1.7176, Val Acc: 0.9199\n",
            "Epoch [1586/4000], Train Loss: 1.7118, Train Acc: 0.9269, Val Loss: 1.7162, Val Acc: 0.9237\n",
            "Epoch [1587/4000], Train Loss: 1.7130, Train Acc: 0.9284, Val Loss: 1.7130, Val Acc: 0.9237\n",
            "Epoch [1588/4000], Train Loss: 1.7138, Train Acc: 0.9258, Val Loss: 1.7135, Val Acc: 0.9256\n",
            "Epoch [1589/4000], Train Loss: 1.7132, Train Acc: 0.9280, Val Loss: 1.7227, Val Acc: 0.9237\n",
            "Epoch [1590/4000], Train Loss: 1.7132, Train Acc: 0.9261, Val Loss: 1.7125, Val Acc: 0.9256\n",
            "Epoch [1591/4000], Train Loss: 1.7152, Train Acc: 0.9242, Val Loss: 1.7117, Val Acc: 0.9256\n",
            "Epoch [1592/4000], Train Loss: 1.7140, Train Acc: 0.9245, Val Loss: 1.7180, Val Acc: 0.9224\n",
            "Epoch [1593/4000], Train Loss: 1.7133, Train Acc: 0.9264, Val Loss: 1.7121, Val Acc: 0.9282\n",
            "Epoch [1594/4000], Train Loss: 1.7141, Train Acc: 0.9256, Val Loss: 1.7121, Val Acc: 0.9263\n",
            "Epoch [1595/4000], Train Loss: 1.7146, Train Acc: 0.9258, Val Loss: 1.7090, Val Acc: 0.9263\n",
            "Epoch [1596/4000], Train Loss: 1.7152, Train Acc: 0.9239, Val Loss: 1.7152, Val Acc: 0.9282\n",
            "Epoch [1597/4000], Train Loss: 1.7146, Train Acc: 0.9256, Val Loss: 1.7111, Val Acc: 0.9256\n",
            "Epoch [1598/4000], Train Loss: 1.7137, Train Acc: 0.9260, Val Loss: 1.7189, Val Acc: 0.9231\n",
            "Epoch [1599/4000], Train Loss: 1.7125, Train Acc: 0.9263, Val Loss: 1.7099, Val Acc: 0.9301\n",
            "Epoch [1600/4000], Train Loss: 1.7127, Train Acc: 0.9269, Val Loss: 1.7112, Val Acc: 0.9288\n",
            "Epoch [1601/4000], Train Loss: 1.7153, Train Acc: 0.9242, Val Loss: 1.7095, Val Acc: 0.9295\n",
            "Epoch [1602/4000], Train Loss: 1.7133, Train Acc: 0.9263, Val Loss: 1.7151, Val Acc: 0.9250\n",
            "Epoch [1603/4000], Train Loss: 1.7128, Train Acc: 0.9274, Val Loss: 1.7163, Val Acc: 0.9205\n",
            "Epoch [1604/4000], Train Loss: 1.7131, Train Acc: 0.9248, Val Loss: 1.7113, Val Acc: 0.9263\n",
            "Epoch [1605/4000], Train Loss: 1.7128, Train Acc: 0.9277, Val Loss: 1.7136, Val Acc: 0.9295\n",
            "Epoch [1606/4000], Train Loss: 1.7131, Train Acc: 0.9255, Val Loss: 1.7117, Val Acc: 0.9282\n",
            "Epoch [1607/4000], Train Loss: 1.7123, Train Acc: 0.9269, Val Loss: 1.7106, Val Acc: 0.9263\n",
            "Epoch [1608/4000], Train Loss: 1.7144, Train Acc: 0.9248, Val Loss: 1.7257, Val Acc: 0.9167\n",
            "Epoch [1609/4000], Train Loss: 1.7153, Train Acc: 0.9255, Val Loss: 1.7172, Val Acc: 0.9282\n",
            "Epoch [1610/4000], Train Loss: 1.7134, Train Acc: 0.9250, Val Loss: 1.7107, Val Acc: 0.9231\n",
            "Epoch [1611/4000], Train Loss: 1.7127, Train Acc: 0.9269, Val Loss: 1.7093, Val Acc: 0.9269\n",
            "Epoch [1612/4000], Train Loss: 1.7113, Train Acc: 0.9277, Val Loss: 1.7109, Val Acc: 0.9276\n",
            "Epoch [1613/4000], Train Loss: 1.7135, Train Acc: 0.9279, Val Loss: 1.7112, Val Acc: 0.9295\n",
            "Epoch [1614/4000], Train Loss: 1.7134, Train Acc: 0.9260, Val Loss: 1.7105, Val Acc: 0.9295\n",
            "Epoch [1615/4000], Train Loss: 1.7136, Train Acc: 0.9263, Val Loss: 1.7170, Val Acc: 0.9237\n",
            "Epoch [1616/4000], Train Loss: 1.7134, Train Acc: 0.9247, Val Loss: 1.7242, Val Acc: 0.9212\n",
            "Epoch [1617/4000], Train Loss: 1.7143, Train Acc: 0.9276, Val Loss: 1.7117, Val Acc: 0.9276\n",
            "Epoch [1618/4000], Train Loss: 1.7131, Train Acc: 0.9271, Val Loss: 1.7117, Val Acc: 0.9256\n",
            "Epoch [1619/4000], Train Loss: 1.7140, Train Acc: 0.9256, Val Loss: 1.7119, Val Acc: 0.9288\n",
            "Epoch [1620/4000], Train Loss: 1.7118, Train Acc: 0.9266, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [1621/4000], Train Loss: 1.7141, Train Acc: 0.9261, Val Loss: 1.7125, Val Acc: 0.9263\n",
            "Epoch [1622/4000], Train Loss: 1.7125, Train Acc: 0.9279, Val Loss: 1.7082, Val Acc: 0.9269\n",
            "Epoch [1623/4000], Train Loss: 1.7126, Train Acc: 0.9266, Val Loss: 1.7153, Val Acc: 0.9244\n",
            "Epoch [1624/4000], Train Loss: 1.7119, Train Acc: 0.9271, Val Loss: 1.7128, Val Acc: 0.9263\n",
            "Epoch [1625/4000], Train Loss: 1.7143, Train Acc: 0.9264, Val Loss: 1.7177, Val Acc: 0.9199\n",
            "Epoch [1626/4000], Train Loss: 1.7142, Train Acc: 0.9252, Val Loss: 1.7097, Val Acc: 0.9269\n",
            "Epoch [1627/4000], Train Loss: 1.7147, Train Acc: 0.9253, Val Loss: 1.7139, Val Acc: 0.9263\n",
            "Epoch [1628/4000], Train Loss: 1.7138, Train Acc: 0.9271, Val Loss: 1.7092, Val Acc: 0.9288\n",
            "Epoch [1629/4000], Train Loss: 1.7129, Train Acc: 0.9269, Val Loss: 1.7119, Val Acc: 0.9308\n",
            "Epoch [1630/4000], Train Loss: 1.7125, Train Acc: 0.9271, Val Loss: 1.7128, Val Acc: 0.9263\n",
            "Epoch [1631/4000], Train Loss: 1.7120, Train Acc: 0.9274, Val Loss: 1.7111, Val Acc: 0.9301\n",
            "Epoch [1632/4000], Train Loss: 1.7131, Train Acc: 0.9256, Val Loss: 1.7141, Val Acc: 0.9301\n",
            "Epoch [1633/4000], Train Loss: 1.7134, Train Acc: 0.9261, Val Loss: 1.7110, Val Acc: 0.9282\n",
            "Epoch [1634/4000], Train Loss: 1.7110, Train Acc: 0.9268, Val Loss: 1.7112, Val Acc: 0.9288\n",
            "Epoch [1635/4000], Train Loss: 1.7124, Train Acc: 0.9274, Val Loss: 1.7193, Val Acc: 0.9231\n",
            "Epoch [1636/4000], Train Loss: 1.7129, Train Acc: 0.9268, Val Loss: 1.7111, Val Acc: 0.9263\n",
            "Epoch [1637/4000], Train Loss: 1.7132, Train Acc: 0.9261, Val Loss: 1.7147, Val Acc: 0.9250\n",
            "Epoch [1638/4000], Train Loss: 1.7123, Train Acc: 0.9272, Val Loss: 1.7117, Val Acc: 0.9256\n",
            "Epoch [1639/4000], Train Loss: 1.7135, Train Acc: 0.9263, Val Loss: 1.7154, Val Acc: 0.9308\n",
            "Epoch [1640/4000], Train Loss: 1.7127, Train Acc: 0.9261, Val Loss: 1.7086, Val Acc: 0.9276\n",
            "Epoch [1641/4000], Train Loss: 1.7147, Train Acc: 0.9266, Val Loss: 1.7102, Val Acc: 0.9237\n",
            "Epoch [1642/4000], Train Loss: 1.7134, Train Acc: 0.9250, Val Loss: 1.7117, Val Acc: 0.9288\n",
            "Epoch [1643/4000], Train Loss: 1.7131, Train Acc: 0.9266, Val Loss: 1.7110, Val Acc: 0.9256\n",
            "Epoch [1644/4000], Train Loss: 1.7120, Train Acc: 0.9274, Val Loss: 1.7120, Val Acc: 0.9250\n",
            "Epoch [1645/4000], Train Loss: 1.7116, Train Acc: 0.9279, Val Loss: 1.7113, Val Acc: 0.9269\n",
            "Epoch [1646/4000], Train Loss: 1.7135, Train Acc: 0.9276, Val Loss: 1.7142, Val Acc: 0.9288\n",
            "Epoch [1647/4000], Train Loss: 1.7121, Train Acc: 0.9264, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [1648/4000], Train Loss: 1.7131, Train Acc: 0.9272, Val Loss: 1.7100, Val Acc: 0.9269\n",
            "Epoch [1649/4000], Train Loss: 1.7142, Train Acc: 0.9261, Val Loss: 1.7120, Val Acc: 0.9256\n",
            "Epoch [1650/4000], Train Loss: 1.7124, Train Acc: 0.9277, Val Loss: 1.7098, Val Acc: 0.9276\n",
            "Epoch [1651/4000], Train Loss: 1.7111, Train Acc: 0.9266, Val Loss: 1.7140, Val Acc: 0.9269\n",
            "Epoch [1652/4000], Train Loss: 1.7133, Train Acc: 0.9268, Val Loss: 1.7155, Val Acc: 0.9288\n",
            "Epoch [1653/4000], Train Loss: 1.7147, Train Acc: 0.9250, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [1654/4000], Train Loss: 1.7136, Train Acc: 0.9272, Val Loss: 1.7111, Val Acc: 0.9288\n",
            "Epoch [1655/4000], Train Loss: 1.7128, Train Acc: 0.9272, Val Loss: 1.7109, Val Acc: 0.9288\n",
            "Epoch [1656/4000], Train Loss: 1.7138, Train Acc: 0.9272, Val Loss: 1.7153, Val Acc: 0.9282\n",
            "Epoch [1657/4000], Train Loss: 1.7138, Train Acc: 0.9245, Val Loss: 1.7114, Val Acc: 0.9263\n",
            "Epoch [1658/4000], Train Loss: 1.7147, Train Acc: 0.9250, Val Loss: 1.7182, Val Acc: 0.9269\n",
            "Epoch [1659/4000], Train Loss: 1.7128, Train Acc: 0.9264, Val Loss: 1.7144, Val Acc: 0.9276\n",
            "Epoch [1660/4000], Train Loss: 1.7107, Train Acc: 0.9293, Val Loss: 1.7109, Val Acc: 0.9244\n",
            "Epoch [1661/4000], Train Loss: 1.7138, Train Acc: 0.9274, Val Loss: 1.7116, Val Acc: 0.9263\n",
            "Epoch [1662/4000], Train Loss: 1.7121, Train Acc: 0.9279, Val Loss: 1.7112, Val Acc: 0.9288\n",
            "Epoch [1663/4000], Train Loss: 1.7127, Train Acc: 0.9266, Val Loss: 1.7193, Val Acc: 0.9199\n",
            "Epoch [1664/4000], Train Loss: 1.7138, Train Acc: 0.9239, Val Loss: 1.7149, Val Acc: 0.9263\n",
            "Epoch [1665/4000], Train Loss: 1.7133, Train Acc: 0.9272, Val Loss: 1.7126, Val Acc: 0.9237\n",
            "Epoch [1666/4000], Train Loss: 1.7123, Train Acc: 0.9276, Val Loss: 1.7133, Val Acc: 0.9263\n",
            "Epoch [1667/4000], Train Loss: 1.7145, Train Acc: 0.9266, Val Loss: 1.7145, Val Acc: 0.9276\n",
            "Epoch [1668/4000], Train Loss: 1.7138, Train Acc: 0.9260, Val Loss: 1.7108, Val Acc: 0.9269\n",
            "Epoch [1669/4000], Train Loss: 1.7119, Train Acc: 0.9264, Val Loss: 1.7127, Val Acc: 0.9295\n",
            "Epoch [1670/4000], Train Loss: 1.7131, Train Acc: 0.9248, Val Loss: 1.7110, Val Acc: 0.9244\n",
            "Epoch [1671/4000], Train Loss: 1.7126, Train Acc: 0.9276, Val Loss: 1.7182, Val Acc: 0.9231\n",
            "Epoch [1672/4000], Train Loss: 1.7130, Train Acc: 0.9258, Val Loss: 1.7115, Val Acc: 0.9276\n",
            "Epoch [1673/4000], Train Loss: 1.7125, Train Acc: 0.9277, Val Loss: 1.7117, Val Acc: 0.9282\n",
            "Epoch [1674/4000], Train Loss: 1.7118, Train Acc: 0.9266, Val Loss: 1.7161, Val Acc: 0.9263\n",
            "Epoch [1675/4000], Train Loss: 1.7128, Train Acc: 0.9269, Val Loss: 1.7152, Val Acc: 0.9256\n",
            "Epoch [1676/4000], Train Loss: 1.7141, Train Acc: 0.9250, Val Loss: 1.7179, Val Acc: 0.9224\n",
            "Epoch [1677/4000], Train Loss: 1.7130, Train Acc: 0.9264, Val Loss: 1.7238, Val Acc: 0.9186\n",
            "Epoch [1678/4000], Train Loss: 1.7131, Train Acc: 0.9263, Val Loss: 1.7132, Val Acc: 0.9237\n",
            "Epoch [1679/4000], Train Loss: 1.7116, Train Acc: 0.9271, Val Loss: 1.7170, Val Acc: 0.9256\n",
            "Epoch [1680/4000], Train Loss: 1.7138, Train Acc: 0.9269, Val Loss: 1.7111, Val Acc: 0.9276\n",
            "Epoch [1681/4000], Train Loss: 1.7121, Train Acc: 0.9274, Val Loss: 1.7146, Val Acc: 0.9256\n",
            "Epoch [1682/4000], Train Loss: 1.7116, Train Acc: 0.9276, Val Loss: 1.7219, Val Acc: 0.9224\n",
            "Epoch [1683/4000], Train Loss: 1.7142, Train Acc: 0.9264, Val Loss: 1.7197, Val Acc: 0.9192\n",
            "Epoch [1684/4000], Train Loss: 1.7132, Train Acc: 0.9264, Val Loss: 1.7197, Val Acc: 0.9212\n",
            "Epoch [1685/4000], Train Loss: 1.7126, Train Acc: 0.9268, Val Loss: 1.7105, Val Acc: 0.9301\n",
            "Epoch [1686/4000], Train Loss: 1.7125, Train Acc: 0.9268, Val Loss: 1.7095, Val Acc: 0.9288\n",
            "Epoch [1687/4000], Train Loss: 1.7122, Train Acc: 0.9261, Val Loss: 1.7116, Val Acc: 0.9288\n",
            "Epoch [1688/4000], Train Loss: 1.7127, Train Acc: 0.9261, Val Loss: 1.7128, Val Acc: 0.9276\n",
            "Epoch [1689/4000], Train Loss: 1.7130, Train Acc: 0.9271, Val Loss: 1.7114, Val Acc: 0.9276\n",
            "Epoch [1690/4000], Train Loss: 1.7155, Train Acc: 0.9247, Val Loss: 1.7115, Val Acc: 0.9269\n",
            "Epoch [1691/4000], Train Loss: 1.7110, Train Acc: 0.9271, Val Loss: 1.7126, Val Acc: 0.9256\n",
            "Epoch [1692/4000], Train Loss: 1.7121, Train Acc: 0.9256, Val Loss: 1.7170, Val Acc: 0.9224\n",
            "Epoch [1693/4000], Train Loss: 1.7136, Train Acc: 0.9248, Val Loss: 1.7120, Val Acc: 0.9295\n",
            "Epoch [1694/4000], Train Loss: 1.7142, Train Acc: 0.9252, Val Loss: 1.7132, Val Acc: 0.9256\n",
            "Epoch [1695/4000], Train Loss: 1.7118, Train Acc: 0.9277, Val Loss: 1.7117, Val Acc: 0.9263\n",
            "Epoch [1696/4000], Train Loss: 1.7157, Train Acc: 0.9244, Val Loss: 1.7114, Val Acc: 0.9288\n",
            "Epoch [1697/4000], Train Loss: 1.7133, Train Acc: 0.9271, Val Loss: 1.7141, Val Acc: 0.9288\n",
            "Epoch [1698/4000], Train Loss: 1.7133, Train Acc: 0.9264, Val Loss: 1.7170, Val Acc: 0.9250\n",
            "Epoch [1699/4000], Train Loss: 1.7127, Train Acc: 0.9266, Val Loss: 1.7121, Val Acc: 0.9288\n",
            "Epoch [1700/4000], Train Loss: 1.7130, Train Acc: 0.9261, Val Loss: 1.7112, Val Acc: 0.9282\n",
            "Epoch [1701/4000], Train Loss: 1.7134, Train Acc: 0.9274, Val Loss: 1.7231, Val Acc: 0.9212\n",
            "Epoch [1702/4000], Train Loss: 1.7134, Train Acc: 0.9260, Val Loss: 1.7144, Val Acc: 0.9295\n",
            "Epoch [1703/4000], Train Loss: 1.7137, Train Acc: 0.9260, Val Loss: 1.7111, Val Acc: 0.9269\n",
            "Epoch [1704/4000], Train Loss: 1.7142, Train Acc: 0.9260, Val Loss: 1.7122, Val Acc: 0.9237\n",
            "Epoch [1705/4000], Train Loss: 1.7130, Train Acc: 0.9261, Val Loss: 1.7140, Val Acc: 0.9276\n",
            "Epoch [1706/4000], Train Loss: 1.7118, Train Acc: 0.9276, Val Loss: 1.7117, Val Acc: 0.9269\n",
            "Epoch [1707/4000], Train Loss: 1.7116, Train Acc: 0.9287, Val Loss: 1.7119, Val Acc: 0.9269\n",
            "Epoch [1708/4000], Train Loss: 1.7112, Train Acc: 0.9277, Val Loss: 1.7175, Val Acc: 0.9224\n",
            "Epoch [1709/4000], Train Loss: 1.7116, Train Acc: 0.9274, Val Loss: 1.7186, Val Acc: 0.9237\n",
            "Epoch [1710/4000], Train Loss: 1.7130, Train Acc: 0.9274, Val Loss: 1.7094, Val Acc: 0.9288\n",
            "Epoch [1711/4000], Train Loss: 1.7132, Train Acc: 0.9269, Val Loss: 1.7183, Val Acc: 0.9224\n",
            "Epoch [1712/4000], Train Loss: 1.7140, Train Acc: 0.9239, Val Loss: 1.7111, Val Acc: 0.9301\n",
            "Epoch [1713/4000], Train Loss: 1.7116, Train Acc: 0.9271, Val Loss: 1.7221, Val Acc: 0.9218\n",
            "Epoch [1714/4000], Train Loss: 1.7122, Train Acc: 0.9272, Val Loss: 1.7112, Val Acc: 0.9263\n",
            "Epoch [1715/4000], Train Loss: 1.7149, Train Acc: 0.9240, Val Loss: 1.7144, Val Acc: 0.9250\n",
            "Epoch [1716/4000], Train Loss: 1.7145, Train Acc: 0.9261, Val Loss: 1.7159, Val Acc: 0.9250\n",
            "Epoch [1717/4000], Train Loss: 1.7142, Train Acc: 0.9252, Val Loss: 1.7109, Val Acc: 0.9282\n",
            "Epoch [1718/4000], Train Loss: 1.7120, Train Acc: 0.9276, Val Loss: 1.7130, Val Acc: 0.9276\n",
            "Epoch [1719/4000], Train Loss: 1.7127, Train Acc: 0.9266, Val Loss: 1.7125, Val Acc: 0.9263\n",
            "Epoch [1720/4000], Train Loss: 1.7118, Train Acc: 0.9264, Val Loss: 1.7118, Val Acc: 0.9231\n",
            "Epoch [1721/4000], Train Loss: 1.7153, Train Acc: 0.9239, Val Loss: 1.7141, Val Acc: 0.9244\n",
            "Epoch [1722/4000], Train Loss: 1.7145, Train Acc: 0.9248, Val Loss: 1.7141, Val Acc: 0.9250\n",
            "Epoch [1723/4000], Train Loss: 1.7130, Train Acc: 0.9279, Val Loss: 1.7100, Val Acc: 0.9295\n",
            "Epoch [1724/4000], Train Loss: 1.7127, Train Acc: 0.9263, Val Loss: 1.7130, Val Acc: 0.9244\n",
            "Epoch [1725/4000], Train Loss: 1.7136, Train Acc: 0.9253, Val Loss: 1.7167, Val Acc: 0.9276\n",
            "Epoch [1726/4000], Train Loss: 1.7140, Train Acc: 0.9252, Val Loss: 1.7126, Val Acc: 0.9276\n",
            "Epoch [1727/4000], Train Loss: 1.7124, Train Acc: 0.9271, Val Loss: 1.7089, Val Acc: 0.9269\n",
            "Epoch [1728/4000], Train Loss: 1.7133, Train Acc: 0.9256, Val Loss: 1.7326, Val Acc: 0.9038\n",
            "Epoch [1729/4000], Train Loss: 1.7159, Train Acc: 0.9250, Val Loss: 1.7088, Val Acc: 0.9314\n",
            "Epoch [1730/4000], Train Loss: 1.7131, Train Acc: 0.9269, Val Loss: 1.7145, Val Acc: 0.9250\n",
            "Epoch [1731/4000], Train Loss: 1.7122, Train Acc: 0.9266, Val Loss: 1.7140, Val Acc: 0.9269\n",
            "Epoch [1732/4000], Train Loss: 1.7152, Train Acc: 0.9255, Val Loss: 1.7169, Val Acc: 0.9218\n",
            "Epoch [1733/4000], Train Loss: 1.7125, Train Acc: 0.9263, Val Loss: 1.7072, Val Acc: 0.9282\n",
            "Epoch [1734/4000], Train Loss: 1.7126, Train Acc: 0.9263, Val Loss: 1.7136, Val Acc: 0.9256\n",
            "Epoch [1735/4000], Train Loss: 1.7115, Train Acc: 0.9276, Val Loss: 1.7111, Val Acc: 0.9295\n",
            "Epoch [1736/4000], Train Loss: 1.7129, Train Acc: 0.9260, Val Loss: 1.7161, Val Acc: 0.9250\n",
            "Epoch [1737/4000], Train Loss: 1.7143, Train Acc: 0.9253, Val Loss: 1.7123, Val Acc: 0.9276\n",
            "Epoch [1738/4000], Train Loss: 1.7133, Train Acc: 0.9268, Val Loss: 1.7092, Val Acc: 0.9288\n",
            "Epoch [1739/4000], Train Loss: 1.7132, Train Acc: 0.9245, Val Loss: 1.7090, Val Acc: 0.9282\n",
            "Epoch [1740/4000], Train Loss: 1.7115, Train Acc: 0.9277, Val Loss: 1.7173, Val Acc: 0.9205\n",
            "Epoch [1741/4000], Train Loss: 1.7121, Train Acc: 0.9269, Val Loss: 1.7150, Val Acc: 0.9276\n",
            "Epoch [1742/4000], Train Loss: 1.7124, Train Acc: 0.9256, Val Loss: 1.7112, Val Acc: 0.9295\n",
            "Epoch [1743/4000], Train Loss: 1.7114, Train Acc: 0.9277, Val Loss: 1.7097, Val Acc: 0.9282\n",
            "Epoch [1744/4000], Train Loss: 1.7119, Train Acc: 0.9287, Val Loss: 1.7091, Val Acc: 0.9276\n",
            "Epoch [1745/4000], Train Loss: 1.7120, Train Acc: 0.9277, Val Loss: 1.7108, Val Acc: 0.9314\n",
            "Epoch [1746/4000], Train Loss: 1.7123, Train Acc: 0.9274, Val Loss: 1.7138, Val Acc: 0.9288\n",
            "Epoch [1747/4000], Train Loss: 1.7133, Train Acc: 0.9269, Val Loss: 1.7080, Val Acc: 0.9295\n",
            "Epoch [1748/4000], Train Loss: 1.7149, Train Acc: 0.9280, Val Loss: 1.7142, Val Acc: 0.9256\n",
            "Epoch [1749/4000], Train Loss: 1.7131, Train Acc: 0.9253, Val Loss: 1.7144, Val Acc: 0.9276\n",
            "Epoch [1750/4000], Train Loss: 1.7136, Train Acc: 0.9264, Val Loss: 1.7128, Val Acc: 0.9282\n",
            "Epoch [1751/4000], Train Loss: 1.7118, Train Acc: 0.9264, Val Loss: 1.7105, Val Acc: 0.9288\n",
            "Epoch [1752/4000], Train Loss: 1.7127, Train Acc: 0.9247, Val Loss: 1.7147, Val Acc: 0.9263\n",
            "Epoch [1753/4000], Train Loss: 1.7123, Train Acc: 0.9269, Val Loss: 1.7199, Val Acc: 0.9205\n",
            "Epoch [1754/4000], Train Loss: 1.7122, Train Acc: 0.9271, Val Loss: 1.7096, Val Acc: 0.9269\n",
            "Epoch [1755/4000], Train Loss: 1.7138, Train Acc: 0.9258, Val Loss: 1.7123, Val Acc: 0.9244\n",
            "Epoch [1756/4000], Train Loss: 1.7119, Train Acc: 0.9264, Val Loss: 1.7118, Val Acc: 0.9263\n",
            "Epoch [1757/4000], Train Loss: 1.7130, Train Acc: 0.9260, Val Loss: 1.7197, Val Acc: 0.9224\n",
            "Epoch [1758/4000], Train Loss: 1.7126, Train Acc: 0.9263, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [1759/4000], Train Loss: 1.7108, Train Acc: 0.9282, Val Loss: 1.7084, Val Acc: 0.9301\n",
            "Epoch [1760/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7131, Val Acc: 0.9282\n",
            "Epoch [1761/4000], Train Loss: 1.7126, Train Acc: 0.9269, Val Loss: 1.7103, Val Acc: 0.9288\n",
            "Epoch [1762/4000], Train Loss: 1.7132, Train Acc: 0.9263, Val Loss: 1.7169, Val Acc: 0.9231\n",
            "Epoch [1763/4000], Train Loss: 1.7133, Train Acc: 0.9253, Val Loss: 1.7137, Val Acc: 0.9256\n",
            "Epoch [1764/4000], Train Loss: 1.7110, Train Acc: 0.9274, Val Loss: 1.7134, Val Acc: 0.9231\n",
            "Epoch [1765/4000], Train Loss: 1.7123, Train Acc: 0.9279, Val Loss: 1.7171, Val Acc: 0.9218\n",
            "Epoch [1766/4000], Train Loss: 1.7145, Train Acc: 0.9260, Val Loss: 1.7128, Val Acc: 0.9263\n",
            "Epoch [1767/4000], Train Loss: 1.7131, Train Acc: 0.9268, Val Loss: 1.7108, Val Acc: 0.9282\n",
            "Epoch [1768/4000], Train Loss: 1.7133, Train Acc: 0.9253, Val Loss: 1.7140, Val Acc: 0.9263\n",
            "Epoch [1769/4000], Train Loss: 1.7123, Train Acc: 0.9255, Val Loss: 1.7104, Val Acc: 0.9256\n",
            "Epoch [1770/4000], Train Loss: 1.7127, Train Acc: 0.9272, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [1771/4000], Train Loss: 1.7131, Train Acc: 0.9248, Val Loss: 1.7135, Val Acc: 0.9282\n",
            "Epoch [1772/4000], Train Loss: 1.7131, Train Acc: 0.9260, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [1773/4000], Train Loss: 1.7114, Train Acc: 0.9279, Val Loss: 1.7114, Val Acc: 0.9295\n",
            "Epoch [1774/4000], Train Loss: 1.7126, Train Acc: 0.9253, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [1775/4000], Train Loss: 1.7136, Train Acc: 0.9271, Val Loss: 1.7107, Val Acc: 0.9282\n",
            "Epoch [1776/4000], Train Loss: 1.7148, Train Acc: 0.9253, Val Loss: 1.7136, Val Acc: 0.9244\n",
            "Epoch [1777/4000], Train Loss: 1.7119, Train Acc: 0.9261, Val Loss: 1.7137, Val Acc: 0.9269\n",
            "Epoch [1778/4000], Train Loss: 1.7123, Train Acc: 0.9274, Val Loss: 1.7121, Val Acc: 0.9231\n",
            "Epoch [1779/4000], Train Loss: 1.7107, Train Acc: 0.9290, Val Loss: 1.7113, Val Acc: 0.9288\n",
            "Epoch [1780/4000], Train Loss: 1.7123, Train Acc: 0.9279, Val Loss: 1.7270, Val Acc: 0.9199\n",
            "Epoch [1781/4000], Train Loss: 1.7138, Train Acc: 0.9250, Val Loss: 1.7143, Val Acc: 0.9212\n",
            "Epoch [1782/4000], Train Loss: 1.7118, Train Acc: 0.9279, Val Loss: 1.7140, Val Acc: 0.9295\n",
            "Epoch [1783/4000], Train Loss: 1.7111, Train Acc: 0.9272, Val Loss: 1.7092, Val Acc: 0.9308\n",
            "Epoch [1784/4000], Train Loss: 1.7114, Train Acc: 0.9277, Val Loss: 1.7076, Val Acc: 0.9295\n",
            "Epoch [1785/4000], Train Loss: 1.7133, Train Acc: 0.9263, Val Loss: 1.7113, Val Acc: 0.9282\n",
            "Epoch [1786/4000], Train Loss: 1.7131, Train Acc: 0.9269, Val Loss: 1.7119, Val Acc: 0.9269\n",
            "Epoch [1787/4000], Train Loss: 1.7126, Train Acc: 0.9276, Val Loss: 1.7122, Val Acc: 0.9244\n",
            "Epoch [1788/4000], Train Loss: 1.7113, Train Acc: 0.9276, Val Loss: 1.7112, Val Acc: 0.9288\n",
            "Epoch [1789/4000], Train Loss: 1.7139, Train Acc: 0.9245, Val Loss: 1.7319, Val Acc: 0.9205\n",
            "Epoch [1790/4000], Train Loss: 1.7150, Train Acc: 0.9274, Val Loss: 1.7115, Val Acc: 0.9282\n",
            "Epoch [1791/4000], Train Loss: 1.7110, Train Acc: 0.9279, Val Loss: 1.7137, Val Acc: 0.9244\n",
            "Epoch [1792/4000], Train Loss: 1.7129, Train Acc: 0.9271, Val Loss: 1.7122, Val Acc: 0.9308\n",
            "Epoch [1793/4000], Train Loss: 1.7119, Train Acc: 0.9268, Val Loss: 1.7105, Val Acc: 0.9276\n",
            "Epoch [1794/4000], Train Loss: 1.7115, Train Acc: 0.9276, Val Loss: 1.7153, Val Acc: 0.9269\n",
            "Epoch [1795/4000], Train Loss: 1.7116, Train Acc: 0.9287, Val Loss: 1.7105, Val Acc: 0.9276\n",
            "Epoch [1796/4000], Train Loss: 1.7133, Train Acc: 0.9253, Val Loss: 1.7110, Val Acc: 0.9269\n",
            "Epoch [1797/4000], Train Loss: 1.7117, Train Acc: 0.9263, Val Loss: 1.7109, Val Acc: 0.9250\n",
            "Epoch [1798/4000], Train Loss: 1.7126, Train Acc: 0.9271, Val Loss: 1.7092, Val Acc: 0.9301\n",
            "Epoch [1799/4000], Train Loss: 1.7125, Train Acc: 0.9258, Val Loss: 1.7128, Val Acc: 0.9256\n",
            "Epoch [1800/4000], Train Loss: 1.7129, Train Acc: 0.9285, Val Loss: 1.7097, Val Acc: 0.9276\n",
            "Epoch [1801/4000], Train Loss: 1.7123, Train Acc: 0.9263, Val Loss: 1.7182, Val Acc: 0.9237\n",
            "Epoch [1802/4000], Train Loss: 1.7130, Train Acc: 0.9276, Val Loss: 1.7121, Val Acc: 0.9250\n",
            "Epoch [1803/4000], Train Loss: 1.7123, Train Acc: 0.9272, Val Loss: 1.7140, Val Acc: 0.9263\n",
            "Epoch [1804/4000], Train Loss: 1.7113, Train Acc: 0.9274, Val Loss: 1.7104, Val Acc: 0.9269\n",
            "Epoch [1805/4000], Train Loss: 1.7142, Train Acc: 0.9237, Val Loss: 1.7125, Val Acc: 0.9276\n",
            "Epoch [1806/4000], Train Loss: 1.7129, Train Acc: 0.9271, Val Loss: 1.7108, Val Acc: 0.9282\n",
            "Epoch [1807/4000], Train Loss: 1.7113, Train Acc: 0.9271, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [1808/4000], Train Loss: 1.7111, Train Acc: 0.9279, Val Loss: 1.7121, Val Acc: 0.9282\n",
            "Epoch [1809/4000], Train Loss: 1.7119, Train Acc: 0.9261, Val Loss: 1.7093, Val Acc: 0.9288\n",
            "Epoch [1810/4000], Train Loss: 1.7137, Train Acc: 0.9263, Val Loss: 1.7180, Val Acc: 0.9282\n",
            "Epoch [1811/4000], Train Loss: 1.7120, Train Acc: 0.9269, Val Loss: 1.7106, Val Acc: 0.9308\n",
            "Epoch [1812/4000], Train Loss: 1.7113, Train Acc: 0.9277, Val Loss: 1.7098, Val Acc: 0.9314\n",
            "Epoch [1813/4000], Train Loss: 1.7131, Train Acc: 0.9269, Val Loss: 1.7147, Val Acc: 0.9263\n",
            "Epoch [1814/4000], Train Loss: 1.7117, Train Acc: 0.9271, Val Loss: 1.7101, Val Acc: 0.9308\n",
            "Epoch [1815/4000], Train Loss: 1.7135, Train Acc: 0.9258, Val Loss: 1.7109, Val Acc: 0.9295\n",
            "Epoch [1816/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7091, Val Acc: 0.9288\n",
            "Epoch [1817/4000], Train Loss: 1.7112, Train Acc: 0.9277, Val Loss: 1.7114, Val Acc: 0.9301\n",
            "Epoch [1818/4000], Train Loss: 1.7117, Train Acc: 0.9271, Val Loss: 1.7125, Val Acc: 0.9282\n",
            "Epoch [1819/4000], Train Loss: 1.7118, Train Acc: 0.9282, Val Loss: 1.7140, Val Acc: 0.9276\n",
            "Epoch [1820/4000], Train Loss: 1.7125, Train Acc: 0.9269, Val Loss: 1.7080, Val Acc: 0.9308\n",
            "Epoch [1821/4000], Train Loss: 1.7126, Train Acc: 0.9276, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [1822/4000], Train Loss: 1.7106, Train Acc: 0.9287, Val Loss: 1.7112, Val Acc: 0.9282\n",
            "Epoch [1823/4000], Train Loss: 1.7110, Train Acc: 0.9266, Val Loss: 1.7095, Val Acc: 0.9288\n",
            "Epoch [1824/4000], Train Loss: 1.7121, Train Acc: 0.9276, Val Loss: 1.7135, Val Acc: 0.9276\n",
            "Epoch [1825/4000], Train Loss: 1.7152, Train Acc: 0.9248, Val Loss: 1.7114, Val Acc: 0.9327\n",
            "Epoch [1826/4000], Train Loss: 1.7119, Train Acc: 0.9272, Val Loss: 1.7131, Val Acc: 0.9244\n",
            "Epoch [1827/4000], Train Loss: 1.7117, Train Acc: 0.9279, Val Loss: 1.7094, Val Acc: 0.9282\n",
            "Epoch [1828/4000], Train Loss: 1.7111, Train Acc: 0.9288, Val Loss: 1.7158, Val Acc: 0.9256\n",
            "Epoch [1829/4000], Train Loss: 1.7118, Train Acc: 0.9282, Val Loss: 1.7122, Val Acc: 0.9276\n",
            "Epoch [1830/4000], Train Loss: 1.7122, Train Acc: 0.9276, Val Loss: 1.7111, Val Acc: 0.9308\n",
            "Epoch [1831/4000], Train Loss: 1.7121, Train Acc: 0.9268, Val Loss: 1.7119, Val Acc: 0.9288\n",
            "Epoch [1832/4000], Train Loss: 1.7111, Train Acc: 0.9290, Val Loss: 1.7119, Val Acc: 0.9276\n",
            "Epoch [1833/4000], Train Loss: 1.7130, Train Acc: 0.9279, Val Loss: 1.7114, Val Acc: 0.9269\n",
            "Epoch [1834/4000], Train Loss: 1.7120, Train Acc: 0.9266, Val Loss: 1.7171, Val Acc: 0.9263\n",
            "Epoch [1835/4000], Train Loss: 1.7138, Train Acc: 0.9256, Val Loss: 1.7126, Val Acc: 0.9250\n",
            "Epoch [1836/4000], Train Loss: 1.7120, Train Acc: 0.9277, Val Loss: 1.7160, Val Acc: 0.9269\n",
            "Epoch [1837/4000], Train Loss: 1.7110, Train Acc: 0.9287, Val Loss: 1.7109, Val Acc: 0.9276\n",
            "Epoch [1838/4000], Train Loss: 1.7130, Train Acc: 0.9279, Val Loss: 1.7142, Val Acc: 0.9244\n",
            "Epoch [1839/4000], Train Loss: 1.7121, Train Acc: 0.9274, Val Loss: 1.7137, Val Acc: 0.9256\n",
            "Epoch [1840/4000], Train Loss: 1.7110, Train Acc: 0.9272, Val Loss: 1.7136, Val Acc: 0.9295\n",
            "Epoch [1841/4000], Train Loss: 1.7125, Train Acc: 0.9276, Val Loss: 1.7126, Val Acc: 0.9295\n",
            "Epoch [1842/4000], Train Loss: 1.7119, Train Acc: 0.9276, Val Loss: 1.7106, Val Acc: 0.9269\n",
            "Epoch [1843/4000], Train Loss: 1.7114, Train Acc: 0.9274, Val Loss: 1.7066, Val Acc: 0.9282\n",
            "Epoch [1844/4000], Train Loss: 1.7117, Train Acc: 0.9271, Val Loss: 1.7102, Val Acc: 0.9276\n",
            "Epoch [1845/4000], Train Loss: 1.7125, Train Acc: 0.9266, Val Loss: 1.7130, Val Acc: 0.9282\n",
            "Epoch [1846/4000], Train Loss: 1.7117, Train Acc: 0.9284, Val Loss: 1.7111, Val Acc: 0.9263\n",
            "Epoch [1847/4000], Train Loss: 1.7128, Train Acc: 0.9245, Val Loss: 1.7117, Val Acc: 0.9288\n",
            "Epoch [1848/4000], Train Loss: 1.7135, Train Acc: 0.9255, Val Loss: 1.7069, Val Acc: 0.9327\n",
            "Epoch [1849/4000], Train Loss: 1.7108, Train Acc: 0.9277, Val Loss: 1.7090, Val Acc: 0.9314\n",
            "Epoch [1850/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7104, Val Acc: 0.9327\n",
            "Epoch [1851/4000], Train Loss: 1.7129, Train Acc: 0.9271, Val Loss: 1.7138, Val Acc: 0.9224\n",
            "Epoch [1852/4000], Train Loss: 1.7132, Train Acc: 0.9264, Val Loss: 1.7132, Val Acc: 0.9276\n",
            "Epoch [1853/4000], Train Loss: 1.7128, Train Acc: 0.9269, Val Loss: 1.7120, Val Acc: 0.9276\n",
            "Epoch [1854/4000], Train Loss: 1.7111, Train Acc: 0.9287, Val Loss: 1.7114, Val Acc: 0.9282\n",
            "Epoch [1855/4000], Train Loss: 1.7124, Train Acc: 0.9272, Val Loss: 1.7131, Val Acc: 0.9301\n",
            "Epoch [1856/4000], Train Loss: 1.7124, Train Acc: 0.9288, Val Loss: 1.7217, Val Acc: 0.9231\n",
            "Epoch [1857/4000], Train Loss: 1.7131, Train Acc: 0.9255, Val Loss: 1.7145, Val Acc: 0.9256\n",
            "Epoch [1858/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [1859/4000], Train Loss: 1.7112, Train Acc: 0.9279, Val Loss: 1.7142, Val Acc: 0.9301\n",
            "Epoch [1860/4000], Train Loss: 1.7133, Train Acc: 0.9266, Val Loss: 1.7147, Val Acc: 0.9276\n",
            "Epoch [1861/4000], Train Loss: 1.7119, Train Acc: 0.9287, Val Loss: 1.7133, Val Acc: 0.9301\n",
            "Epoch [1862/4000], Train Loss: 1.7128, Train Acc: 0.9279, Val Loss: 1.7083, Val Acc: 0.9321\n",
            "Epoch [1863/4000], Train Loss: 1.7123, Train Acc: 0.9269, Val Loss: 1.7115, Val Acc: 0.9314\n",
            "Epoch [1864/4000], Train Loss: 1.7110, Train Acc: 0.9277, Val Loss: 1.7107, Val Acc: 0.9276\n",
            "Epoch [1865/4000], Train Loss: 1.7136, Train Acc: 0.9263, Val Loss: 1.7197, Val Acc: 0.9218\n",
            "Epoch [1866/4000], Train Loss: 1.7125, Train Acc: 0.9269, Val Loss: 1.7097, Val Acc: 0.9333\n",
            "Epoch [1867/4000], Train Loss: 1.7119, Train Acc: 0.9268, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [1868/4000], Train Loss: 1.7115, Train Acc: 0.9290, Val Loss: 1.7118, Val Acc: 0.9256\n",
            "Epoch [1869/4000], Train Loss: 1.7125, Train Acc: 0.9272, Val Loss: 1.7088, Val Acc: 0.9263\n",
            "Epoch [1870/4000], Train Loss: 1.7101, Train Acc: 0.9285, Val Loss: 1.7108, Val Acc: 0.9321\n",
            "Epoch [1871/4000], Train Loss: 1.7127, Train Acc: 0.9274, Val Loss: 1.7110, Val Acc: 0.9308\n",
            "Epoch [1872/4000], Train Loss: 1.7118, Train Acc: 0.9271, Val Loss: 1.7121, Val Acc: 0.9250\n",
            "Epoch [1873/4000], Train Loss: 1.7124, Train Acc: 0.9244, Val Loss: 1.7149, Val Acc: 0.9263\n",
            "Epoch [1874/4000], Train Loss: 1.7104, Train Acc: 0.9284, Val Loss: 1.7109, Val Acc: 0.9250\n",
            "Epoch [1875/4000], Train Loss: 1.7124, Train Acc: 0.9258, Val Loss: 1.7066, Val Acc: 0.9301\n",
            "Epoch [1876/4000], Train Loss: 1.7114, Train Acc: 0.9288, Val Loss: 1.7081, Val Acc: 0.9282\n",
            "Epoch [1877/4000], Train Loss: 1.7110, Train Acc: 0.9284, Val Loss: 1.7130, Val Acc: 0.9269\n",
            "Epoch [1878/4000], Train Loss: 1.7120, Train Acc: 0.9274, Val Loss: 1.7127, Val Acc: 0.9250\n",
            "Epoch [1879/4000], Train Loss: 1.7120, Train Acc: 0.9276, Val Loss: 1.7097, Val Acc: 0.9288\n",
            "Epoch [1880/4000], Train Loss: 1.7116, Train Acc: 0.9282, Val Loss: 1.7085, Val Acc: 0.9314\n",
            "Epoch [1881/4000], Train Loss: 1.7104, Train Acc: 0.9277, Val Loss: 1.7133, Val Acc: 0.9269\n",
            "Epoch [1882/4000], Train Loss: 1.7122, Train Acc: 0.9266, Val Loss: 1.7147, Val Acc: 0.9256\n",
            "Epoch [1883/4000], Train Loss: 1.7118, Train Acc: 0.9268, Val Loss: 1.7113, Val Acc: 0.9263\n",
            "Epoch [1884/4000], Train Loss: 1.7125, Train Acc: 0.9279, Val Loss: 1.7213, Val Acc: 0.9173\n",
            "Epoch [1885/4000], Train Loss: 1.7115, Train Acc: 0.9285, Val Loss: 1.7120, Val Acc: 0.9295\n",
            "Epoch [1886/4000], Train Loss: 1.7125, Train Acc: 0.9266, Val Loss: 1.7118, Val Acc: 0.9288\n",
            "Epoch [1887/4000], Train Loss: 1.7108, Train Acc: 0.9276, Val Loss: 1.7136, Val Acc: 0.9301\n",
            "Epoch [1888/4000], Train Loss: 1.7124, Train Acc: 0.9268, Val Loss: 1.7087, Val Acc: 0.9288\n",
            "Epoch [1889/4000], Train Loss: 1.7103, Train Acc: 0.9285, Val Loss: 1.7115, Val Acc: 0.9263\n",
            "Epoch [1890/4000], Train Loss: 1.7136, Train Acc: 0.9264, Val Loss: 1.7130, Val Acc: 0.9256\n",
            "Epoch [1891/4000], Train Loss: 1.7119, Train Acc: 0.9272, Val Loss: 1.7145, Val Acc: 0.9269\n",
            "Epoch [1892/4000], Train Loss: 1.7128, Train Acc: 0.9280, Val Loss: 1.7230, Val Acc: 0.9186\n",
            "Epoch [1893/4000], Train Loss: 1.7114, Train Acc: 0.9272, Val Loss: 1.7102, Val Acc: 0.9295\n",
            "Epoch [1894/4000], Train Loss: 1.7121, Train Acc: 0.9282, Val Loss: 1.7251, Val Acc: 0.9128\n",
            "Epoch [1895/4000], Train Loss: 1.7131, Train Acc: 0.9255, Val Loss: 1.7136, Val Acc: 0.9263\n",
            "Epoch [1896/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7082, Val Acc: 0.9301\n",
            "Epoch [1897/4000], Train Loss: 1.7103, Train Acc: 0.9296, Val Loss: 1.7084, Val Acc: 0.9301\n",
            "Epoch [1898/4000], Train Loss: 1.7125, Train Acc: 0.9279, Val Loss: 1.7095, Val Acc: 0.9333\n",
            "Epoch [1899/4000], Train Loss: 1.7111, Train Acc: 0.9296, Val Loss: 1.7133, Val Acc: 0.9269\n",
            "Epoch [1900/4000], Train Loss: 1.7111, Train Acc: 0.9287, Val Loss: 1.7181, Val Acc: 0.9224\n",
            "Epoch [1901/4000], Train Loss: 1.7115, Train Acc: 0.9266, Val Loss: 1.7072, Val Acc: 0.9314\n",
            "Epoch [1902/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7082, Val Acc: 0.9327\n",
            "Epoch [1903/4000], Train Loss: 1.7128, Train Acc: 0.9271, Val Loss: 1.7152, Val Acc: 0.9295\n",
            "Epoch [1904/4000], Train Loss: 1.7109, Train Acc: 0.9277, Val Loss: 1.7118, Val Acc: 0.9237\n",
            "Epoch [1905/4000], Train Loss: 1.7124, Train Acc: 0.9280, Val Loss: 1.7125, Val Acc: 0.9276\n",
            "Epoch [1906/4000], Train Loss: 1.7113, Train Acc: 0.9276, Val Loss: 1.7116, Val Acc: 0.9288\n",
            "Epoch [1907/4000], Train Loss: 1.7121, Train Acc: 0.9293, Val Loss: 1.7203, Val Acc: 0.9244\n",
            "Epoch [1908/4000], Train Loss: 1.7124, Train Acc: 0.9261, Val Loss: 1.7100, Val Acc: 0.9288\n",
            "Epoch [1909/4000], Train Loss: 1.7121, Train Acc: 0.9266, Val Loss: 1.7127, Val Acc: 0.9282\n",
            "Epoch [1910/4000], Train Loss: 1.7118, Train Acc: 0.9279, Val Loss: 1.7134, Val Acc: 0.9269\n",
            "Epoch [1911/4000], Train Loss: 1.7117, Train Acc: 0.9279, Val Loss: 1.7170, Val Acc: 0.9250\n",
            "Epoch [1912/4000], Train Loss: 1.7140, Train Acc: 0.9277, Val Loss: 1.7272, Val Acc: 0.9173\n",
            "Epoch [1913/4000], Train Loss: 1.7141, Train Acc: 0.9264, Val Loss: 1.7119, Val Acc: 0.9237\n",
            "Epoch [1914/4000], Train Loss: 1.7122, Train Acc: 0.9271, Val Loss: 1.7112, Val Acc: 0.9295\n",
            "Epoch [1915/4000], Train Loss: 1.7124, Train Acc: 0.9263, Val Loss: 1.7109, Val Acc: 0.9282\n",
            "Epoch [1916/4000], Train Loss: 1.7106, Train Acc: 0.9269, Val Loss: 1.7091, Val Acc: 0.9295\n",
            "Epoch [1917/4000], Train Loss: 1.7119, Train Acc: 0.9269, Val Loss: 1.7112, Val Acc: 0.9282\n",
            "Epoch [1918/4000], Train Loss: 1.7120, Train Acc: 0.9276, Val Loss: 1.7116, Val Acc: 0.9205\n",
            "Epoch [1919/4000], Train Loss: 1.7126, Train Acc: 0.9277, Val Loss: 1.7128, Val Acc: 0.9301\n",
            "Epoch [1920/4000], Train Loss: 1.7124, Train Acc: 0.9282, Val Loss: 1.7116, Val Acc: 0.9276\n",
            "Epoch [1921/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7120, Val Acc: 0.9282\n",
            "Epoch [1922/4000], Train Loss: 1.7121, Train Acc: 0.9263, Val Loss: 1.7074, Val Acc: 0.9314\n",
            "Epoch [1923/4000], Train Loss: 1.7122, Train Acc: 0.9277, Val Loss: 1.7263, Val Acc: 0.9192\n",
            "Epoch [1924/4000], Train Loss: 1.7119, Train Acc: 0.9290, Val Loss: 1.7179, Val Acc: 0.9192\n",
            "Epoch [1925/4000], Train Loss: 1.7114, Train Acc: 0.9293, Val Loss: 1.7098, Val Acc: 0.9308\n",
            "Epoch [1926/4000], Train Loss: 1.7114, Train Acc: 0.9276, Val Loss: 1.7135, Val Acc: 0.9276\n",
            "Epoch [1927/4000], Train Loss: 1.7122, Train Acc: 0.9269, Val Loss: 1.7101, Val Acc: 0.9282\n",
            "Epoch [1928/4000], Train Loss: 1.7146, Train Acc: 0.9250, Val Loss: 1.7080, Val Acc: 0.9321\n",
            "Epoch [1929/4000], Train Loss: 1.7109, Train Acc: 0.9279, Val Loss: 1.7157, Val Acc: 0.9263\n",
            "Epoch [1930/4000], Train Loss: 1.7116, Train Acc: 0.9279, Val Loss: 1.7115, Val Acc: 0.9276\n",
            "Epoch [1931/4000], Train Loss: 1.7124, Train Acc: 0.9274, Val Loss: 1.7099, Val Acc: 0.9301\n",
            "Epoch [1932/4000], Train Loss: 1.7109, Train Acc: 0.9285, Val Loss: 1.7074, Val Acc: 0.9346\n",
            "Epoch [1933/4000], Train Loss: 1.7105, Train Acc: 0.9285, Val Loss: 1.7097, Val Acc: 0.9276\n",
            "Epoch [1934/4000], Train Loss: 1.7119, Train Acc: 0.9279, Val Loss: 1.7090, Val Acc: 0.9308\n",
            "Epoch [1935/4000], Train Loss: 1.7113, Train Acc: 0.9277, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [1936/4000], Train Loss: 1.7126, Train Acc: 0.9261, Val Loss: 1.7081, Val Acc: 0.9333\n",
            "Epoch [1937/4000], Train Loss: 1.7114, Train Acc: 0.9280, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [1938/4000], Train Loss: 1.7114, Train Acc: 0.9279, Val Loss: 1.7112, Val Acc: 0.9308\n",
            "Epoch [1939/4000], Train Loss: 1.7112, Train Acc: 0.9271, Val Loss: 1.7093, Val Acc: 0.9288\n",
            "Epoch [1940/4000], Train Loss: 1.7138, Train Acc: 0.9261, Val Loss: 1.7088, Val Acc: 0.9282\n",
            "Epoch [1941/4000], Train Loss: 1.7105, Train Acc: 0.9290, Val Loss: 1.7087, Val Acc: 0.9282\n",
            "Epoch [1942/4000], Train Loss: 1.7114, Train Acc: 0.9282, Val Loss: 1.7141, Val Acc: 0.9263\n",
            "Epoch [1943/4000], Train Loss: 1.7109, Train Acc: 0.9285, Val Loss: 1.7069, Val Acc: 0.9327\n",
            "Epoch [1944/4000], Train Loss: 1.7116, Train Acc: 0.9288, Val Loss: 1.7114, Val Acc: 0.9321\n",
            "Epoch [1945/4000], Train Loss: 1.7115, Train Acc: 0.9279, Val Loss: 1.7118, Val Acc: 0.9224\n",
            "Epoch [1946/4000], Train Loss: 1.7131, Train Acc: 0.9258, Val Loss: 1.7095, Val Acc: 0.9295\n",
            "Epoch [1947/4000], Train Loss: 1.7124, Train Acc: 0.9279, Val Loss: 1.7078, Val Acc: 0.9301\n",
            "Epoch [1948/4000], Train Loss: 1.7106, Train Acc: 0.9279, Val Loss: 1.7100, Val Acc: 0.9288\n",
            "Epoch [1949/4000], Train Loss: 1.7126, Train Acc: 0.9274, Val Loss: 1.7172, Val Acc: 0.9212\n",
            "Epoch [1950/4000], Train Loss: 1.7148, Train Acc: 0.9253, Val Loss: 1.7113, Val Acc: 0.9288\n",
            "Epoch [1951/4000], Train Loss: 1.7121, Train Acc: 0.9284, Val Loss: 1.7123, Val Acc: 0.9282\n",
            "Epoch [1952/4000], Train Loss: 1.7127, Train Acc: 0.9266, Val Loss: 1.7131, Val Acc: 0.9276\n",
            "Epoch [1953/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7151, Val Acc: 0.9263\n",
            "Epoch [1954/4000], Train Loss: 1.7119, Train Acc: 0.9271, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [1955/4000], Train Loss: 1.7130, Train Acc: 0.9263, Val Loss: 1.7144, Val Acc: 0.9256\n",
            "Epoch [1956/4000], Train Loss: 1.7128, Train Acc: 0.9272, Val Loss: 1.7097, Val Acc: 0.9308\n",
            "Epoch [1957/4000], Train Loss: 1.7107, Train Acc: 0.9293, Val Loss: 1.7107, Val Acc: 0.9256\n",
            "Epoch [1958/4000], Train Loss: 1.7121, Train Acc: 0.9287, Val Loss: 1.7102, Val Acc: 0.9301\n",
            "Epoch [1959/4000], Train Loss: 1.7115, Train Acc: 0.9292, Val Loss: 1.7126, Val Acc: 0.9269\n",
            "Epoch [1960/4000], Train Loss: 1.7115, Train Acc: 0.9271, Val Loss: 1.7111, Val Acc: 0.9263\n",
            "Epoch [1961/4000], Train Loss: 1.7110, Train Acc: 0.9280, Val Loss: 1.7112, Val Acc: 0.9288\n",
            "Epoch [1962/4000], Train Loss: 1.7124, Train Acc: 0.9276, Val Loss: 1.7071, Val Acc: 0.9327\n",
            "Epoch [1963/4000], Train Loss: 1.7116, Train Acc: 0.9266, Val Loss: 1.7135, Val Acc: 0.9269\n",
            "Epoch [1964/4000], Train Loss: 1.7104, Train Acc: 0.9282, Val Loss: 1.7125, Val Acc: 0.9295\n",
            "Epoch [1965/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7095, Val Acc: 0.9333\n",
            "Epoch [1966/4000], Train Loss: 1.7113, Train Acc: 0.9280, Val Loss: 1.7149, Val Acc: 0.9263\n",
            "Epoch [1967/4000], Train Loss: 1.7120, Train Acc: 0.9274, Val Loss: 1.7105, Val Acc: 0.9308\n",
            "Epoch [1968/4000], Train Loss: 1.7117, Train Acc: 0.9274, Val Loss: 1.7110, Val Acc: 0.9308\n",
            "Epoch [1969/4000], Train Loss: 1.7123, Train Acc: 0.9274, Val Loss: 1.7108, Val Acc: 0.9288\n",
            "Epoch [1970/4000], Train Loss: 1.7111, Train Acc: 0.9285, Val Loss: 1.7158, Val Acc: 0.9218\n",
            "Epoch [1971/4000], Train Loss: 1.7124, Train Acc: 0.9285, Val Loss: 1.7116, Val Acc: 0.9288\n",
            "Epoch [1972/4000], Train Loss: 1.7134, Train Acc: 0.9250, Val Loss: 1.7101, Val Acc: 0.9288\n",
            "Epoch [1973/4000], Train Loss: 1.7105, Train Acc: 0.9279, Val Loss: 1.7069, Val Acc: 0.9321\n",
            "Epoch [1974/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7133, Val Acc: 0.9244\n",
            "Epoch [1975/4000], Train Loss: 1.7111, Train Acc: 0.9287, Val Loss: 1.7185, Val Acc: 0.9212\n",
            "Epoch [1976/4000], Train Loss: 1.7114, Train Acc: 0.9287, Val Loss: 1.7106, Val Acc: 0.9295\n",
            "Epoch [1977/4000], Train Loss: 1.7120, Train Acc: 0.9263, Val Loss: 1.7084, Val Acc: 0.9288\n",
            "Epoch [1978/4000], Train Loss: 1.7112, Train Acc: 0.9274, Val Loss: 1.7113, Val Acc: 0.9256\n",
            "Epoch [1979/4000], Train Loss: 1.7123, Train Acc: 0.9277, Val Loss: 1.7116, Val Acc: 0.9244\n",
            "Epoch [1980/4000], Train Loss: 1.7114, Train Acc: 0.9272, Val Loss: 1.7105, Val Acc: 0.9276\n",
            "Epoch [1981/4000], Train Loss: 1.7115, Train Acc: 0.9274, Val Loss: 1.7120, Val Acc: 0.9250\n",
            "Epoch [1982/4000], Train Loss: 1.7125, Train Acc: 0.9258, Val Loss: 1.7145, Val Acc: 0.9263\n",
            "Epoch [1983/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7099, Val Acc: 0.9321\n",
            "Epoch [1984/4000], Train Loss: 1.7118, Train Acc: 0.9287, Val Loss: 1.7081, Val Acc: 0.9340\n",
            "Epoch [1985/4000], Train Loss: 1.7124, Train Acc: 0.9268, Val Loss: 1.7152, Val Acc: 0.9218\n",
            "Epoch [1986/4000], Train Loss: 1.7107, Train Acc: 0.9274, Val Loss: 1.7129, Val Acc: 0.9288\n",
            "Epoch [1987/4000], Train Loss: 1.7127, Train Acc: 0.9261, Val Loss: 1.7087, Val Acc: 0.9295\n",
            "Epoch [1988/4000], Train Loss: 1.7109, Train Acc: 0.9282, Val Loss: 1.7163, Val Acc: 0.9269\n",
            "Epoch [1989/4000], Train Loss: 1.7115, Train Acc: 0.9271, Val Loss: 1.7153, Val Acc: 0.9256\n",
            "Epoch [1990/4000], Train Loss: 1.7123, Train Acc: 0.9274, Val Loss: 1.7098, Val Acc: 0.9288\n",
            "Epoch [1991/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7091, Val Acc: 0.9263\n",
            "Epoch [1992/4000], Train Loss: 1.7128, Train Acc: 0.9256, Val Loss: 1.7083, Val Acc: 0.9333\n",
            "Epoch [1993/4000], Train Loss: 1.7128, Train Acc: 0.9276, Val Loss: 1.7120, Val Acc: 0.9288\n",
            "Epoch [1994/4000], Train Loss: 1.7116, Train Acc: 0.9284, Val Loss: 1.7162, Val Acc: 0.9295\n",
            "Epoch [1995/4000], Train Loss: 1.7110, Train Acc: 0.9301, Val Loss: 1.7107, Val Acc: 0.9282\n",
            "Epoch [1996/4000], Train Loss: 1.7125, Train Acc: 0.9279, Val Loss: 1.7106, Val Acc: 0.9282\n",
            "Epoch [1997/4000], Train Loss: 1.7119, Train Acc: 0.9276, Val Loss: 1.7122, Val Acc: 0.9295\n",
            "Epoch [1998/4000], Train Loss: 1.7115, Train Acc: 0.9295, Val Loss: 1.7113, Val Acc: 0.9256\n",
            "Epoch [1999/4000], Train Loss: 1.7118, Train Acc: 0.9271, Val Loss: 1.7076, Val Acc: 0.9321\n",
            "Epoch [2000/4000], Train Loss: 1.7100, Train Acc: 0.9292, Val Loss: 1.7218, Val Acc: 0.9167\n",
            "Epoch [2001/4000], Train Loss: 1.7132, Train Acc: 0.9277, Val Loss: 1.7115, Val Acc: 0.9282\n",
            "Epoch [2002/4000], Train Loss: 1.7097, Train Acc: 0.9300, Val Loss: 1.7091, Val Acc: 0.9301\n",
            "Epoch [2003/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7085, Val Acc: 0.9295\n",
            "Epoch [2004/4000], Train Loss: 1.7102, Train Acc: 0.9292, Val Loss: 1.7090, Val Acc: 0.9308\n",
            "Epoch [2005/4000], Train Loss: 1.7116, Train Acc: 0.9288, Val Loss: 1.7093, Val Acc: 0.9288\n",
            "Epoch [2006/4000], Train Loss: 1.7094, Train Acc: 0.9311, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [2007/4000], Train Loss: 1.7103, Train Acc: 0.9284, Val Loss: 1.7103, Val Acc: 0.9333\n",
            "Epoch [2008/4000], Train Loss: 1.7110, Train Acc: 0.9303, Val Loss: 1.7100, Val Acc: 0.9288\n",
            "Epoch [2009/4000], Train Loss: 1.7107, Train Acc: 0.9277, Val Loss: 1.7107, Val Acc: 0.9276\n",
            "Epoch [2010/4000], Train Loss: 1.7111, Train Acc: 0.9285, Val Loss: 1.7098, Val Acc: 0.9321\n",
            "Epoch [2011/4000], Train Loss: 1.7132, Train Acc: 0.9277, Val Loss: 1.7091, Val Acc: 0.9301\n",
            "Epoch [2012/4000], Train Loss: 1.7129, Train Acc: 0.9277, Val Loss: 1.7169, Val Acc: 0.9224\n",
            "Epoch [2013/4000], Train Loss: 1.7126, Train Acc: 0.9268, Val Loss: 1.7095, Val Acc: 0.9269\n",
            "Epoch [2014/4000], Train Loss: 1.7102, Train Acc: 0.9292, Val Loss: 1.7131, Val Acc: 0.9301\n",
            "Epoch [2015/4000], Train Loss: 1.7109, Train Acc: 0.9290, Val Loss: 1.7097, Val Acc: 0.9295\n",
            "Epoch [2016/4000], Train Loss: 1.7108, Train Acc: 0.9284, Val Loss: 1.7066, Val Acc: 0.9314\n",
            "Epoch [2017/4000], Train Loss: 1.7123, Train Acc: 0.9269, Val Loss: 1.7130, Val Acc: 0.9308\n",
            "Epoch [2018/4000], Train Loss: 1.7109, Train Acc: 0.9296, Val Loss: 1.7163, Val Acc: 0.9282\n",
            "Epoch [2019/4000], Train Loss: 1.7115, Train Acc: 0.9277, Val Loss: 1.7113, Val Acc: 0.9295\n",
            "Epoch [2020/4000], Train Loss: 1.7118, Train Acc: 0.9276, Val Loss: 1.7249, Val Acc: 0.9192\n",
            "Epoch [2021/4000], Train Loss: 1.7114, Train Acc: 0.9284, Val Loss: 1.7069, Val Acc: 0.9321\n",
            "Epoch [2022/4000], Train Loss: 1.7107, Train Acc: 0.9282, Val Loss: 1.7106, Val Acc: 0.9308\n",
            "Epoch [2023/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7085, Val Acc: 0.9282\n",
            "Epoch [2024/4000], Train Loss: 1.7107, Train Acc: 0.9271, Val Loss: 1.7083, Val Acc: 0.9282\n",
            "Epoch [2025/4000], Train Loss: 1.7118, Train Acc: 0.9277, Val Loss: 1.7115, Val Acc: 0.9288\n",
            "Epoch [2026/4000], Train Loss: 1.7119, Train Acc: 0.9271, Val Loss: 1.7092, Val Acc: 0.9301\n",
            "Epoch [2027/4000], Train Loss: 1.7103, Train Acc: 0.9295, Val Loss: 1.7141, Val Acc: 0.9301\n",
            "Epoch [2028/4000], Train Loss: 1.7132, Train Acc: 0.9258, Val Loss: 1.7071, Val Acc: 0.9321\n",
            "Epoch [2029/4000], Train Loss: 1.7118, Train Acc: 0.9277, Val Loss: 1.7115, Val Acc: 0.9301\n",
            "Epoch [2030/4000], Train Loss: 1.7112, Train Acc: 0.9277, Val Loss: 1.7163, Val Acc: 0.9250\n",
            "Epoch [2031/4000], Train Loss: 1.7120, Train Acc: 0.9268, Val Loss: 1.7085, Val Acc: 0.9295\n",
            "Epoch [2032/4000], Train Loss: 1.7116, Train Acc: 0.9284, Val Loss: 1.7152, Val Acc: 0.9231\n",
            "Epoch [2033/4000], Train Loss: 1.7114, Train Acc: 0.9279, Val Loss: 1.7103, Val Acc: 0.9333\n",
            "Epoch [2034/4000], Train Loss: 1.7110, Train Acc: 0.9276, Val Loss: 1.7142, Val Acc: 0.9308\n",
            "Epoch [2035/4000], Train Loss: 1.7104, Train Acc: 0.9293, Val Loss: 1.7080, Val Acc: 0.9321\n",
            "Epoch [2036/4000], Train Loss: 1.7114, Train Acc: 0.9285, Val Loss: 1.7094, Val Acc: 0.9327\n",
            "Epoch [2037/4000], Train Loss: 1.7108, Train Acc: 0.9287, Val Loss: 1.7114, Val Acc: 0.9308\n",
            "Epoch [2038/4000], Train Loss: 1.7103, Train Acc: 0.9295, Val Loss: 1.7182, Val Acc: 0.9218\n",
            "Epoch [2039/4000], Train Loss: 1.7129, Train Acc: 0.9260, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [2040/4000], Train Loss: 1.7100, Train Acc: 0.9290, Val Loss: 1.7116, Val Acc: 0.9288\n",
            "Epoch [2041/4000], Train Loss: 1.7128, Train Acc: 0.9269, Val Loss: 1.7104, Val Acc: 0.9333\n",
            "Epoch [2042/4000], Train Loss: 1.7102, Train Acc: 0.9287, Val Loss: 1.7076, Val Acc: 0.9295\n",
            "Epoch [2043/4000], Train Loss: 1.7113, Train Acc: 0.9293, Val Loss: 1.7143, Val Acc: 0.9295\n",
            "Epoch [2044/4000], Train Loss: 1.7106, Train Acc: 0.9280, Val Loss: 1.7109, Val Acc: 0.9244\n",
            "Epoch [2045/4000], Train Loss: 1.7113, Train Acc: 0.9290, Val Loss: 1.7198, Val Acc: 0.9237\n",
            "Epoch [2046/4000], Train Loss: 1.7107, Train Acc: 0.9285, Val Loss: 1.7101, Val Acc: 0.9276\n",
            "Epoch [2047/4000], Train Loss: 1.7108, Train Acc: 0.9295, Val Loss: 1.7139, Val Acc: 0.9288\n",
            "Epoch [2048/4000], Train Loss: 1.7112, Train Acc: 0.9285, Val Loss: 1.7069, Val Acc: 0.9282\n",
            "Epoch [2049/4000], Train Loss: 1.7096, Train Acc: 0.9308, Val Loss: 1.7092, Val Acc: 0.9321\n",
            "Epoch [2050/4000], Train Loss: 1.7112, Train Acc: 0.9287, Val Loss: 1.7116, Val Acc: 0.9269\n",
            "Epoch [2051/4000], Train Loss: 1.7110, Train Acc: 0.9282, Val Loss: 1.7105, Val Acc: 0.9288\n",
            "Epoch [2052/4000], Train Loss: 1.7129, Train Acc: 0.9276, Val Loss: 1.7096, Val Acc: 0.9282\n",
            "Epoch [2053/4000], Train Loss: 1.7107, Train Acc: 0.9288, Val Loss: 1.7092, Val Acc: 0.9288\n",
            "Epoch [2054/4000], Train Loss: 1.7103, Train Acc: 0.9285, Val Loss: 1.7109, Val Acc: 0.9301\n",
            "Epoch [2055/4000], Train Loss: 1.7142, Train Acc: 0.9269, Val Loss: 1.7101, Val Acc: 0.9301\n",
            "Epoch [2056/4000], Train Loss: 1.7103, Train Acc: 0.9288, Val Loss: 1.7124, Val Acc: 0.9250\n",
            "Epoch [2057/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7096, Val Acc: 0.9295\n",
            "Epoch [2058/4000], Train Loss: 1.7107, Train Acc: 0.9285, Val Loss: 1.7070, Val Acc: 0.9301\n",
            "Epoch [2059/4000], Train Loss: 1.7119, Train Acc: 0.9269, Val Loss: 1.7093, Val Acc: 0.9301\n",
            "Epoch [2060/4000], Train Loss: 1.7125, Train Acc: 0.9282, Val Loss: 1.7111, Val Acc: 0.9295\n",
            "Epoch [2061/4000], Train Loss: 1.7112, Train Acc: 0.9274, Val Loss: 1.7104, Val Acc: 0.9276\n",
            "Epoch [2062/4000], Train Loss: 1.7120, Train Acc: 0.9285, Val Loss: 1.7141, Val Acc: 0.9301\n",
            "Epoch [2063/4000], Train Loss: 1.7126, Train Acc: 0.9284, Val Loss: 1.7114, Val Acc: 0.9256\n",
            "Epoch [2064/4000], Train Loss: 1.7110, Train Acc: 0.9292, Val Loss: 1.7115, Val Acc: 0.9269\n",
            "Epoch [2065/4000], Train Loss: 1.7108, Train Acc: 0.9308, Val Loss: 1.7130, Val Acc: 0.9276\n",
            "Epoch [2066/4000], Train Loss: 1.7094, Train Acc: 0.9300, Val Loss: 1.7127, Val Acc: 0.9301\n",
            "Epoch [2067/4000], Train Loss: 1.7115, Train Acc: 0.9271, Val Loss: 1.7101, Val Acc: 0.9308\n",
            "Epoch [2068/4000], Train Loss: 1.7111, Train Acc: 0.9271, Val Loss: 1.7128, Val Acc: 0.9282\n",
            "Epoch [2069/4000], Train Loss: 1.7103, Train Acc: 0.9295, Val Loss: 1.7169, Val Acc: 0.9295\n",
            "Epoch [2070/4000], Train Loss: 1.7120, Train Acc: 0.9279, Val Loss: 1.7114, Val Acc: 0.9276\n",
            "Epoch [2071/4000], Train Loss: 1.7124, Train Acc: 0.9276, Val Loss: 1.7083, Val Acc: 0.9288\n",
            "Epoch [2072/4000], Train Loss: 1.7110, Train Acc: 0.9287, Val Loss: 1.7101, Val Acc: 0.9288\n",
            "Epoch [2073/4000], Train Loss: 1.7130, Train Acc: 0.9274, Val Loss: 1.7118, Val Acc: 0.9288\n",
            "Epoch [2074/4000], Train Loss: 1.7117, Train Acc: 0.9284, Val Loss: 1.7103, Val Acc: 0.9288\n",
            "Epoch [2075/4000], Train Loss: 1.7117, Train Acc: 0.9285, Val Loss: 1.7125, Val Acc: 0.9301\n",
            "Epoch [2076/4000], Train Loss: 1.7124, Train Acc: 0.9276, Val Loss: 1.7110, Val Acc: 0.9244\n",
            "Epoch [2077/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7100, Val Acc: 0.9295\n",
            "Epoch [2078/4000], Train Loss: 1.7106, Train Acc: 0.9276, Val Loss: 1.7139, Val Acc: 0.9237\n",
            "Epoch [2079/4000], Train Loss: 1.7099, Train Acc: 0.9274, Val Loss: 1.7158, Val Acc: 0.9244\n",
            "Epoch [2080/4000], Train Loss: 1.7109, Train Acc: 0.9280, Val Loss: 1.7143, Val Acc: 0.9282\n",
            "Epoch [2081/4000], Train Loss: 1.7121, Train Acc: 0.9282, Val Loss: 1.7176, Val Acc: 0.9186\n",
            "Epoch [2082/4000], Train Loss: 1.7144, Train Acc: 0.9263, Val Loss: 1.7074, Val Acc: 0.9321\n",
            "Epoch [2083/4000], Train Loss: 1.7147, Train Acc: 0.9245, Val Loss: 1.7136, Val Acc: 0.9282\n",
            "Epoch [2084/4000], Train Loss: 1.7106, Train Acc: 0.9288, Val Loss: 1.7120, Val Acc: 0.9288\n",
            "Epoch [2085/4000], Train Loss: 1.7116, Train Acc: 0.9284, Val Loss: 1.7124, Val Acc: 0.9276\n",
            "Epoch [2086/4000], Train Loss: 1.7098, Train Acc: 0.9284, Val Loss: 1.7094, Val Acc: 0.9301\n",
            "Epoch [2087/4000], Train Loss: 1.7107, Train Acc: 0.9287, Val Loss: 1.7146, Val Acc: 0.9256\n",
            "Epoch [2088/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7150, Val Acc: 0.9308\n",
            "Epoch [2089/4000], Train Loss: 1.7135, Train Acc: 0.9274, Val Loss: 1.7172, Val Acc: 0.9237\n",
            "Epoch [2090/4000], Train Loss: 1.7129, Train Acc: 0.9264, Val Loss: 1.7188, Val Acc: 0.9250\n",
            "Epoch [2091/4000], Train Loss: 1.7135, Train Acc: 0.9258, Val Loss: 1.7131, Val Acc: 0.9256\n",
            "Epoch [2092/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7103, Val Acc: 0.9314\n",
            "Epoch [2093/4000], Train Loss: 1.7117, Train Acc: 0.9284, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [2094/4000], Train Loss: 1.7116, Train Acc: 0.9279, Val Loss: 1.7103, Val Acc: 0.9314\n",
            "Epoch [2095/4000], Train Loss: 1.7104, Train Acc: 0.9282, Val Loss: 1.7082, Val Acc: 0.9314\n",
            "Epoch [2096/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7073, Val Acc: 0.9314\n",
            "Epoch [2097/4000], Train Loss: 1.7128, Train Acc: 0.9269, Val Loss: 1.7148, Val Acc: 0.9263\n",
            "Epoch [2098/4000], Train Loss: 1.7103, Train Acc: 0.9287, Val Loss: 1.7150, Val Acc: 0.9269\n",
            "Epoch [2099/4000], Train Loss: 1.7112, Train Acc: 0.9280, Val Loss: 1.7160, Val Acc: 0.9295\n",
            "Epoch [2100/4000], Train Loss: 1.7107, Train Acc: 0.9287, Val Loss: 1.7096, Val Acc: 0.9308\n",
            "Epoch [2101/4000], Train Loss: 1.7105, Train Acc: 0.9301, Val Loss: 1.7095, Val Acc: 0.9314\n",
            "Epoch [2102/4000], Train Loss: 1.7109, Train Acc: 0.9290, Val Loss: 1.7163, Val Acc: 0.9276\n",
            "Epoch [2103/4000], Train Loss: 1.7124, Train Acc: 0.9268, Val Loss: 1.7138, Val Acc: 0.9276\n",
            "Epoch [2104/4000], Train Loss: 1.7116, Train Acc: 0.9272, Val Loss: 1.7114, Val Acc: 0.9301\n",
            "Epoch [2105/4000], Train Loss: 1.7114, Train Acc: 0.9287, Val Loss: 1.7216, Val Acc: 0.9154\n",
            "Epoch [2106/4000], Train Loss: 1.7120, Train Acc: 0.9277, Val Loss: 1.7099, Val Acc: 0.9327\n",
            "Epoch [2107/4000], Train Loss: 1.7102, Train Acc: 0.9276, Val Loss: 1.7096, Val Acc: 0.9308\n",
            "Epoch [2108/4000], Train Loss: 1.7108, Train Acc: 0.9276, Val Loss: 1.7153, Val Acc: 0.9276\n",
            "Epoch [2109/4000], Train Loss: 1.7105, Train Acc: 0.9295, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [2110/4000], Train Loss: 1.7131, Train Acc: 0.9264, Val Loss: 1.7083, Val Acc: 0.9308\n",
            "Epoch [2111/4000], Train Loss: 1.7109, Train Acc: 0.9295, Val Loss: 1.7069, Val Acc: 0.9321\n",
            "Epoch [2112/4000], Train Loss: 1.7138, Train Acc: 0.9269, Val Loss: 1.7188, Val Acc: 0.9244\n",
            "Epoch [2113/4000], Train Loss: 1.7103, Train Acc: 0.9272, Val Loss: 1.7118, Val Acc: 0.9186\n",
            "Epoch [2114/4000], Train Loss: 1.7105, Train Acc: 0.9288, Val Loss: 1.7105, Val Acc: 0.9333\n",
            "Epoch [2115/4000], Train Loss: 1.7106, Train Acc: 0.9285, Val Loss: 1.7086, Val Acc: 0.9308\n",
            "Epoch [2116/4000], Train Loss: 1.7121, Train Acc: 0.9271, Val Loss: 1.7099, Val Acc: 0.9288\n",
            "Epoch [2117/4000], Train Loss: 1.7124, Train Acc: 0.9269, Val Loss: 1.7246, Val Acc: 0.9147\n",
            "Epoch [2118/4000], Train Loss: 1.7125, Train Acc: 0.9279, Val Loss: 1.7097, Val Acc: 0.9333\n",
            "Epoch [2119/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7097, Val Acc: 0.9269\n",
            "Epoch [2120/4000], Train Loss: 1.7128, Train Acc: 0.9266, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [2121/4000], Train Loss: 1.7113, Train Acc: 0.9293, Val Loss: 1.7093, Val Acc: 0.9301\n",
            "Epoch [2122/4000], Train Loss: 1.7116, Train Acc: 0.9282, Val Loss: 1.7109, Val Acc: 0.9282\n",
            "Epoch [2123/4000], Train Loss: 1.7124, Train Acc: 0.9287, Val Loss: 1.7092, Val Acc: 0.9327\n",
            "Epoch [2124/4000], Train Loss: 1.7106, Train Acc: 0.9295, Val Loss: 1.7109, Val Acc: 0.9269\n",
            "Epoch [2125/4000], Train Loss: 1.7100, Train Acc: 0.9279, Val Loss: 1.7080, Val Acc: 0.9314\n",
            "Epoch [2126/4000], Train Loss: 1.7100, Train Acc: 0.9288, Val Loss: 1.7140, Val Acc: 0.9282\n",
            "Epoch [2127/4000], Train Loss: 1.7114, Train Acc: 0.9276, Val Loss: 1.7136, Val Acc: 0.9295\n",
            "Epoch [2128/4000], Train Loss: 1.7126, Train Acc: 0.9266, Val Loss: 1.7167, Val Acc: 0.9224\n",
            "Epoch [2129/4000], Train Loss: 1.7122, Train Acc: 0.9282, Val Loss: 1.7093, Val Acc: 0.9295\n",
            "Epoch [2130/4000], Train Loss: 1.7117, Train Acc: 0.9282, Val Loss: 1.7122, Val Acc: 0.9282\n",
            "Epoch [2131/4000], Train Loss: 1.7139, Train Acc: 0.9252, Val Loss: 1.7099, Val Acc: 0.9321\n",
            "Epoch [2132/4000], Train Loss: 1.7126, Train Acc: 0.9274, Val Loss: 1.7136, Val Acc: 0.9276\n",
            "Epoch [2133/4000], Train Loss: 1.7115, Train Acc: 0.9271, Val Loss: 1.7078, Val Acc: 0.9282\n",
            "Epoch [2134/4000], Train Loss: 1.7120, Train Acc: 0.9274, Val Loss: 1.7085, Val Acc: 0.9282\n",
            "Epoch [2135/4000], Train Loss: 1.7112, Train Acc: 0.9276, Val Loss: 1.7089, Val Acc: 0.9276\n",
            "Epoch [2136/4000], Train Loss: 1.7114, Train Acc: 0.9287, Val Loss: 1.7076, Val Acc: 0.9288\n",
            "Epoch [2137/4000], Train Loss: 1.7112, Train Acc: 0.9272, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [2138/4000], Train Loss: 1.7107, Train Acc: 0.9277, Val Loss: 1.7099, Val Acc: 0.9288\n",
            "Epoch [2139/4000], Train Loss: 1.7135, Train Acc: 0.9266, Val Loss: 1.7141, Val Acc: 0.9288\n",
            "Epoch [2140/4000], Train Loss: 1.7106, Train Acc: 0.9282, Val Loss: 1.7070, Val Acc: 0.9314\n",
            "Epoch [2141/4000], Train Loss: 1.7102, Train Acc: 0.9293, Val Loss: 1.7133, Val Acc: 0.9269\n",
            "Epoch [2142/4000], Train Loss: 1.7116, Train Acc: 0.9279, Val Loss: 1.7133, Val Acc: 0.9250\n",
            "Epoch [2143/4000], Train Loss: 1.7103, Train Acc: 0.9280, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [2144/4000], Train Loss: 1.7115, Train Acc: 0.9287, Val Loss: 1.7152, Val Acc: 0.9218\n",
            "Epoch [2145/4000], Train Loss: 1.7126, Train Acc: 0.9271, Val Loss: 1.7136, Val Acc: 0.9256\n",
            "Epoch [2146/4000], Train Loss: 1.7134, Train Acc: 0.9260, Val Loss: 1.7173, Val Acc: 0.9250\n",
            "Epoch [2147/4000], Train Loss: 1.7112, Train Acc: 0.9292, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [2148/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7095, Val Acc: 0.9314\n",
            "Epoch [2149/4000], Train Loss: 1.7105, Train Acc: 0.9301, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [2150/4000], Train Loss: 1.7128, Train Acc: 0.9274, Val Loss: 1.7111, Val Acc: 0.9321\n",
            "Epoch [2151/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7089, Val Acc: 0.9301\n",
            "Epoch [2152/4000], Train Loss: 1.7120, Train Acc: 0.9277, Val Loss: 1.7132, Val Acc: 0.9237\n",
            "Epoch [2153/4000], Train Loss: 1.7120, Train Acc: 0.9282, Val Loss: 1.7080, Val Acc: 0.9333\n",
            "Epoch [2154/4000], Train Loss: 1.7098, Train Acc: 0.9290, Val Loss: 1.7089, Val Acc: 0.9327\n",
            "Epoch [2155/4000], Train Loss: 1.7106, Train Acc: 0.9285, Val Loss: 1.7107, Val Acc: 0.9295\n",
            "Epoch [2156/4000], Train Loss: 1.7121, Train Acc: 0.9277, Val Loss: 1.7076, Val Acc: 0.9308\n",
            "Epoch [2157/4000], Train Loss: 1.7106, Train Acc: 0.9300, Val Loss: 1.7096, Val Acc: 0.9308\n",
            "Epoch [2158/4000], Train Loss: 1.7096, Train Acc: 0.9303, Val Loss: 1.7076, Val Acc: 0.9276\n",
            "Epoch [2159/4000], Train Loss: 1.7100, Train Acc: 0.9296, Val Loss: 1.7082, Val Acc: 0.9314\n",
            "Epoch [2160/4000], Train Loss: 1.7108, Train Acc: 0.9303, Val Loss: 1.7109, Val Acc: 0.9288\n",
            "Epoch [2161/4000], Train Loss: 1.7122, Train Acc: 0.9253, Val Loss: 1.7125, Val Acc: 0.9269\n",
            "Epoch [2162/4000], Train Loss: 1.7125, Train Acc: 0.9269, Val Loss: 1.7089, Val Acc: 0.9301\n",
            "Epoch [2163/4000], Train Loss: 1.7128, Train Acc: 0.9277, Val Loss: 1.7078, Val Acc: 0.9301\n",
            "Epoch [2164/4000], Train Loss: 1.7114, Train Acc: 0.9284, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [2165/4000], Train Loss: 1.7116, Train Acc: 0.9280, Val Loss: 1.7122, Val Acc: 0.9250\n",
            "Epoch [2166/4000], Train Loss: 1.7097, Train Acc: 0.9288, Val Loss: 1.7092, Val Acc: 0.9308\n",
            "Epoch [2167/4000], Train Loss: 1.7113, Train Acc: 0.9292, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [2168/4000], Train Loss: 1.7126, Train Acc: 0.9277, Val Loss: 1.7131, Val Acc: 0.9288\n",
            "Epoch [2169/4000], Train Loss: 1.7111, Train Acc: 0.9284, Val Loss: 1.7149, Val Acc: 0.9276\n",
            "Epoch [2170/4000], Train Loss: 1.7114, Train Acc: 0.9285, Val Loss: 1.7078, Val Acc: 0.9288\n",
            "Epoch [2171/4000], Train Loss: 1.7122, Train Acc: 0.9276, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [2172/4000], Train Loss: 1.7110, Train Acc: 0.9285, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [2173/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7135, Val Acc: 0.9250\n",
            "Epoch [2174/4000], Train Loss: 1.7148, Train Acc: 0.9252, Val Loss: 1.7101, Val Acc: 0.9282\n",
            "Epoch [2175/4000], Train Loss: 1.7103, Train Acc: 0.9282, Val Loss: 1.7137, Val Acc: 0.9256\n",
            "Epoch [2176/4000], Train Loss: 1.7102, Train Acc: 0.9292, Val Loss: 1.7162, Val Acc: 0.9301\n",
            "Epoch [2177/4000], Train Loss: 1.7105, Train Acc: 0.9295, Val Loss: 1.7169, Val Acc: 0.9192\n",
            "Epoch [2178/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [2179/4000], Train Loss: 1.7095, Train Acc: 0.9301, Val Loss: 1.7068, Val Acc: 0.9314\n",
            "Epoch [2180/4000], Train Loss: 1.7099, Train Acc: 0.9293, Val Loss: 1.7094, Val Acc: 0.9295\n",
            "Epoch [2181/4000], Train Loss: 1.7109, Train Acc: 0.9284, Val Loss: 1.7128, Val Acc: 0.9276\n",
            "Epoch [2182/4000], Train Loss: 1.7132, Train Acc: 0.9271, Val Loss: 1.7095, Val Acc: 0.9308\n",
            "Epoch [2183/4000], Train Loss: 1.7098, Train Acc: 0.9284, Val Loss: 1.7086, Val Acc: 0.9282\n",
            "Epoch [2184/4000], Train Loss: 1.7108, Train Acc: 0.9296, Val Loss: 1.7118, Val Acc: 0.9276\n",
            "Epoch [2185/4000], Train Loss: 1.7111, Train Acc: 0.9282, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [2186/4000], Train Loss: 1.7125, Train Acc: 0.9277, Val Loss: 1.7112, Val Acc: 0.9263\n",
            "Epoch [2187/4000], Train Loss: 1.7107, Train Acc: 0.9284, Val Loss: 1.7097, Val Acc: 0.9250\n",
            "Epoch [2188/4000], Train Loss: 1.7127, Train Acc: 0.9272, Val Loss: 1.7113, Val Acc: 0.9333\n",
            "Epoch [2189/4000], Train Loss: 1.7115, Train Acc: 0.9287, Val Loss: 1.7115, Val Acc: 0.9301\n",
            "Epoch [2190/4000], Train Loss: 1.7103, Train Acc: 0.9287, Val Loss: 1.7067, Val Acc: 0.9314\n",
            "Epoch [2191/4000], Train Loss: 1.7102, Train Acc: 0.9304, Val Loss: 1.7097, Val Acc: 0.9308\n",
            "Epoch [2192/4000], Train Loss: 1.7113, Train Acc: 0.9269, Val Loss: 1.7107, Val Acc: 0.9301\n",
            "Epoch [2193/4000], Train Loss: 1.7121, Train Acc: 0.9274, Val Loss: 1.7119, Val Acc: 0.9256\n",
            "Epoch [2194/4000], Train Loss: 1.7116, Train Acc: 0.9280, Val Loss: 1.7102, Val Acc: 0.9321\n",
            "Epoch [2195/4000], Train Loss: 1.7123, Train Acc: 0.9269, Val Loss: 1.7141, Val Acc: 0.9199\n",
            "Epoch [2196/4000], Train Loss: 1.7131, Train Acc: 0.9242, Val Loss: 1.7177, Val Acc: 0.9199\n",
            "Epoch [2197/4000], Train Loss: 1.7098, Train Acc: 0.9287, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [2198/4000], Train Loss: 1.7110, Train Acc: 0.9293, Val Loss: 1.7102, Val Acc: 0.9282\n",
            "Epoch [2199/4000], Train Loss: 1.7114, Train Acc: 0.9300, Val Loss: 1.7085, Val Acc: 0.9308\n",
            "Epoch [2200/4000], Train Loss: 1.7105, Train Acc: 0.9285, Val Loss: 1.7144, Val Acc: 0.9269\n",
            "Epoch [2201/4000], Train Loss: 1.7124, Train Acc: 0.9266, Val Loss: 1.7120, Val Acc: 0.9256\n",
            "Epoch [2202/4000], Train Loss: 1.7112, Train Acc: 0.9279, Val Loss: 1.7094, Val Acc: 0.9321\n",
            "Epoch [2203/4000], Train Loss: 1.7110, Train Acc: 0.9268, Val Loss: 1.7117, Val Acc: 0.9308\n",
            "Epoch [2204/4000], Train Loss: 1.7100, Train Acc: 0.9292, Val Loss: 1.7089, Val Acc: 0.9263\n",
            "Epoch [2205/4000], Train Loss: 1.7120, Train Acc: 0.9288, Val Loss: 1.7084, Val Acc: 0.9340\n",
            "Epoch [2206/4000], Train Loss: 1.7116, Train Acc: 0.9280, Val Loss: 1.7132, Val Acc: 0.9276\n",
            "Epoch [2207/4000], Train Loss: 1.7110, Train Acc: 0.9282, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [2208/4000], Train Loss: 1.7108, Train Acc: 0.9287, Val Loss: 1.7072, Val Acc: 0.9333\n",
            "Epoch [2209/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7092, Val Acc: 0.9308\n",
            "Epoch [2210/4000], Train Loss: 1.7116, Train Acc: 0.9290, Val Loss: 1.7077, Val Acc: 0.9308\n",
            "Epoch [2211/4000], Train Loss: 1.7105, Train Acc: 0.9285, Val Loss: 1.7121, Val Acc: 0.9288\n",
            "Epoch [2212/4000], Train Loss: 1.7108, Train Acc: 0.9300, Val Loss: 1.7124, Val Acc: 0.9263\n",
            "Epoch [2213/4000], Train Loss: 1.7116, Train Acc: 0.9277, Val Loss: 1.7086, Val Acc: 0.9340\n",
            "Epoch [2214/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7117, Val Acc: 0.9288\n",
            "Epoch [2215/4000], Train Loss: 1.7132, Train Acc: 0.9272, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [2216/4000], Train Loss: 1.7117, Train Acc: 0.9279, Val Loss: 1.7061, Val Acc: 0.9295\n",
            "Epoch [2217/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7079, Val Acc: 0.9340\n",
            "Epoch [2218/4000], Train Loss: 1.7122, Train Acc: 0.9277, Val Loss: 1.7138, Val Acc: 0.9244\n",
            "Epoch [2219/4000], Train Loss: 1.7142, Train Acc: 0.9266, Val Loss: 1.7077, Val Acc: 0.9295\n",
            "Epoch [2220/4000], Train Loss: 1.7116, Train Acc: 0.9284, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [2221/4000], Train Loss: 1.7121, Train Acc: 0.9266, Val Loss: 1.7090, Val Acc: 0.9308\n",
            "Epoch [2222/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7145, Val Acc: 0.9250\n",
            "Epoch [2223/4000], Train Loss: 1.7102, Train Acc: 0.9288, Val Loss: 1.7097, Val Acc: 0.9295\n",
            "Epoch [2224/4000], Train Loss: 1.7102, Train Acc: 0.9293, Val Loss: 1.7116, Val Acc: 0.9276\n",
            "Epoch [2225/4000], Train Loss: 1.7106, Train Acc: 0.9292, Val Loss: 1.7094, Val Acc: 0.9288\n",
            "Epoch [2226/4000], Train Loss: 1.7120, Train Acc: 0.9280, Val Loss: 1.7161, Val Acc: 0.9250\n",
            "Epoch [2227/4000], Train Loss: 1.7129, Train Acc: 0.9271, Val Loss: 1.7123, Val Acc: 0.9288\n",
            "Epoch [2228/4000], Train Loss: 1.7101, Train Acc: 0.9284, Val Loss: 1.7150, Val Acc: 0.9224\n",
            "Epoch [2229/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7055, Val Acc: 0.9340\n",
            "Epoch [2230/4000], Train Loss: 1.7100, Train Acc: 0.9296, Val Loss: 1.7055, Val Acc: 0.9295\n",
            "Epoch [2231/4000], Train Loss: 1.7114, Train Acc: 0.9274, Val Loss: 1.7149, Val Acc: 0.9263\n",
            "Epoch [2232/4000], Train Loss: 1.7103, Train Acc: 0.9287, Val Loss: 1.7088, Val Acc: 0.9282\n",
            "Epoch [2233/4000], Train Loss: 1.7110, Train Acc: 0.9277, Val Loss: 1.7076, Val Acc: 0.9321\n",
            "Epoch [2234/4000], Train Loss: 1.7108, Train Acc: 0.9296, Val Loss: 1.7131, Val Acc: 0.9250\n",
            "Epoch [2235/4000], Train Loss: 1.7129, Train Acc: 0.9292, Val Loss: 1.7086, Val Acc: 0.9308\n",
            "Epoch [2236/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7084, Val Acc: 0.9301\n",
            "Epoch [2237/4000], Train Loss: 1.7106, Train Acc: 0.9288, Val Loss: 1.7088, Val Acc: 0.9321\n",
            "Epoch [2238/4000], Train Loss: 1.7104, Train Acc: 0.9279, Val Loss: 1.7087, Val Acc: 0.9308\n",
            "Epoch [2239/4000], Train Loss: 1.7118, Train Acc: 0.9282, Val Loss: 1.7117, Val Acc: 0.9321\n",
            "Epoch [2240/4000], Train Loss: 1.7108, Train Acc: 0.9284, Val Loss: 1.7113, Val Acc: 0.9269\n",
            "Epoch [2241/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7113, Val Acc: 0.9314\n",
            "Epoch [2242/4000], Train Loss: 1.7097, Train Acc: 0.9293, Val Loss: 1.7078, Val Acc: 0.9282\n",
            "Epoch [2243/4000], Train Loss: 1.7102, Train Acc: 0.9316, Val Loss: 1.7209, Val Acc: 0.9186\n",
            "Epoch [2244/4000], Train Loss: 1.7121, Train Acc: 0.9284, Val Loss: 1.7091, Val Acc: 0.9282\n",
            "Epoch [2245/4000], Train Loss: 1.7107, Train Acc: 0.9300, Val Loss: 1.7095, Val Acc: 0.9308\n",
            "Epoch [2246/4000], Train Loss: 1.7107, Train Acc: 0.9279, Val Loss: 1.7104, Val Acc: 0.9301\n",
            "Epoch [2247/4000], Train Loss: 1.7102, Train Acc: 0.9284, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [2248/4000], Train Loss: 1.7110, Train Acc: 0.9300, Val Loss: 1.7163, Val Acc: 0.9256\n",
            "Epoch [2249/4000], Train Loss: 1.7118, Train Acc: 0.9287, Val Loss: 1.7161, Val Acc: 0.9269\n",
            "Epoch [2250/4000], Train Loss: 1.7107, Train Acc: 0.9285, Val Loss: 1.7095, Val Acc: 0.9301\n",
            "Epoch [2251/4000], Train Loss: 1.7116, Train Acc: 0.9264, Val Loss: 1.7095, Val Acc: 0.9301\n",
            "Epoch [2252/4000], Train Loss: 1.7096, Train Acc: 0.9287, Val Loss: 1.7099, Val Acc: 0.9327\n",
            "Epoch [2253/4000], Train Loss: 1.7108, Train Acc: 0.9274, Val Loss: 1.7096, Val Acc: 0.9327\n",
            "Epoch [2254/4000], Train Loss: 1.7095, Train Acc: 0.9301, Val Loss: 1.7094, Val Acc: 0.9301\n",
            "Epoch [2255/4000], Train Loss: 1.7097, Train Acc: 0.9290, Val Loss: 1.7114, Val Acc: 0.9295\n",
            "Epoch [2256/4000], Train Loss: 1.7112, Train Acc: 0.9290, Val Loss: 1.7101, Val Acc: 0.9295\n",
            "Epoch [2257/4000], Train Loss: 1.7094, Train Acc: 0.9290, Val Loss: 1.7152, Val Acc: 0.9218\n",
            "Epoch [2258/4000], Train Loss: 1.7112, Train Acc: 0.9284, Val Loss: 1.7099, Val Acc: 0.9301\n",
            "Epoch [2259/4000], Train Loss: 1.7100, Train Acc: 0.9290, Val Loss: 1.7089, Val Acc: 0.9301\n",
            "Epoch [2260/4000], Train Loss: 1.7120, Train Acc: 0.9290, Val Loss: 1.7159, Val Acc: 0.9295\n",
            "Epoch [2261/4000], Train Loss: 1.7104, Train Acc: 0.9296, Val Loss: 1.7126, Val Acc: 0.9301\n",
            "Epoch [2262/4000], Train Loss: 1.7120, Train Acc: 0.9279, Val Loss: 1.7142, Val Acc: 0.9256\n",
            "Epoch [2263/4000], Train Loss: 1.7114, Train Acc: 0.9279, Val Loss: 1.7092, Val Acc: 0.9276\n",
            "Epoch [2264/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7132, Val Acc: 0.9263\n",
            "Epoch [2265/4000], Train Loss: 1.7099, Train Acc: 0.9293, Val Loss: 1.7083, Val Acc: 0.9333\n",
            "Epoch [2266/4000], Train Loss: 1.7105, Train Acc: 0.9300, Val Loss: 1.7153, Val Acc: 0.9308\n",
            "Epoch [2267/4000], Train Loss: 1.7144, Train Acc: 0.9252, Val Loss: 1.7113, Val Acc: 0.9295\n",
            "Epoch [2268/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7098, Val Acc: 0.9308\n",
            "Epoch [2269/4000], Train Loss: 1.7105, Train Acc: 0.9280, Val Loss: 1.7065, Val Acc: 0.9301\n",
            "Epoch [2270/4000], Train Loss: 1.7115, Train Acc: 0.9274, Val Loss: 1.7160, Val Acc: 0.9288\n",
            "Epoch [2271/4000], Train Loss: 1.7135, Train Acc: 0.9247, Val Loss: 1.7125, Val Acc: 0.9256\n",
            "Epoch [2272/4000], Train Loss: 1.7102, Train Acc: 0.9295, Val Loss: 1.7160, Val Acc: 0.9212\n",
            "Epoch [2273/4000], Train Loss: 1.7118, Train Acc: 0.9271, Val Loss: 1.7219, Val Acc: 0.9205\n",
            "Epoch [2274/4000], Train Loss: 1.7131, Train Acc: 0.9255, Val Loss: 1.7121, Val Acc: 0.9295\n",
            "Epoch [2275/4000], Train Loss: 1.7117, Train Acc: 0.9263, Val Loss: 1.7101, Val Acc: 0.9301\n",
            "Epoch [2276/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7132, Val Acc: 0.9301\n",
            "Epoch [2277/4000], Train Loss: 1.7099, Train Acc: 0.9296, Val Loss: 1.7092, Val Acc: 0.9308\n",
            "Epoch [2278/4000], Train Loss: 1.7124, Train Acc: 0.9272, Val Loss: 1.7088, Val Acc: 0.9314\n",
            "Epoch [2279/4000], Train Loss: 1.7099, Train Acc: 0.9282, Val Loss: 1.7062, Val Acc: 0.9327\n",
            "Epoch [2280/4000], Train Loss: 1.7109, Train Acc: 0.9279, Val Loss: 1.7076, Val Acc: 0.9308\n",
            "Epoch [2281/4000], Train Loss: 1.7107, Train Acc: 0.9288, Val Loss: 1.7083, Val Acc: 0.9282\n",
            "Epoch [2282/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [2283/4000], Train Loss: 1.7106, Train Acc: 0.9279, Val Loss: 1.7088, Val Acc: 0.9301\n",
            "Epoch [2284/4000], Train Loss: 1.7118, Train Acc: 0.9268, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [2285/4000], Train Loss: 1.7108, Train Acc: 0.9300, Val Loss: 1.7136, Val Acc: 0.9269\n",
            "Epoch [2286/4000], Train Loss: 1.7108, Train Acc: 0.9287, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [2287/4000], Train Loss: 1.7101, Train Acc: 0.9301, Val Loss: 1.7131, Val Acc: 0.9295\n",
            "Epoch [2288/4000], Train Loss: 1.7127, Train Acc: 0.9263, Val Loss: 1.7082, Val Acc: 0.9327\n",
            "Epoch [2289/4000], Train Loss: 1.7111, Train Acc: 0.9293, Val Loss: 1.7130, Val Acc: 0.9276\n",
            "Epoch [2290/4000], Train Loss: 1.7101, Train Acc: 0.9293, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [2291/4000], Train Loss: 1.7108, Train Acc: 0.9279, Val Loss: 1.7088, Val Acc: 0.9288\n",
            "Epoch [2292/4000], Train Loss: 1.7109, Train Acc: 0.9285, Val Loss: 1.7082, Val Acc: 0.9295\n",
            "Epoch [2293/4000], Train Loss: 1.7104, Train Acc: 0.9288, Val Loss: 1.7053, Val Acc: 0.9340\n",
            "Epoch [2294/4000], Train Loss: 1.7128, Train Acc: 0.9272, Val Loss: 1.7109, Val Acc: 0.9256\n",
            "Epoch [2295/4000], Train Loss: 1.7109, Train Acc: 0.9284, Val Loss: 1.7109, Val Acc: 0.9314\n",
            "Epoch [2296/4000], Train Loss: 1.7128, Train Acc: 0.9277, Val Loss: 1.7113, Val Acc: 0.9282\n",
            "Epoch [2297/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7106, Val Acc: 0.9327\n",
            "Epoch [2298/4000], Train Loss: 1.7102, Train Acc: 0.9282, Val Loss: 1.7098, Val Acc: 0.9308\n",
            "Epoch [2299/4000], Train Loss: 1.7095, Train Acc: 0.9296, Val Loss: 1.7102, Val Acc: 0.9314\n",
            "Epoch [2300/4000], Train Loss: 1.7103, Train Acc: 0.9284, Val Loss: 1.7130, Val Acc: 0.9269\n",
            "Epoch [2301/4000], Train Loss: 1.7124, Train Acc: 0.9279, Val Loss: 1.7081, Val Acc: 0.9314\n",
            "Epoch [2302/4000], Train Loss: 1.7110, Train Acc: 0.9282, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [2303/4000], Train Loss: 1.7126, Train Acc: 0.9287, Val Loss: 1.7101, Val Acc: 0.9314\n",
            "Epoch [2304/4000], Train Loss: 1.7109, Train Acc: 0.9284, Val Loss: 1.7091, Val Acc: 0.9321\n",
            "Epoch [2305/4000], Train Loss: 1.7106, Train Acc: 0.9276, Val Loss: 1.7116, Val Acc: 0.9282\n",
            "Epoch [2306/4000], Train Loss: 1.7095, Train Acc: 0.9288, Val Loss: 1.7086, Val Acc: 0.9288\n",
            "Epoch [2307/4000], Train Loss: 1.7101, Train Acc: 0.9280, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [2308/4000], Train Loss: 1.7097, Train Acc: 0.9300, Val Loss: 1.7109, Val Acc: 0.9308\n",
            "Epoch [2309/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7098, Val Acc: 0.9346\n",
            "Epoch [2310/4000], Train Loss: 1.7114, Train Acc: 0.9290, Val Loss: 1.7136, Val Acc: 0.9269\n",
            "Epoch [2311/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7176, Val Acc: 0.9244\n",
            "Epoch [2312/4000], Train Loss: 1.7117, Train Acc: 0.9285, Val Loss: 1.7096, Val Acc: 0.9308\n",
            "Epoch [2313/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7083, Val Acc: 0.9327\n",
            "Epoch [2314/4000], Train Loss: 1.7117, Train Acc: 0.9274, Val Loss: 1.7073, Val Acc: 0.9340\n",
            "Epoch [2315/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7109, Val Acc: 0.9269\n",
            "Epoch [2316/4000], Train Loss: 1.7110, Train Acc: 0.9296, Val Loss: 1.7105, Val Acc: 0.9308\n",
            "Epoch [2317/4000], Train Loss: 1.7099, Train Acc: 0.9277, Val Loss: 1.7124, Val Acc: 0.9263\n",
            "Epoch [2318/4000], Train Loss: 1.7119, Train Acc: 0.9285, Val Loss: 1.7136, Val Acc: 0.9288\n",
            "Epoch [2319/4000], Train Loss: 1.7108, Train Acc: 0.9292, Val Loss: 1.7133, Val Acc: 0.9282\n",
            "Epoch [2320/4000], Train Loss: 1.7118, Train Acc: 0.9271, Val Loss: 1.7074, Val Acc: 0.9301\n",
            "Epoch [2321/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7075, Val Acc: 0.9327\n",
            "Epoch [2322/4000], Train Loss: 1.7104, Train Acc: 0.9292, Val Loss: 1.7084, Val Acc: 0.9282\n",
            "Epoch [2323/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7088, Val Acc: 0.9276\n",
            "Epoch [2324/4000], Train Loss: 1.7116, Train Acc: 0.9274, Val Loss: 1.7097, Val Acc: 0.9282\n",
            "Epoch [2325/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7091, Val Acc: 0.9314\n",
            "Epoch [2326/4000], Train Loss: 1.7103, Train Acc: 0.9303, Val Loss: 1.7096, Val Acc: 0.9269\n",
            "Epoch [2327/4000], Train Loss: 1.7106, Train Acc: 0.9292, Val Loss: 1.7084, Val Acc: 0.9308\n",
            "Epoch [2328/4000], Train Loss: 1.7088, Train Acc: 0.9300, Val Loss: 1.7091, Val Acc: 0.9288\n",
            "Epoch [2329/4000], Train Loss: 1.7113, Train Acc: 0.9287, Val Loss: 1.7110, Val Acc: 0.9295\n",
            "Epoch [2330/4000], Train Loss: 1.7111, Train Acc: 0.9301, Val Loss: 1.7187, Val Acc: 0.9218\n",
            "Epoch [2331/4000], Train Loss: 1.7107, Train Acc: 0.9266, Val Loss: 1.7072, Val Acc: 0.9321\n",
            "Epoch [2332/4000], Train Loss: 1.7089, Train Acc: 0.9308, Val Loss: 1.7075, Val Acc: 0.9327\n",
            "Epoch [2333/4000], Train Loss: 1.7106, Train Acc: 0.9288, Val Loss: 1.7079, Val Acc: 0.9301\n",
            "Epoch [2334/4000], Train Loss: 1.7093, Train Acc: 0.9304, Val Loss: 1.7091, Val Acc: 0.9288\n",
            "Epoch [2335/4000], Train Loss: 1.7109, Train Acc: 0.9285, Val Loss: 1.7116, Val Acc: 0.9282\n",
            "Epoch [2336/4000], Train Loss: 1.7111, Train Acc: 0.9293, Val Loss: 1.7159, Val Acc: 0.9269\n",
            "Epoch [2337/4000], Train Loss: 1.7100, Train Acc: 0.9292, Val Loss: 1.7119, Val Acc: 0.9288\n",
            "Epoch [2338/4000], Train Loss: 1.7102, Train Acc: 0.9285, Val Loss: 1.7166, Val Acc: 0.9295\n",
            "Epoch [2339/4000], Train Loss: 1.7108, Train Acc: 0.9285, Val Loss: 1.7084, Val Acc: 0.9308\n",
            "Epoch [2340/4000], Train Loss: 1.7116, Train Acc: 0.9296, Val Loss: 1.7159, Val Acc: 0.9269\n",
            "Epoch [2341/4000], Train Loss: 1.7097, Train Acc: 0.9300, Val Loss: 1.7129, Val Acc: 0.9314\n",
            "Epoch [2342/4000], Train Loss: 1.7099, Train Acc: 0.9293, Val Loss: 1.7161, Val Acc: 0.9256\n",
            "Epoch [2343/4000], Train Loss: 1.7118, Train Acc: 0.9277, Val Loss: 1.7106, Val Acc: 0.9314\n",
            "Epoch [2344/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7080, Val Acc: 0.9321\n",
            "Epoch [2345/4000], Train Loss: 1.7124, Train Acc: 0.9272, Val Loss: 1.7072, Val Acc: 0.9321\n",
            "Epoch [2346/4000], Train Loss: 1.7111, Train Acc: 0.9300, Val Loss: 1.7103, Val Acc: 0.9314\n",
            "Epoch [2347/4000], Train Loss: 1.7114, Train Acc: 0.9282, Val Loss: 1.7106, Val Acc: 0.9321\n",
            "Epoch [2348/4000], Train Loss: 1.7108, Train Acc: 0.9288, Val Loss: 1.7069, Val Acc: 0.9301\n",
            "Epoch [2349/4000], Train Loss: 1.7113, Train Acc: 0.9272, Val Loss: 1.7076, Val Acc: 0.9314\n",
            "Epoch [2350/4000], Train Loss: 1.7111, Train Acc: 0.9290, Val Loss: 1.7076, Val Acc: 0.9321\n",
            "Epoch [2351/4000], Train Loss: 1.7091, Train Acc: 0.9293, Val Loss: 1.7100, Val Acc: 0.9269\n",
            "Epoch [2352/4000], Train Loss: 1.7101, Train Acc: 0.9293, Val Loss: 1.7132, Val Acc: 0.9269\n",
            "Epoch [2353/4000], Train Loss: 1.7099, Train Acc: 0.9285, Val Loss: 1.7066, Val Acc: 0.9308\n",
            "Epoch [2354/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7081, Val Acc: 0.9327\n",
            "Epoch [2355/4000], Train Loss: 1.7111, Train Acc: 0.9295, Val Loss: 1.7168, Val Acc: 0.9231\n",
            "Epoch [2356/4000], Train Loss: 1.7098, Train Acc: 0.9295, Val Loss: 1.7104, Val Acc: 0.9269\n",
            "Epoch [2357/4000], Train Loss: 1.7104, Train Acc: 0.9279, Val Loss: 1.7111, Val Acc: 0.9282\n",
            "Epoch [2358/4000], Train Loss: 1.7094, Train Acc: 0.9300, Val Loss: 1.7096, Val Acc: 0.9276\n",
            "Epoch [2359/4000], Train Loss: 1.7101, Train Acc: 0.9296, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [2360/4000], Train Loss: 1.7115, Train Acc: 0.9293, Val Loss: 1.7122, Val Acc: 0.9276\n",
            "Epoch [2361/4000], Train Loss: 1.7100, Train Acc: 0.9301, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [2362/4000], Train Loss: 1.7126, Train Acc: 0.9282, Val Loss: 1.7125, Val Acc: 0.9269\n",
            "Epoch [2363/4000], Train Loss: 1.7122, Train Acc: 0.9277, Val Loss: 1.7175, Val Acc: 0.9269\n",
            "Epoch [2364/4000], Train Loss: 1.7101, Train Acc: 0.9293, Val Loss: 1.7082, Val Acc: 0.9314\n",
            "Epoch [2365/4000], Train Loss: 1.7107, Train Acc: 0.9279, Val Loss: 1.7089, Val Acc: 0.9314\n",
            "Epoch [2366/4000], Train Loss: 1.7110, Train Acc: 0.9282, Val Loss: 1.7094, Val Acc: 0.9295\n",
            "Epoch [2367/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7106, Val Acc: 0.9288\n",
            "Epoch [2368/4000], Train Loss: 1.7114, Train Acc: 0.9271, Val Loss: 1.7087, Val Acc: 0.9308\n",
            "Epoch [2369/4000], Train Loss: 1.7099, Train Acc: 0.9292, Val Loss: 1.7142, Val Acc: 0.9263\n",
            "Epoch [2370/4000], Train Loss: 1.7111, Train Acc: 0.9290, Val Loss: 1.7121, Val Acc: 0.9269\n",
            "Epoch [2371/4000], Train Loss: 1.7119, Train Acc: 0.9280, Val Loss: 1.7124, Val Acc: 0.9308\n",
            "Epoch [2372/4000], Train Loss: 1.7115, Train Acc: 0.9269, Val Loss: 1.7151, Val Acc: 0.9244\n",
            "Epoch [2373/4000], Train Loss: 1.7099, Train Acc: 0.9287, Val Loss: 1.7128, Val Acc: 0.9256\n",
            "Epoch [2374/4000], Train Loss: 1.7119, Train Acc: 0.9274, Val Loss: 1.7098, Val Acc: 0.9321\n",
            "Epoch [2375/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7124, Val Acc: 0.9276\n",
            "Epoch [2376/4000], Train Loss: 1.7108, Train Acc: 0.9292, Val Loss: 1.7094, Val Acc: 0.9321\n",
            "Epoch [2377/4000], Train Loss: 1.7106, Train Acc: 0.9304, Val Loss: 1.7152, Val Acc: 0.9295\n",
            "Epoch [2378/4000], Train Loss: 1.7114, Train Acc: 0.9285, Val Loss: 1.7113, Val Acc: 0.9250\n",
            "Epoch [2379/4000], Train Loss: 1.7138, Train Acc: 0.9271, Val Loss: 1.7065, Val Acc: 0.9333\n",
            "Epoch [2380/4000], Train Loss: 1.7121, Train Acc: 0.9263, Val Loss: 1.7071, Val Acc: 0.9288\n",
            "Epoch [2381/4000], Train Loss: 1.7097, Train Acc: 0.9306, Val Loss: 1.7068, Val Acc: 0.9295\n",
            "Epoch [2382/4000], Train Loss: 1.7104, Train Acc: 0.9279, Val Loss: 1.7061, Val Acc: 0.9295\n",
            "Epoch [2383/4000], Train Loss: 1.7090, Train Acc: 0.9304, Val Loss: 1.7082, Val Acc: 0.9327\n",
            "Epoch [2384/4000], Train Loss: 1.7098, Train Acc: 0.9303, Val Loss: 1.7084, Val Acc: 0.9333\n",
            "Epoch [2385/4000], Train Loss: 1.7102, Train Acc: 0.9292, Val Loss: 1.7094, Val Acc: 0.9308\n",
            "Epoch [2386/4000], Train Loss: 1.7106, Train Acc: 0.9293, Val Loss: 1.7096, Val Acc: 0.9276\n",
            "Epoch [2387/4000], Train Loss: 1.7092, Train Acc: 0.9306, Val Loss: 1.7125, Val Acc: 0.9263\n",
            "Epoch [2388/4000], Train Loss: 1.7092, Train Acc: 0.9293, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [2389/4000], Train Loss: 1.7107, Train Acc: 0.9287, Val Loss: 1.7074, Val Acc: 0.9327\n",
            "Epoch [2390/4000], Train Loss: 1.7119, Train Acc: 0.9279, Val Loss: 1.7110, Val Acc: 0.9263\n",
            "Epoch [2391/4000], Train Loss: 1.7100, Train Acc: 0.9304, Val Loss: 1.7154, Val Acc: 0.9276\n",
            "Epoch [2392/4000], Train Loss: 1.7104, Train Acc: 0.9288, Val Loss: 1.7091, Val Acc: 0.9282\n",
            "Epoch [2393/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7095, Val Acc: 0.9288\n",
            "Epoch [2394/4000], Train Loss: 1.7111, Train Acc: 0.9288, Val Loss: 1.7179, Val Acc: 0.9256\n",
            "Epoch [2395/4000], Train Loss: 1.7109, Train Acc: 0.9295, Val Loss: 1.7164, Val Acc: 0.9250\n",
            "Epoch [2396/4000], Train Loss: 1.7114, Train Acc: 0.9271, Val Loss: 1.7118, Val Acc: 0.9288\n",
            "Epoch [2397/4000], Train Loss: 1.7117, Train Acc: 0.9268, Val Loss: 1.7082, Val Acc: 0.9301\n",
            "Epoch [2398/4000], Train Loss: 1.7095, Train Acc: 0.9301, Val Loss: 1.7077, Val Acc: 0.9308\n",
            "Epoch [2399/4000], Train Loss: 1.7088, Train Acc: 0.9301, Val Loss: 1.7076, Val Acc: 0.9314\n",
            "Epoch [2400/4000], Train Loss: 1.7106, Train Acc: 0.9300, Val Loss: 1.7080, Val Acc: 0.9333\n",
            "Epoch [2401/4000], Train Loss: 1.7096, Train Acc: 0.9295, Val Loss: 1.7157, Val Acc: 0.9250\n",
            "Epoch [2402/4000], Train Loss: 1.7107, Train Acc: 0.9292, Val Loss: 1.7150, Val Acc: 0.9276\n",
            "Epoch [2403/4000], Train Loss: 1.7105, Train Acc: 0.9282, Val Loss: 1.7084, Val Acc: 0.9308\n",
            "Epoch [2404/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7129, Val Acc: 0.9295\n",
            "Epoch [2405/4000], Train Loss: 1.7124, Train Acc: 0.9288, Val Loss: 1.7230, Val Acc: 0.9199\n",
            "Epoch [2406/4000], Train Loss: 1.7116, Train Acc: 0.9276, Val Loss: 1.7133, Val Acc: 0.9269\n",
            "Epoch [2407/4000], Train Loss: 1.7100, Train Acc: 0.9290, Val Loss: 1.7104, Val Acc: 0.9288\n",
            "Epoch [2408/4000], Train Loss: 1.7107, Train Acc: 0.9284, Val Loss: 1.7106, Val Acc: 0.9295\n",
            "Epoch [2409/4000], Train Loss: 1.7113, Train Acc: 0.9266, Val Loss: 1.7102, Val Acc: 0.9295\n",
            "Epoch [2410/4000], Train Loss: 1.7103, Train Acc: 0.9285, Val Loss: 1.7132, Val Acc: 0.9321\n",
            "Epoch [2411/4000], Train Loss: 1.7104, Train Acc: 0.9295, Val Loss: 1.7105, Val Acc: 0.9308\n",
            "Epoch [2412/4000], Train Loss: 1.7099, Train Acc: 0.9309, Val Loss: 1.7143, Val Acc: 0.9308\n",
            "Epoch [2413/4000], Train Loss: 1.7123, Train Acc: 0.9285, Val Loss: 1.7099, Val Acc: 0.9288\n",
            "Epoch [2414/4000], Train Loss: 1.7104, Train Acc: 0.9268, Val Loss: 1.7161, Val Acc: 0.9250\n",
            "Epoch [2415/4000], Train Loss: 1.7098, Train Acc: 0.9288, Val Loss: 1.7112, Val Acc: 0.9269\n",
            "Epoch [2416/4000], Train Loss: 1.7095, Train Acc: 0.9288, Val Loss: 1.7090, Val Acc: 0.9314\n",
            "Epoch [2417/4000], Train Loss: 1.7111, Train Acc: 0.9290, Val Loss: 1.7095, Val Acc: 0.9333\n",
            "Epoch [2418/4000], Train Loss: 1.7109, Train Acc: 0.9288, Val Loss: 1.7131, Val Acc: 0.9269\n",
            "Epoch [2419/4000], Train Loss: 1.7094, Train Acc: 0.9290, Val Loss: 1.7132, Val Acc: 0.9256\n",
            "Epoch [2420/4000], Train Loss: 1.7113, Train Acc: 0.9279, Val Loss: 1.7093, Val Acc: 0.9295\n",
            "Epoch [2421/4000], Train Loss: 1.7105, Train Acc: 0.9279, Val Loss: 1.7163, Val Acc: 0.9224\n",
            "Epoch [2422/4000], Train Loss: 1.7151, Train Acc: 0.9253, Val Loss: 1.7104, Val Acc: 0.9295\n",
            "Epoch [2423/4000], Train Loss: 1.7106, Train Acc: 0.9282, Val Loss: 1.7103, Val Acc: 0.9301\n",
            "Epoch [2424/4000], Train Loss: 1.7102, Train Acc: 0.9292, Val Loss: 1.7120, Val Acc: 0.9295\n",
            "Epoch [2425/4000], Train Loss: 1.7102, Train Acc: 0.9287, Val Loss: 1.7112, Val Acc: 0.9308\n",
            "Epoch [2426/4000], Train Loss: 1.7108, Train Acc: 0.9293, Val Loss: 1.7144, Val Acc: 0.9282\n",
            "Epoch [2427/4000], Train Loss: 1.7106, Train Acc: 0.9287, Val Loss: 1.7095, Val Acc: 0.9333\n",
            "Epoch [2428/4000], Train Loss: 1.7110, Train Acc: 0.9288, Val Loss: 1.7076, Val Acc: 0.9314\n",
            "Epoch [2429/4000], Train Loss: 1.7124, Train Acc: 0.9274, Val Loss: 1.7128, Val Acc: 0.9282\n",
            "Epoch [2430/4000], Train Loss: 1.7086, Train Acc: 0.9300, Val Loss: 1.7104, Val Acc: 0.9308\n",
            "Epoch [2431/4000], Train Loss: 1.7116, Train Acc: 0.9290, Val Loss: 1.7208, Val Acc: 0.9173\n",
            "Epoch [2432/4000], Train Loss: 1.7115, Train Acc: 0.9288, Val Loss: 1.7122, Val Acc: 0.9308\n",
            "Epoch [2433/4000], Train Loss: 1.7106, Train Acc: 0.9288, Val Loss: 1.7118, Val Acc: 0.9288\n",
            "Epoch [2434/4000], Train Loss: 1.7095, Train Acc: 0.9308, Val Loss: 1.7088, Val Acc: 0.9308\n",
            "Epoch [2435/4000], Train Loss: 1.7094, Train Acc: 0.9308, Val Loss: 1.7081, Val Acc: 0.9314\n",
            "Epoch [2436/4000], Train Loss: 1.7084, Train Acc: 0.9304, Val Loss: 1.7088, Val Acc: 0.9295\n",
            "Epoch [2437/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7107, Val Acc: 0.9288\n",
            "Epoch [2438/4000], Train Loss: 1.7110, Train Acc: 0.9288, Val Loss: 1.7077, Val Acc: 0.9327\n",
            "Epoch [2439/4000], Train Loss: 1.7099, Train Acc: 0.9282, Val Loss: 1.7104, Val Acc: 0.9327\n",
            "Epoch [2440/4000], Train Loss: 1.7115, Train Acc: 0.9263, Val Loss: 1.7124, Val Acc: 0.9340\n",
            "Epoch [2441/4000], Train Loss: 1.7121, Train Acc: 0.9287, Val Loss: 1.7167, Val Acc: 0.9237\n",
            "Epoch [2442/4000], Train Loss: 1.7096, Train Acc: 0.9300, Val Loss: 1.7092, Val Acc: 0.9308\n",
            "Epoch [2443/4000], Train Loss: 1.7097, Train Acc: 0.9296, Val Loss: 1.7109, Val Acc: 0.9276\n",
            "Epoch [2444/4000], Train Loss: 1.7111, Train Acc: 0.9295, Val Loss: 1.7093, Val Acc: 0.9321\n",
            "Epoch [2445/4000], Train Loss: 1.7095, Train Acc: 0.9301, Val Loss: 1.7108, Val Acc: 0.9295\n",
            "Epoch [2446/4000], Train Loss: 1.7105, Train Acc: 0.9290, Val Loss: 1.7138, Val Acc: 0.9301\n",
            "Epoch [2447/4000], Train Loss: 1.7092, Train Acc: 0.9292, Val Loss: 1.7103, Val Acc: 0.9321\n",
            "Epoch [2448/4000], Train Loss: 1.7113, Train Acc: 0.9284, Val Loss: 1.7098, Val Acc: 0.9308\n",
            "Epoch [2449/4000], Train Loss: 1.7092, Train Acc: 0.9306, Val Loss: 1.7153, Val Acc: 0.9327\n",
            "Epoch [2450/4000], Train Loss: 1.7116, Train Acc: 0.9276, Val Loss: 1.7136, Val Acc: 0.9269\n",
            "Epoch [2451/4000], Train Loss: 1.7094, Train Acc: 0.9303, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [2452/4000], Train Loss: 1.7115, Train Acc: 0.9287, Val Loss: 1.7105, Val Acc: 0.9308\n",
            "Epoch [2453/4000], Train Loss: 1.7113, Train Acc: 0.9287, Val Loss: 1.7088, Val Acc: 0.9295\n",
            "Epoch [2454/4000], Train Loss: 1.7104, Train Acc: 0.9284, Val Loss: 1.7049, Val Acc: 0.9308\n",
            "Epoch [2455/4000], Train Loss: 1.7103, Train Acc: 0.9295, Val Loss: 1.7140, Val Acc: 0.9301\n",
            "Epoch [2456/4000], Train Loss: 1.7100, Train Acc: 0.9284, Val Loss: 1.7079, Val Acc: 0.9314\n",
            "Epoch [2457/4000], Train Loss: 1.7096, Train Acc: 0.9288, Val Loss: 1.7128, Val Acc: 0.9256\n",
            "Epoch [2458/4000], Train Loss: 1.7120, Train Acc: 0.9292, Val Loss: 1.7194, Val Acc: 0.9244\n",
            "Epoch [2459/4000], Train Loss: 1.7093, Train Acc: 0.9285, Val Loss: 1.7094, Val Acc: 0.9288\n",
            "Epoch [2460/4000], Train Loss: 1.7100, Train Acc: 0.9284, Val Loss: 1.7087, Val Acc: 0.9314\n",
            "Epoch [2461/4000], Train Loss: 1.7086, Train Acc: 0.9303, Val Loss: 1.7248, Val Acc: 0.9160\n",
            "Epoch [2462/4000], Train Loss: 1.7106, Train Acc: 0.9309, Val Loss: 1.7094, Val Acc: 0.9308\n",
            "Epoch [2463/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7144, Val Acc: 0.9218\n",
            "Epoch [2464/4000], Train Loss: 1.7108, Train Acc: 0.9290, Val Loss: 1.7089, Val Acc: 0.9288\n",
            "Epoch [2465/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7089, Val Acc: 0.9308\n",
            "Epoch [2466/4000], Train Loss: 1.7103, Train Acc: 0.9271, Val Loss: 1.7127, Val Acc: 0.9288\n",
            "Epoch [2467/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7145, Val Acc: 0.9224\n",
            "Epoch [2468/4000], Train Loss: 1.7101, Train Acc: 0.9282, Val Loss: 1.7086, Val Acc: 0.9333\n",
            "Epoch [2469/4000], Train Loss: 1.7084, Train Acc: 0.9308, Val Loss: 1.7088, Val Acc: 0.9308\n",
            "Epoch [2470/4000], Train Loss: 1.7089, Train Acc: 0.9301, Val Loss: 1.7064, Val Acc: 0.9301\n",
            "Epoch [2471/4000], Train Loss: 1.7111, Train Acc: 0.9304, Val Loss: 1.7120, Val Acc: 0.9282\n",
            "Epoch [2472/4000], Train Loss: 1.7131, Train Acc: 0.9264, Val Loss: 1.7108, Val Acc: 0.9314\n",
            "Epoch [2473/4000], Train Loss: 1.7102, Train Acc: 0.9285, Val Loss: 1.7127, Val Acc: 0.9301\n",
            "Epoch [2474/4000], Train Loss: 1.7110, Train Acc: 0.9271, Val Loss: 1.7050, Val Acc: 0.9327\n",
            "Epoch [2475/4000], Train Loss: 1.7105, Train Acc: 0.9293, Val Loss: 1.7113, Val Acc: 0.9295\n",
            "Epoch [2476/4000], Train Loss: 1.7108, Train Acc: 0.9285, Val Loss: 1.7086, Val Acc: 0.9295\n",
            "Epoch [2477/4000], Train Loss: 1.7111, Train Acc: 0.9290, Val Loss: 1.7100, Val Acc: 0.9314\n",
            "Epoch [2478/4000], Train Loss: 1.7100, Train Acc: 0.9295, Val Loss: 1.7068, Val Acc: 0.9327\n",
            "Epoch [2479/4000], Train Loss: 1.7126, Train Acc: 0.9266, Val Loss: 1.7134, Val Acc: 0.9295\n",
            "Epoch [2480/4000], Train Loss: 1.7112, Train Acc: 0.9288, Val Loss: 1.7077, Val Acc: 0.9301\n",
            "Epoch [2481/4000], Train Loss: 1.7110, Train Acc: 0.9296, Val Loss: 1.7101, Val Acc: 0.9301\n",
            "Epoch [2482/4000], Train Loss: 1.7105, Train Acc: 0.9279, Val Loss: 1.7093, Val Acc: 0.9308\n",
            "Epoch [2483/4000], Train Loss: 1.7108, Train Acc: 0.9288, Val Loss: 1.7160, Val Acc: 0.9256\n",
            "Epoch [2484/4000], Train Loss: 1.7114, Train Acc: 0.9277, Val Loss: 1.7124, Val Acc: 0.9301\n",
            "Epoch [2485/4000], Train Loss: 1.7109, Train Acc: 0.9274, Val Loss: 1.7047, Val Acc: 0.9340\n",
            "Epoch [2486/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7116, Val Acc: 0.9308\n",
            "Epoch [2487/4000], Train Loss: 1.7112, Train Acc: 0.9298, Val Loss: 1.7150, Val Acc: 0.9250\n",
            "Epoch [2488/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7117, Val Acc: 0.9263\n",
            "Epoch [2489/4000], Train Loss: 1.7113, Train Acc: 0.9288, Val Loss: 1.7191, Val Acc: 0.9212\n",
            "Epoch [2490/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7056, Val Acc: 0.9353\n",
            "Epoch [2491/4000], Train Loss: 1.7093, Train Acc: 0.9298, Val Loss: 1.7177, Val Acc: 0.9269\n",
            "Epoch [2492/4000], Train Loss: 1.7093, Train Acc: 0.9303, Val Loss: 1.7066, Val Acc: 0.9314\n",
            "Epoch [2493/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7107, Val Acc: 0.9295\n",
            "Epoch [2494/4000], Train Loss: 1.7087, Train Acc: 0.9306, Val Loss: 1.7067, Val Acc: 0.9321\n",
            "Epoch [2495/4000], Train Loss: 1.7116, Train Acc: 0.9268, Val Loss: 1.7096, Val Acc: 0.9314\n",
            "Epoch [2496/4000], Train Loss: 1.7093, Train Acc: 0.9300, Val Loss: 1.7116, Val Acc: 0.9301\n",
            "Epoch [2497/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7212, Val Acc: 0.9224\n",
            "Epoch [2498/4000], Train Loss: 1.7105, Train Acc: 0.9295, Val Loss: 1.7093, Val Acc: 0.9295\n",
            "Epoch [2499/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7060, Val Acc: 0.9308\n",
            "Epoch [2500/4000], Train Loss: 1.7108, Train Acc: 0.9292, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [2501/4000], Train Loss: 1.7092, Train Acc: 0.9303, Val Loss: 1.7112, Val Acc: 0.9288\n",
            "Epoch [2502/4000], Train Loss: 1.7125, Train Acc: 0.9290, Val Loss: 1.7115, Val Acc: 0.9288\n",
            "Epoch [2503/4000], Train Loss: 1.7106, Train Acc: 0.9296, Val Loss: 1.7130, Val Acc: 0.9263\n",
            "Epoch [2504/4000], Train Loss: 1.7099, Train Acc: 0.9282, Val Loss: 1.7078, Val Acc: 0.9295\n",
            "Epoch [2505/4000], Train Loss: 1.7105, Train Acc: 0.9282, Val Loss: 1.7087, Val Acc: 0.9314\n",
            "Epoch [2506/4000], Train Loss: 1.7133, Train Acc: 0.9272, Val Loss: 1.7078, Val Acc: 0.9340\n",
            "Epoch [2507/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7137, Val Acc: 0.9276\n",
            "Epoch [2508/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7140, Val Acc: 0.9237\n",
            "Epoch [2509/4000], Train Loss: 1.7102, Train Acc: 0.9284, Val Loss: 1.7072, Val Acc: 0.9288\n",
            "Epoch [2510/4000], Train Loss: 1.7107, Train Acc: 0.9293, Val Loss: 1.7103, Val Acc: 0.9333\n",
            "Epoch [2511/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7132, Val Acc: 0.9263\n",
            "Epoch [2512/4000], Train Loss: 1.7110, Train Acc: 0.9293, Val Loss: 1.7124, Val Acc: 0.9308\n",
            "Epoch [2513/4000], Train Loss: 1.7112, Train Acc: 0.9284, Val Loss: 1.7118, Val Acc: 0.9263\n",
            "Epoch [2514/4000], Train Loss: 1.7098, Train Acc: 0.9293, Val Loss: 1.7077, Val Acc: 0.9327\n",
            "Epoch [2515/4000], Train Loss: 1.7108, Train Acc: 0.9296, Val Loss: 1.7102, Val Acc: 0.9301\n",
            "Epoch [2516/4000], Train Loss: 1.7112, Train Acc: 0.9293, Val Loss: 1.7102, Val Acc: 0.9250\n",
            "Epoch [2517/4000], Train Loss: 1.7107, Train Acc: 0.9277, Val Loss: 1.7094, Val Acc: 0.9308\n",
            "Epoch [2518/4000], Train Loss: 1.7093, Train Acc: 0.9293, Val Loss: 1.7116, Val Acc: 0.9282\n",
            "Epoch [2519/4000], Train Loss: 1.7098, Train Acc: 0.9296, Val Loss: 1.7129, Val Acc: 0.9288\n",
            "Epoch [2520/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7134, Val Acc: 0.9256\n",
            "Epoch [2521/4000], Train Loss: 1.7104, Train Acc: 0.9285, Val Loss: 1.7164, Val Acc: 0.9263\n",
            "Epoch [2522/4000], Train Loss: 1.7107, Train Acc: 0.9279, Val Loss: 1.7097, Val Acc: 0.9314\n",
            "Epoch [2523/4000], Train Loss: 1.7122, Train Acc: 0.9282, Val Loss: 1.7128, Val Acc: 0.9256\n",
            "Epoch [2524/4000], Train Loss: 1.7098, Train Acc: 0.9295, Val Loss: 1.7078, Val Acc: 0.9333\n",
            "Epoch [2525/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7199, Val Acc: 0.9263\n",
            "Epoch [2526/4000], Train Loss: 1.7096, Train Acc: 0.9292, Val Loss: 1.7064, Val Acc: 0.9321\n",
            "Epoch [2527/4000], Train Loss: 1.7100, Train Acc: 0.9304, Val Loss: 1.7093, Val Acc: 0.9301\n",
            "Epoch [2528/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7107, Val Acc: 0.9282\n",
            "Epoch [2529/4000], Train Loss: 1.7140, Train Acc: 0.9255, Val Loss: 1.7111, Val Acc: 0.9295\n",
            "Epoch [2530/4000], Train Loss: 1.7111, Train Acc: 0.9287, Val Loss: 1.7117, Val Acc: 0.9308\n",
            "Epoch [2531/4000], Train Loss: 1.7110, Train Acc: 0.9288, Val Loss: 1.7084, Val Acc: 0.9333\n",
            "Epoch [2532/4000], Train Loss: 1.7089, Train Acc: 0.9301, Val Loss: 1.7117, Val Acc: 0.9250\n",
            "Epoch [2533/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7092, Val Acc: 0.9340\n",
            "Epoch [2534/4000], Train Loss: 1.7105, Train Acc: 0.9288, Val Loss: 1.7077, Val Acc: 0.9333\n",
            "Epoch [2535/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7074, Val Acc: 0.9308\n",
            "Epoch [2536/4000], Train Loss: 1.7114, Train Acc: 0.9292, Val Loss: 1.7113, Val Acc: 0.9288\n",
            "Epoch [2537/4000], Train Loss: 1.7108, Train Acc: 0.9279, Val Loss: 1.7099, Val Acc: 0.9308\n",
            "Epoch [2538/4000], Train Loss: 1.7100, Train Acc: 0.9300, Val Loss: 1.7110, Val Acc: 0.9256\n",
            "Epoch [2539/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7093, Val Acc: 0.9327\n",
            "Epoch [2540/4000], Train Loss: 1.7092, Train Acc: 0.9314, Val Loss: 1.7096, Val Acc: 0.9301\n",
            "Epoch [2541/4000], Train Loss: 1.7125, Train Acc: 0.9274, Val Loss: 1.7089, Val Acc: 0.9321\n",
            "Epoch [2542/4000], Train Loss: 1.7094, Train Acc: 0.9290, Val Loss: 1.7089, Val Acc: 0.9327\n",
            "Epoch [2543/4000], Train Loss: 1.7112, Train Acc: 0.9290, Val Loss: 1.7155, Val Acc: 0.9256\n",
            "Epoch [2544/4000], Train Loss: 1.7102, Train Acc: 0.9300, Val Loss: 1.7091, Val Acc: 0.9288\n",
            "Epoch [2545/4000], Train Loss: 1.7104, Train Acc: 0.9295, Val Loss: 1.7090, Val Acc: 0.9333\n",
            "Epoch [2546/4000], Train Loss: 1.7107, Train Acc: 0.9288, Val Loss: 1.7065, Val Acc: 0.9327\n",
            "Epoch [2547/4000], Train Loss: 1.7119, Train Acc: 0.9293, Val Loss: 1.7119, Val Acc: 0.9231\n",
            "Epoch [2548/4000], Train Loss: 1.7107, Train Acc: 0.9287, Val Loss: 1.7095, Val Acc: 0.9295\n",
            "Epoch [2549/4000], Train Loss: 1.7099, Train Acc: 0.9288, Val Loss: 1.7096, Val Acc: 0.9301\n",
            "Epoch [2550/4000], Train Loss: 1.7102, Train Acc: 0.9282, Val Loss: 1.7080, Val Acc: 0.9321\n",
            "Epoch [2551/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7072, Val Acc: 0.9321\n",
            "Epoch [2552/4000], Train Loss: 1.7098, Train Acc: 0.9298, Val Loss: 1.7085, Val Acc: 0.9301\n",
            "Epoch [2553/4000], Train Loss: 1.7105, Train Acc: 0.9300, Val Loss: 1.7093, Val Acc: 0.9288\n",
            "Epoch [2554/4000], Train Loss: 1.7113, Train Acc: 0.9279, Val Loss: 1.7103, Val Acc: 0.9295\n",
            "Epoch [2555/4000], Train Loss: 1.7091, Train Acc: 0.9298, Val Loss: 1.7154, Val Acc: 0.9263\n",
            "Epoch [2556/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7102, Val Acc: 0.9321\n",
            "Epoch [2557/4000], Train Loss: 1.7090, Train Acc: 0.9293, Val Loss: 1.7111, Val Acc: 0.9295\n",
            "Epoch [2558/4000], Train Loss: 1.7102, Train Acc: 0.9311, Val Loss: 1.7131, Val Acc: 0.9327\n",
            "Epoch [2559/4000], Train Loss: 1.7095, Train Acc: 0.9280, Val Loss: 1.7103, Val Acc: 0.9288\n",
            "Epoch [2560/4000], Train Loss: 1.7115, Train Acc: 0.9282, Val Loss: 1.7070, Val Acc: 0.9333\n",
            "Epoch [2561/4000], Train Loss: 1.7093, Train Acc: 0.9290, Val Loss: 1.7065, Val Acc: 0.9333\n",
            "Epoch [2562/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7110, Val Acc: 0.9282\n",
            "Epoch [2563/4000], Train Loss: 1.7111, Train Acc: 0.9300, Val Loss: 1.7178, Val Acc: 0.9231\n",
            "Epoch [2564/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7116, Val Acc: 0.9288\n",
            "Epoch [2565/4000], Train Loss: 1.7101, Train Acc: 0.9301, Val Loss: 1.7100, Val Acc: 0.9321\n",
            "Epoch [2566/4000], Train Loss: 1.7107, Train Acc: 0.9293, Val Loss: 1.7063, Val Acc: 0.9321\n",
            "Epoch [2567/4000], Train Loss: 1.7118, Train Acc: 0.9269, Val Loss: 1.7104, Val Acc: 0.9333\n",
            "Epoch [2568/4000], Train Loss: 1.7103, Train Acc: 0.9303, Val Loss: 1.7096, Val Acc: 0.9321\n",
            "Epoch [2569/4000], Train Loss: 1.7111, Train Acc: 0.9282, Val Loss: 1.7145, Val Acc: 0.9276\n",
            "Epoch [2570/4000], Train Loss: 1.7098, Train Acc: 0.9292, Val Loss: 1.7158, Val Acc: 0.9269\n",
            "Epoch [2571/4000], Train Loss: 1.7100, Train Acc: 0.9285, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [2572/4000], Train Loss: 1.7115, Train Acc: 0.9263, Val Loss: 1.7111, Val Acc: 0.9301\n",
            "Epoch [2573/4000], Train Loss: 1.7110, Train Acc: 0.9284, Val Loss: 1.7067, Val Acc: 0.9333\n",
            "Epoch [2574/4000], Train Loss: 1.7110, Train Acc: 0.9288, Val Loss: 1.7084, Val Acc: 0.9301\n",
            "Epoch [2575/4000], Train Loss: 1.7120, Train Acc: 0.9260, Val Loss: 1.7112, Val Acc: 0.9321\n",
            "Epoch [2576/4000], Train Loss: 1.7092, Train Acc: 0.9295, Val Loss: 1.7130, Val Acc: 0.9295\n",
            "Epoch [2577/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7143, Val Acc: 0.9295\n",
            "Epoch [2578/4000], Train Loss: 1.7107, Train Acc: 0.9290, Val Loss: 1.7113, Val Acc: 0.9295\n",
            "Epoch [2579/4000], Train Loss: 1.7096, Train Acc: 0.9288, Val Loss: 1.7108, Val Acc: 0.9288\n",
            "Epoch [2580/4000], Train Loss: 1.7122, Train Acc: 0.9276, Val Loss: 1.7072, Val Acc: 0.9301\n",
            "Epoch [2581/4000], Train Loss: 1.7093, Train Acc: 0.9287, Val Loss: 1.7075, Val Acc: 0.9308\n",
            "Epoch [2582/4000], Train Loss: 1.7101, Train Acc: 0.9271, Val Loss: 1.7056, Val Acc: 0.9346\n",
            "Epoch [2583/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7122, Val Acc: 0.9256\n",
            "Epoch [2584/4000], Train Loss: 1.7116, Train Acc: 0.9274, Val Loss: 1.7083, Val Acc: 0.9295\n",
            "Epoch [2585/4000], Train Loss: 1.7109, Train Acc: 0.9280, Val Loss: 1.7086, Val Acc: 0.9282\n",
            "Epoch [2586/4000], Train Loss: 1.7096, Train Acc: 0.9295, Val Loss: 1.7079, Val Acc: 0.9333\n",
            "Epoch [2587/4000], Train Loss: 1.7110, Train Acc: 0.9285, Val Loss: 1.7108, Val Acc: 0.9276\n",
            "Epoch [2588/4000], Train Loss: 1.7103, Train Acc: 0.9288, Val Loss: 1.7147, Val Acc: 0.9295\n",
            "Epoch [2589/4000], Train Loss: 1.7103, Train Acc: 0.9293, Val Loss: 1.7073, Val Acc: 0.9327\n",
            "Epoch [2590/4000], Train Loss: 1.7115, Train Acc: 0.9279, Val Loss: 1.7185, Val Acc: 0.9231\n",
            "Epoch [2591/4000], Train Loss: 1.7127, Train Acc: 0.9277, Val Loss: 1.7094, Val Acc: 0.9282\n",
            "Epoch [2592/4000], Train Loss: 1.7110, Train Acc: 0.9290, Val Loss: 1.7074, Val Acc: 0.9346\n",
            "Epoch [2593/4000], Train Loss: 1.7100, Train Acc: 0.9282, Val Loss: 1.7118, Val Acc: 0.9301\n",
            "Epoch [2594/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7131, Val Acc: 0.9256\n",
            "Epoch [2595/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7066, Val Acc: 0.9295\n",
            "Epoch [2596/4000], Train Loss: 1.7123, Train Acc: 0.9263, Val Loss: 1.7151, Val Acc: 0.9295\n",
            "Epoch [2597/4000], Train Loss: 1.7100, Train Acc: 0.9295, Val Loss: 1.7104, Val Acc: 0.9321\n",
            "Epoch [2598/4000], Train Loss: 1.7106, Train Acc: 0.9295, Val Loss: 1.7069, Val Acc: 0.9314\n",
            "Epoch [2599/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7071, Val Acc: 0.9321\n",
            "Epoch [2600/4000], Train Loss: 1.7089, Train Acc: 0.9308, Val Loss: 1.7106, Val Acc: 0.9314\n",
            "Epoch [2601/4000], Train Loss: 1.7128, Train Acc: 0.9268, Val Loss: 1.7126, Val Acc: 0.9276\n",
            "Epoch [2602/4000], Train Loss: 1.7113, Train Acc: 0.9279, Val Loss: 1.7135, Val Acc: 0.9263\n",
            "Epoch [2603/4000], Train Loss: 1.7099, Train Acc: 0.9282, Val Loss: 1.7100, Val Acc: 0.9308\n",
            "Epoch [2604/4000], Train Loss: 1.7096, Train Acc: 0.9306, Val Loss: 1.7165, Val Acc: 0.9244\n",
            "Epoch [2605/4000], Train Loss: 1.7103, Train Acc: 0.9293, Val Loss: 1.7106, Val Acc: 0.9288\n",
            "Epoch [2606/4000], Train Loss: 1.7104, Train Acc: 0.9303, Val Loss: 1.7109, Val Acc: 0.9276\n",
            "Epoch [2607/4000], Train Loss: 1.7100, Train Acc: 0.9282, Val Loss: 1.7130, Val Acc: 0.9340\n",
            "Epoch [2608/4000], Train Loss: 1.7109, Train Acc: 0.9290, Val Loss: 1.7104, Val Acc: 0.9314\n",
            "Epoch [2609/4000], Train Loss: 1.7113, Train Acc: 0.9293, Val Loss: 1.7093, Val Acc: 0.9263\n",
            "Epoch [2610/4000], Train Loss: 1.7093, Train Acc: 0.9285, Val Loss: 1.7092, Val Acc: 0.9263\n",
            "Epoch [2611/4000], Train Loss: 1.7123, Train Acc: 0.9288, Val Loss: 1.7129, Val Acc: 0.9256\n",
            "Epoch [2612/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7105, Val Acc: 0.9314\n",
            "Epoch [2613/4000], Train Loss: 1.7092, Train Acc: 0.9300, Val Loss: 1.7136, Val Acc: 0.9205\n",
            "Epoch [2614/4000], Train Loss: 1.7100, Train Acc: 0.9290, Val Loss: 1.7093, Val Acc: 0.9308\n",
            "Epoch [2615/4000], Train Loss: 1.7106, Train Acc: 0.9293, Val Loss: 1.7073, Val Acc: 0.9314\n",
            "Epoch [2616/4000], Train Loss: 1.7100, Train Acc: 0.9292, Val Loss: 1.7118, Val Acc: 0.9295\n",
            "Epoch [2617/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7092, Val Acc: 0.9276\n",
            "Epoch [2618/4000], Train Loss: 1.7095, Train Acc: 0.9304, Val Loss: 1.7098, Val Acc: 0.9327\n",
            "Epoch [2619/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7063, Val Acc: 0.9340\n",
            "Epoch [2620/4000], Train Loss: 1.7110, Train Acc: 0.9276, Val Loss: 1.7102, Val Acc: 0.9314\n",
            "Epoch [2621/4000], Train Loss: 1.7102, Train Acc: 0.9303, Val Loss: 1.7096, Val Acc: 0.9288\n",
            "Epoch [2622/4000], Train Loss: 1.7109, Train Acc: 0.9293, Val Loss: 1.7087, Val Acc: 0.9321\n",
            "Epoch [2623/4000], Train Loss: 1.7109, Train Acc: 0.9287, Val Loss: 1.7082, Val Acc: 0.9295\n",
            "Epoch [2624/4000], Train Loss: 1.7119, Train Acc: 0.9271, Val Loss: 1.7106, Val Acc: 0.9295\n",
            "Epoch [2625/4000], Train Loss: 1.7104, Train Acc: 0.9285, Val Loss: 1.7053, Val Acc: 0.9346\n",
            "Epoch [2626/4000], Train Loss: 1.7090, Train Acc: 0.9311, Val Loss: 1.7076, Val Acc: 0.9321\n",
            "Epoch [2627/4000], Train Loss: 1.7107, Train Acc: 0.9271, Val Loss: 1.7121, Val Acc: 0.9295\n",
            "Epoch [2628/4000], Train Loss: 1.7109, Train Acc: 0.9287, Val Loss: 1.7117, Val Acc: 0.9288\n",
            "Epoch [2629/4000], Train Loss: 1.7096, Train Acc: 0.9298, Val Loss: 1.7076, Val Acc: 0.9327\n",
            "Epoch [2630/4000], Train Loss: 1.7121, Train Acc: 0.9261, Val Loss: 1.7119, Val Acc: 0.9276\n",
            "Epoch [2631/4000], Train Loss: 1.7120, Train Acc: 0.9298, Val Loss: 1.7126, Val Acc: 0.9276\n",
            "Epoch [2632/4000], Train Loss: 1.7114, Train Acc: 0.9279, Val Loss: 1.7072, Val Acc: 0.9314\n",
            "Epoch [2633/4000], Train Loss: 1.7101, Train Acc: 0.9293, Val Loss: 1.7107, Val Acc: 0.9301\n",
            "Epoch [2634/4000], Train Loss: 1.7099, Train Acc: 0.9284, Val Loss: 1.7143, Val Acc: 0.9269\n",
            "Epoch [2635/4000], Train Loss: 1.7095, Train Acc: 0.9296, Val Loss: 1.7092, Val Acc: 0.9301\n",
            "Epoch [2636/4000], Train Loss: 1.7102, Train Acc: 0.9282, Val Loss: 1.7092, Val Acc: 0.9301\n",
            "Epoch [2637/4000], Train Loss: 1.7119, Train Acc: 0.9271, Val Loss: 1.7080, Val Acc: 0.9321\n",
            "Epoch [2638/4000], Train Loss: 1.7114, Train Acc: 0.9280, Val Loss: 1.7083, Val Acc: 0.9327\n",
            "Epoch [2639/4000], Train Loss: 1.7104, Train Acc: 0.9277, Val Loss: 1.7119, Val Acc: 0.9250\n",
            "Epoch [2640/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7109, Val Acc: 0.9288\n",
            "Epoch [2641/4000], Train Loss: 1.7111, Train Acc: 0.9279, Val Loss: 1.7114, Val Acc: 0.9301\n",
            "Epoch [2642/4000], Train Loss: 1.7117, Train Acc: 0.9266, Val Loss: 1.7066, Val Acc: 0.9321\n",
            "Epoch [2643/4000], Train Loss: 1.7126, Train Acc: 0.9268, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [2644/4000], Train Loss: 1.7131, Train Acc: 0.9274, Val Loss: 1.7081, Val Acc: 0.9340\n",
            "Epoch [2645/4000], Train Loss: 1.7103, Train Acc: 0.9293, Val Loss: 1.7078, Val Acc: 0.9308\n",
            "Epoch [2646/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7165, Val Acc: 0.9250\n",
            "Epoch [2647/4000], Train Loss: 1.7100, Train Acc: 0.9288, Val Loss: 1.7099, Val Acc: 0.9321\n",
            "Epoch [2648/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7102, Val Acc: 0.9295\n",
            "Epoch [2649/4000], Train Loss: 1.7102, Train Acc: 0.9295, Val Loss: 1.7048, Val Acc: 0.9301\n",
            "Epoch [2650/4000], Train Loss: 1.7101, Train Acc: 0.9303, Val Loss: 1.7179, Val Acc: 0.9179\n",
            "Epoch [2651/4000], Train Loss: 1.7121, Train Acc: 0.9269, Val Loss: 1.7169, Val Acc: 0.9256\n",
            "Epoch [2652/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7076, Val Acc: 0.9282\n",
            "Epoch [2653/4000], Train Loss: 1.7118, Train Acc: 0.9293, Val Loss: 1.7083, Val Acc: 0.9301\n",
            "Epoch [2654/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7122, Val Acc: 0.9250\n",
            "Epoch [2655/4000], Train Loss: 1.7082, Train Acc: 0.9300, Val Loss: 1.7128, Val Acc: 0.9256\n",
            "Epoch [2656/4000], Train Loss: 1.7125, Train Acc: 0.9274, Val Loss: 1.7070, Val Acc: 0.9314\n",
            "Epoch [2657/4000], Train Loss: 1.7106, Train Acc: 0.9279, Val Loss: 1.7091, Val Acc: 0.9269\n",
            "Epoch [2658/4000], Train Loss: 1.7103, Train Acc: 0.9285, Val Loss: 1.7090, Val Acc: 0.9295\n",
            "Epoch [2659/4000], Train Loss: 1.7099, Train Acc: 0.9300, Val Loss: 1.7105, Val Acc: 0.9314\n",
            "Epoch [2660/4000], Train Loss: 1.7085, Train Acc: 0.9301, Val Loss: 1.7065, Val Acc: 0.9288\n",
            "Epoch [2661/4000], Train Loss: 1.7111, Train Acc: 0.9276, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [2662/4000], Train Loss: 1.7100, Train Acc: 0.9296, Val Loss: 1.7098, Val Acc: 0.9269\n",
            "Epoch [2663/4000], Train Loss: 1.7104, Train Acc: 0.9296, Val Loss: 1.7150, Val Acc: 0.9244\n",
            "Epoch [2664/4000], Train Loss: 1.7093, Train Acc: 0.9292, Val Loss: 1.7081, Val Acc: 0.9327\n",
            "Epoch [2665/4000], Train Loss: 1.7102, Train Acc: 0.9290, Val Loss: 1.7108, Val Acc: 0.9301\n",
            "Epoch [2666/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7074, Val Acc: 0.9327\n",
            "Epoch [2667/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7067, Val Acc: 0.9327\n",
            "Epoch [2668/4000], Train Loss: 1.7112, Train Acc: 0.9285, Val Loss: 1.7080, Val Acc: 0.9321\n",
            "Epoch [2669/4000], Train Loss: 1.7087, Train Acc: 0.9306, Val Loss: 1.7119, Val Acc: 0.9301\n",
            "Epoch [2670/4000], Train Loss: 1.7129, Train Acc: 0.9268, Val Loss: 1.7127, Val Acc: 0.9269\n",
            "Epoch [2671/4000], Train Loss: 1.7120, Train Acc: 0.9264, Val Loss: 1.7129, Val Acc: 0.9244\n",
            "Epoch [2672/4000], Train Loss: 1.7103, Train Acc: 0.9276, Val Loss: 1.7104, Val Acc: 0.9314\n",
            "Epoch [2673/4000], Train Loss: 1.7102, Train Acc: 0.9268, Val Loss: 1.7090, Val Acc: 0.9321\n",
            "Epoch [2674/4000], Train Loss: 1.7100, Train Acc: 0.9284, Val Loss: 1.7104, Val Acc: 0.9308\n",
            "Epoch [2675/4000], Train Loss: 1.7090, Train Acc: 0.9296, Val Loss: 1.7089, Val Acc: 0.9308\n",
            "Epoch [2676/4000], Train Loss: 1.7104, Train Acc: 0.9288, Val Loss: 1.7057, Val Acc: 0.9321\n",
            "Epoch [2677/4000], Train Loss: 1.7103, Train Acc: 0.9288, Val Loss: 1.7052, Val Acc: 0.9327\n",
            "Epoch [2678/4000], Train Loss: 1.7089, Train Acc: 0.9287, Val Loss: 1.7078, Val Acc: 0.9333\n",
            "Epoch [2679/4000], Train Loss: 1.7097, Train Acc: 0.9298, Val Loss: 1.7151, Val Acc: 0.9295\n",
            "Epoch [2680/4000], Train Loss: 1.7102, Train Acc: 0.9298, Val Loss: 1.7149, Val Acc: 0.9256\n",
            "Epoch [2681/4000], Train Loss: 1.7120, Train Acc: 0.9272, Val Loss: 1.7093, Val Acc: 0.9269\n",
            "Epoch [2682/4000], Train Loss: 1.7107, Train Acc: 0.9269, Val Loss: 1.7164, Val Acc: 0.9333\n",
            "Epoch [2683/4000], Train Loss: 1.7109, Train Acc: 0.9277, Val Loss: 1.7088, Val Acc: 0.9321\n",
            "Epoch [2684/4000], Train Loss: 1.7102, Train Acc: 0.9288, Val Loss: 1.7219, Val Acc: 0.9237\n",
            "Epoch [2685/4000], Train Loss: 1.7114, Train Acc: 0.9285, Val Loss: 1.7083, Val Acc: 0.9308\n",
            "Epoch [2686/4000], Train Loss: 1.7102, Train Acc: 0.9309, Val Loss: 1.7084, Val Acc: 0.9321\n",
            "Epoch [2687/4000], Train Loss: 1.7092, Train Acc: 0.9303, Val Loss: 1.7095, Val Acc: 0.9333\n",
            "Epoch [2688/4000], Train Loss: 1.7094, Train Acc: 0.9295, Val Loss: 1.7083, Val Acc: 0.9333\n",
            "Epoch [2689/4000], Train Loss: 1.7093, Train Acc: 0.9301, Val Loss: 1.7115, Val Acc: 0.9282\n",
            "Epoch [2690/4000], Train Loss: 1.7100, Train Acc: 0.9288, Val Loss: 1.7088, Val Acc: 0.9282\n",
            "Epoch [2691/4000], Train Loss: 1.7126, Train Acc: 0.9266, Val Loss: 1.7086, Val Acc: 0.9314\n",
            "Epoch [2692/4000], Train Loss: 1.7105, Train Acc: 0.9298, Val Loss: 1.7189, Val Acc: 0.9212\n",
            "Epoch [2693/4000], Train Loss: 1.7109, Train Acc: 0.9288, Val Loss: 1.7144, Val Acc: 0.9250\n",
            "Epoch [2694/4000], Train Loss: 1.7121, Train Acc: 0.9296, Val Loss: 1.7066, Val Acc: 0.9327\n",
            "Epoch [2695/4000], Train Loss: 1.7093, Train Acc: 0.9298, Val Loss: 1.7139, Val Acc: 0.9288\n",
            "Epoch [2696/4000], Train Loss: 1.7097, Train Acc: 0.9287, Val Loss: 1.7108, Val Acc: 0.9308\n",
            "Epoch [2697/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7076, Val Acc: 0.9327\n",
            "Epoch [2698/4000], Train Loss: 1.7087, Train Acc: 0.9303, Val Loss: 1.7095, Val Acc: 0.9314\n",
            "Epoch [2699/4000], Train Loss: 1.7096, Train Acc: 0.9304, Val Loss: 1.7117, Val Acc: 0.9263\n",
            "Epoch [2700/4000], Train Loss: 1.7117, Train Acc: 0.9280, Val Loss: 1.7123, Val Acc: 0.9256\n",
            "Epoch [2701/4000], Train Loss: 1.7106, Train Acc: 0.9290, Val Loss: 1.7120, Val Acc: 0.9301\n",
            "Epoch [2702/4000], Train Loss: 1.7114, Train Acc: 0.9284, Val Loss: 1.7088, Val Acc: 0.9314\n",
            "Epoch [2703/4000], Train Loss: 1.7111, Train Acc: 0.9290, Val Loss: 1.7143, Val Acc: 0.9269\n",
            "Epoch [2704/4000], Train Loss: 1.7112, Train Acc: 0.9277, Val Loss: 1.7129, Val Acc: 0.9263\n",
            "Epoch [2705/4000], Train Loss: 1.7108, Train Acc: 0.9300, Val Loss: 1.7093, Val Acc: 0.9308\n",
            "Epoch [2706/4000], Train Loss: 1.7121, Train Acc: 0.9288, Val Loss: 1.7142, Val Acc: 0.9295\n",
            "Epoch [2707/4000], Train Loss: 1.7100, Train Acc: 0.9301, Val Loss: 1.7145, Val Acc: 0.9263\n",
            "Epoch [2708/4000], Train Loss: 1.7093, Train Acc: 0.9300, Val Loss: 1.7077, Val Acc: 0.9282\n",
            "Epoch [2709/4000], Train Loss: 1.7107, Train Acc: 0.9280, Val Loss: 1.7162, Val Acc: 0.9256\n",
            "Epoch [2710/4000], Train Loss: 1.7105, Train Acc: 0.9285, Val Loss: 1.7096, Val Acc: 0.9314\n",
            "Epoch [2711/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7106, Val Acc: 0.9282\n",
            "Epoch [2712/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7129, Val Acc: 0.9288\n",
            "Epoch [2713/4000], Train Loss: 1.7102, Train Acc: 0.9295, Val Loss: 1.7081, Val Acc: 0.9282\n",
            "Epoch [2714/4000], Train Loss: 1.7110, Train Acc: 0.9272, Val Loss: 1.7096, Val Acc: 0.9288\n",
            "Epoch [2715/4000], Train Loss: 1.7095, Train Acc: 0.9285, Val Loss: 1.7063, Val Acc: 0.9321\n",
            "Epoch [2716/4000], Train Loss: 1.7107, Train Acc: 0.9287, Val Loss: 1.7110, Val Acc: 0.9276\n",
            "Epoch [2717/4000], Train Loss: 1.7097, Train Acc: 0.9293, Val Loss: 1.7105, Val Acc: 0.9301\n",
            "Epoch [2718/4000], Train Loss: 1.7088, Train Acc: 0.9298, Val Loss: 1.7110, Val Acc: 0.9301\n",
            "Epoch [2719/4000], Train Loss: 1.7106, Train Acc: 0.9285, Val Loss: 1.7138, Val Acc: 0.9282\n",
            "Epoch [2720/4000], Train Loss: 1.7123, Train Acc: 0.9282, Val Loss: 1.7079, Val Acc: 0.9321\n",
            "Epoch [2721/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7133, Val Acc: 0.9269\n",
            "Epoch [2722/4000], Train Loss: 1.7109, Train Acc: 0.9277, Val Loss: 1.7117, Val Acc: 0.9244\n",
            "Epoch [2723/4000], Train Loss: 1.7092, Train Acc: 0.9301, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [2724/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7068, Val Acc: 0.9314\n",
            "Epoch [2725/4000], Train Loss: 1.7104, Train Acc: 0.9285, Val Loss: 1.7109, Val Acc: 0.9314\n",
            "Epoch [2726/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7074, Val Acc: 0.9301\n",
            "Epoch [2727/4000], Train Loss: 1.7122, Train Acc: 0.9264, Val Loss: 1.7098, Val Acc: 0.9269\n",
            "Epoch [2728/4000], Train Loss: 1.7103, Train Acc: 0.9288, Val Loss: 1.7106, Val Acc: 0.9288\n",
            "Epoch [2729/4000], Train Loss: 1.7102, Train Acc: 0.9287, Val Loss: 1.7081, Val Acc: 0.9327\n",
            "Epoch [2730/4000], Train Loss: 1.7092, Train Acc: 0.9300, Val Loss: 1.7105, Val Acc: 0.9295\n",
            "Epoch [2731/4000], Train Loss: 1.7114, Train Acc: 0.9285, Val Loss: 1.7145, Val Acc: 0.9256\n",
            "Epoch [2732/4000], Train Loss: 1.7096, Train Acc: 0.9306, Val Loss: 1.7126, Val Acc: 0.9237\n",
            "Epoch [2733/4000], Train Loss: 1.7111, Train Acc: 0.9292, Val Loss: 1.7110, Val Acc: 0.9282\n",
            "Epoch [2734/4000], Train Loss: 1.7100, Train Acc: 0.9290, Val Loss: 1.7079, Val Acc: 0.9301\n",
            "Epoch [2735/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7118, Val Acc: 0.9308\n",
            "Epoch [2736/4000], Train Loss: 1.7126, Train Acc: 0.9268, Val Loss: 1.7075, Val Acc: 0.9327\n",
            "Epoch [2737/4000], Train Loss: 1.7094, Train Acc: 0.9300, Val Loss: 1.7101, Val Acc: 0.9295\n",
            "Epoch [2738/4000], Train Loss: 1.7098, Train Acc: 0.9293, Val Loss: 1.7135, Val Acc: 0.9295\n",
            "Epoch [2739/4000], Train Loss: 1.7113, Train Acc: 0.9282, Val Loss: 1.7119, Val Acc: 0.9295\n",
            "Epoch [2740/4000], Train Loss: 1.7093, Train Acc: 0.9301, Val Loss: 1.7053, Val Acc: 0.9327\n",
            "Epoch [2741/4000], Train Loss: 1.7108, Train Acc: 0.9282, Val Loss: 1.7104, Val Acc: 0.9263\n",
            "Epoch [2742/4000], Train Loss: 1.7122, Train Acc: 0.9274, Val Loss: 1.7160, Val Acc: 0.9250\n",
            "Epoch [2743/4000], Train Loss: 1.7101, Train Acc: 0.9284, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [2744/4000], Train Loss: 1.7110, Train Acc: 0.9293, Val Loss: 1.7127, Val Acc: 0.9282\n",
            "Epoch [2745/4000], Train Loss: 1.7093, Train Acc: 0.9296, Val Loss: 1.7074, Val Acc: 0.9308\n",
            "Epoch [2746/4000], Train Loss: 1.7091, Train Acc: 0.9298, Val Loss: 1.7080, Val Acc: 0.9301\n",
            "Epoch [2747/4000], Train Loss: 1.7090, Train Acc: 0.9306, Val Loss: 1.7060, Val Acc: 0.9308\n",
            "Epoch [2748/4000], Train Loss: 1.7100, Train Acc: 0.9298, Val Loss: 1.7069, Val Acc: 0.9301\n",
            "Epoch [2749/4000], Train Loss: 1.7092, Train Acc: 0.9288, Val Loss: 1.7101, Val Acc: 0.9314\n",
            "Epoch [2750/4000], Train Loss: 1.7128, Train Acc: 0.9274, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [2751/4000], Train Loss: 1.7101, Train Acc: 0.9301, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [2752/4000], Train Loss: 1.7092, Train Acc: 0.9295, Val Loss: 1.7080, Val Acc: 0.9295\n",
            "Epoch [2753/4000], Train Loss: 1.7105, Train Acc: 0.9295, Val Loss: 1.7066, Val Acc: 0.9321\n",
            "Epoch [2754/4000], Train Loss: 1.7093, Train Acc: 0.9293, Val Loss: 1.7087, Val Acc: 0.9314\n",
            "Epoch [2755/4000], Train Loss: 1.7109, Train Acc: 0.9290, Val Loss: 1.7115, Val Acc: 0.9288\n",
            "Epoch [2756/4000], Train Loss: 1.7104, Train Acc: 0.9263, Val Loss: 1.7134, Val Acc: 0.9301\n",
            "Epoch [2757/4000], Train Loss: 1.7119, Train Acc: 0.9269, Val Loss: 1.7088, Val Acc: 0.9321\n",
            "Epoch [2758/4000], Train Loss: 1.7087, Train Acc: 0.9313, Val Loss: 1.7092, Val Acc: 0.9333\n",
            "Epoch [2759/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7078, Val Acc: 0.9308\n",
            "Epoch [2760/4000], Train Loss: 1.7093, Train Acc: 0.9313, Val Loss: 1.7073, Val Acc: 0.9288\n",
            "Epoch [2761/4000], Train Loss: 1.7086, Train Acc: 0.9308, Val Loss: 1.7129, Val Acc: 0.9224\n",
            "Epoch [2762/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7066, Val Acc: 0.9308\n",
            "Epoch [2763/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7079, Val Acc: 0.9327\n",
            "Epoch [2764/4000], Train Loss: 1.7104, Train Acc: 0.9279, Val Loss: 1.7099, Val Acc: 0.9288\n",
            "Epoch [2765/4000], Train Loss: 1.7101, Train Acc: 0.9296, Val Loss: 1.7086, Val Acc: 0.9308\n",
            "Epoch [2766/4000], Train Loss: 1.7103, Train Acc: 0.9284, Val Loss: 1.7084, Val Acc: 0.9314\n",
            "Epoch [2767/4000], Train Loss: 1.7145, Train Acc: 0.9252, Val Loss: 1.7066, Val Acc: 0.9333\n",
            "Epoch [2768/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7132, Val Acc: 0.9282\n",
            "Epoch [2769/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7105, Val Acc: 0.9301\n",
            "Epoch [2770/4000], Train Loss: 1.7109, Train Acc: 0.9301, Val Loss: 1.7112, Val Acc: 0.9295\n",
            "Epoch [2771/4000], Train Loss: 1.7109, Train Acc: 0.9293, Val Loss: 1.7074, Val Acc: 0.9321\n",
            "Epoch [2772/4000], Train Loss: 1.7106, Train Acc: 0.9284, Val Loss: 1.7094, Val Acc: 0.9295\n",
            "Epoch [2773/4000], Train Loss: 1.7093, Train Acc: 0.9285, Val Loss: 1.7074, Val Acc: 0.9333\n",
            "Epoch [2774/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7104, Val Acc: 0.9321\n",
            "Epoch [2775/4000], Train Loss: 1.7106, Train Acc: 0.9293, Val Loss: 1.7159, Val Acc: 0.9224\n",
            "Epoch [2776/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7145, Val Acc: 0.9250\n",
            "Epoch [2777/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [2778/4000], Train Loss: 1.7084, Train Acc: 0.9292, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [2779/4000], Train Loss: 1.7110, Train Acc: 0.9280, Val Loss: 1.7063, Val Acc: 0.9308\n",
            "Epoch [2780/4000], Train Loss: 1.7106, Train Acc: 0.9282, Val Loss: 1.7047, Val Acc: 0.9308\n",
            "Epoch [2781/4000], Train Loss: 1.7109, Train Acc: 0.9288, Val Loss: 1.7165, Val Acc: 0.9250\n",
            "Epoch [2782/4000], Train Loss: 1.7103, Train Acc: 0.9301, Val Loss: 1.7099, Val Acc: 0.9295\n",
            "Epoch [2783/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7174, Val Acc: 0.9263\n",
            "Epoch [2784/4000], Train Loss: 1.7106, Train Acc: 0.9290, Val Loss: 1.7094, Val Acc: 0.9327\n",
            "Epoch [2785/4000], Train Loss: 1.7110, Train Acc: 0.9293, Val Loss: 1.7096, Val Acc: 0.9327\n",
            "Epoch [2786/4000], Train Loss: 1.7101, Train Acc: 0.9285, Val Loss: 1.7067, Val Acc: 0.9301\n",
            "Epoch [2787/4000], Train Loss: 1.7120, Train Acc: 0.9266, Val Loss: 1.7063, Val Acc: 0.9333\n",
            "Epoch [2788/4000], Train Loss: 1.7095, Train Acc: 0.9293, Val Loss: 1.7112, Val Acc: 0.9231\n",
            "Epoch [2789/4000], Train Loss: 1.7110, Train Acc: 0.9290, Val Loss: 1.7080, Val Acc: 0.9295\n",
            "Epoch [2790/4000], Train Loss: 1.7092, Train Acc: 0.9304, Val Loss: 1.7107, Val Acc: 0.9308\n",
            "Epoch [2791/4000], Train Loss: 1.7109, Train Acc: 0.9295, Val Loss: 1.7076, Val Acc: 0.9314\n",
            "Epoch [2792/4000], Train Loss: 1.7108, Train Acc: 0.9271, Val Loss: 1.7094, Val Acc: 0.9314\n",
            "Epoch [2793/4000], Train Loss: 1.7097, Train Acc: 0.9284, Val Loss: 1.7100, Val Acc: 0.9308\n",
            "Epoch [2794/4000], Train Loss: 1.7114, Train Acc: 0.9282, Val Loss: 1.7108, Val Acc: 0.9295\n",
            "Epoch [2795/4000], Train Loss: 1.7112, Train Acc: 0.9290, Val Loss: 1.7101, Val Acc: 0.9276\n",
            "Epoch [2796/4000], Train Loss: 1.7087, Train Acc: 0.9303, Val Loss: 1.7088, Val Acc: 0.9282\n",
            "Epoch [2797/4000], Train Loss: 1.7107, Train Acc: 0.9280, Val Loss: 1.7117, Val Acc: 0.9282\n",
            "Epoch [2798/4000], Train Loss: 1.7108, Train Acc: 0.9285, Val Loss: 1.7087, Val Acc: 0.9314\n",
            "Epoch [2799/4000], Train Loss: 1.7094, Train Acc: 0.9295, Val Loss: 1.7086, Val Acc: 0.9308\n",
            "Epoch [2800/4000], Train Loss: 1.7100, Train Acc: 0.9284, Val Loss: 1.7087, Val Acc: 0.9295\n",
            "Epoch [2801/4000], Train Loss: 1.7107, Train Acc: 0.9292, Val Loss: 1.7058, Val Acc: 0.9321\n",
            "Epoch [2802/4000], Train Loss: 1.7124, Train Acc: 0.9279, Val Loss: 1.7087, Val Acc: 0.9308\n",
            "Epoch [2803/4000], Train Loss: 1.7105, Train Acc: 0.9296, Val Loss: 1.7065, Val Acc: 0.9308\n",
            "Epoch [2804/4000], Train Loss: 1.7097, Train Acc: 0.9282, Val Loss: 1.7104, Val Acc: 0.9321\n",
            "Epoch [2805/4000], Train Loss: 1.7097, Train Acc: 0.9296, Val Loss: 1.7136, Val Acc: 0.9269\n",
            "Epoch [2806/4000], Train Loss: 1.7087, Train Acc: 0.9308, Val Loss: 1.7099, Val Acc: 0.9301\n",
            "Epoch [2807/4000], Train Loss: 1.7092, Train Acc: 0.9303, Val Loss: 1.7168, Val Acc: 0.9231\n",
            "Epoch [2808/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [2809/4000], Train Loss: 1.7094, Train Acc: 0.9309, Val Loss: 1.7133, Val Acc: 0.9244\n",
            "Epoch [2810/4000], Train Loss: 1.7108, Train Acc: 0.9292, Val Loss: 1.7051, Val Acc: 0.9327\n",
            "Epoch [2811/4000], Train Loss: 1.7093, Train Acc: 0.9298, Val Loss: 1.7070, Val Acc: 0.9301\n",
            "Epoch [2812/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7111, Val Acc: 0.9256\n",
            "Epoch [2813/4000], Train Loss: 1.7104, Train Acc: 0.9288, Val Loss: 1.7073, Val Acc: 0.9288\n",
            "Epoch [2814/4000], Train Loss: 1.7103, Train Acc: 0.9296, Val Loss: 1.7158, Val Acc: 0.9256\n",
            "Epoch [2815/4000], Train Loss: 1.7124, Train Acc: 0.9276, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [2816/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7187, Val Acc: 0.9256\n",
            "Epoch [2817/4000], Train Loss: 1.7096, Train Acc: 0.9300, Val Loss: 1.7097, Val Acc: 0.9288\n",
            "Epoch [2818/4000], Train Loss: 1.7098, Train Acc: 0.9293, Val Loss: 1.7121, Val Acc: 0.9250\n",
            "Epoch [2819/4000], Train Loss: 1.7107, Train Acc: 0.9295, Val Loss: 1.7115, Val Acc: 0.9244\n",
            "Epoch [2820/4000], Train Loss: 1.7101, Train Acc: 0.9285, Val Loss: 1.7078, Val Acc: 0.9276\n",
            "Epoch [2821/4000], Train Loss: 1.7108, Train Acc: 0.9279, Val Loss: 1.7092, Val Acc: 0.9295\n",
            "Epoch [2822/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [2823/4000], Train Loss: 1.7095, Train Acc: 0.9284, Val Loss: 1.7081, Val Acc: 0.9314\n",
            "Epoch [2824/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7107, Val Acc: 0.9295\n",
            "Epoch [2825/4000], Train Loss: 1.7103, Train Acc: 0.9303, Val Loss: 1.7072, Val Acc: 0.9308\n",
            "Epoch [2826/4000], Train Loss: 1.7093, Train Acc: 0.9304, Val Loss: 1.7119, Val Acc: 0.9276\n",
            "Epoch [2827/4000], Train Loss: 1.7094, Train Acc: 0.9287, Val Loss: 1.7053, Val Acc: 0.9340\n",
            "Epoch [2828/4000], Train Loss: 1.7091, Train Acc: 0.9295, Val Loss: 1.7064, Val Acc: 0.9314\n",
            "Epoch [2829/4000], Train Loss: 1.7112, Train Acc: 0.9290, Val Loss: 1.7120, Val Acc: 0.9263\n",
            "Epoch [2830/4000], Train Loss: 1.7092, Train Acc: 0.9298, Val Loss: 1.7075, Val Acc: 0.9321\n",
            "Epoch [2831/4000], Train Loss: 1.7088, Train Acc: 0.9301, Val Loss: 1.7079, Val Acc: 0.9301\n",
            "Epoch [2832/4000], Train Loss: 1.7120, Train Acc: 0.9280, Val Loss: 1.7129, Val Acc: 0.9321\n",
            "Epoch [2833/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7105, Val Acc: 0.9269\n",
            "Epoch [2834/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7118, Val Acc: 0.9263\n",
            "Epoch [2835/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7136, Val Acc: 0.9256\n",
            "Epoch [2836/4000], Train Loss: 1.7107, Train Acc: 0.9285, Val Loss: 1.7093, Val Acc: 0.9276\n",
            "Epoch [2837/4000], Train Loss: 1.7105, Train Acc: 0.9296, Val Loss: 1.7074, Val Acc: 0.9308\n",
            "Epoch [2838/4000], Train Loss: 1.7087, Train Acc: 0.9303, Val Loss: 1.7073, Val Acc: 0.9308\n",
            "Epoch [2839/4000], Train Loss: 1.7100, Train Acc: 0.9292, Val Loss: 1.7090, Val Acc: 0.9321\n",
            "Epoch [2840/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7104, Val Acc: 0.9333\n",
            "Epoch [2841/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7096, Val Acc: 0.9295\n",
            "Epoch [2842/4000], Train Loss: 1.7102, Train Acc: 0.9288, Val Loss: 1.7072, Val Acc: 0.9295\n",
            "Epoch [2843/4000], Train Loss: 1.7103, Train Acc: 0.9300, Val Loss: 1.7107, Val Acc: 0.9308\n",
            "Epoch [2844/4000], Train Loss: 1.7106, Train Acc: 0.9284, Val Loss: 1.7092, Val Acc: 0.9327\n",
            "Epoch [2845/4000], Train Loss: 1.7112, Train Acc: 0.9277, Val Loss: 1.7104, Val Acc: 0.9276\n",
            "Epoch [2846/4000], Train Loss: 1.7102, Train Acc: 0.9290, Val Loss: 1.7144, Val Acc: 0.9295\n",
            "Epoch [2847/4000], Train Loss: 1.7098, Train Acc: 0.9300, Val Loss: 1.7219, Val Acc: 0.9205\n",
            "Epoch [2848/4000], Train Loss: 1.7107, Train Acc: 0.9303, Val Loss: 1.7125, Val Acc: 0.9276\n",
            "Epoch [2849/4000], Train Loss: 1.7096, Train Acc: 0.9290, Val Loss: 1.7077, Val Acc: 0.9327\n",
            "Epoch [2850/4000], Train Loss: 1.7100, Train Acc: 0.9288, Val Loss: 1.7114, Val Acc: 0.9256\n",
            "Epoch [2851/4000], Train Loss: 1.7117, Train Acc: 0.9280, Val Loss: 1.7110, Val Acc: 0.9295\n",
            "Epoch [2852/4000], Train Loss: 1.7104, Train Acc: 0.9285, Val Loss: 1.7120, Val Acc: 0.9295\n",
            "Epoch [2853/4000], Train Loss: 1.7111, Train Acc: 0.9279, Val Loss: 1.7205, Val Acc: 0.9244\n",
            "Epoch [2854/4000], Train Loss: 1.7122, Train Acc: 0.9282, Val Loss: 1.7116, Val Acc: 0.9308\n",
            "Epoch [2855/4000], Train Loss: 1.7088, Train Acc: 0.9298, Val Loss: 1.7099, Val Acc: 0.9308\n",
            "Epoch [2856/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7140, Val Acc: 0.9288\n",
            "Epoch [2857/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7067, Val Acc: 0.9333\n",
            "Epoch [2858/4000], Train Loss: 1.7087, Train Acc: 0.9301, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [2859/4000], Train Loss: 1.7104, Train Acc: 0.9288, Val Loss: 1.7109, Val Acc: 0.9244\n",
            "Epoch [2860/4000], Train Loss: 1.7123, Train Acc: 0.9276, Val Loss: 1.7079, Val Acc: 0.9333\n",
            "Epoch [2861/4000], Train Loss: 1.7099, Train Acc: 0.9300, Val Loss: 1.7090, Val Acc: 0.9327\n",
            "Epoch [2862/4000], Train Loss: 1.7092, Train Acc: 0.9292, Val Loss: 1.7082, Val Acc: 0.9308\n",
            "Epoch [2863/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7069, Val Acc: 0.9327\n",
            "Epoch [2864/4000], Train Loss: 1.7090, Train Acc: 0.9298, Val Loss: 1.7098, Val Acc: 0.9340\n",
            "Epoch [2865/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7251, Val Acc: 0.9141\n",
            "Epoch [2866/4000], Train Loss: 1.7124, Train Acc: 0.9280, Val Loss: 1.7100, Val Acc: 0.9314\n",
            "Epoch [2867/4000], Train Loss: 1.7107, Train Acc: 0.9290, Val Loss: 1.7099, Val Acc: 0.9333\n",
            "Epoch [2868/4000], Train Loss: 1.7097, Train Acc: 0.9282, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [2869/4000], Train Loss: 1.7092, Train Acc: 0.9300, Val Loss: 1.7198, Val Acc: 0.9244\n",
            "Epoch [2870/4000], Train Loss: 1.7099, Train Acc: 0.9285, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [2871/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7106, Val Acc: 0.9282\n",
            "Epoch [2872/4000], Train Loss: 1.7101, Train Acc: 0.9276, Val Loss: 1.7106, Val Acc: 0.9295\n",
            "Epoch [2873/4000], Train Loss: 1.7108, Train Acc: 0.9295, Val Loss: 1.7145, Val Acc: 0.9269\n",
            "Epoch [2874/4000], Train Loss: 1.7109, Train Acc: 0.9296, Val Loss: 1.7065, Val Acc: 0.9314\n",
            "Epoch [2875/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7189, Val Acc: 0.9250\n",
            "Epoch [2876/4000], Train Loss: 1.7093, Train Acc: 0.9300, Val Loss: 1.7079, Val Acc: 0.9301\n",
            "Epoch [2877/4000], Train Loss: 1.7091, Train Acc: 0.9319, Val Loss: 1.7095, Val Acc: 0.9295\n",
            "Epoch [2878/4000], Train Loss: 1.7099, Train Acc: 0.9284, Val Loss: 1.7104, Val Acc: 0.9353\n",
            "Epoch [2879/4000], Train Loss: 1.7102, Train Acc: 0.9290, Val Loss: 1.7132, Val Acc: 0.9295\n",
            "Epoch [2880/4000], Train Loss: 1.7095, Train Acc: 0.9300, Val Loss: 1.7080, Val Acc: 0.9308\n",
            "Epoch [2881/4000], Train Loss: 1.7081, Train Acc: 0.9298, Val Loss: 1.7093, Val Acc: 0.9295\n",
            "Epoch [2882/4000], Train Loss: 1.7109, Train Acc: 0.9306, Val Loss: 1.7184, Val Acc: 0.9179\n",
            "Epoch [2883/4000], Train Loss: 1.7112, Train Acc: 0.9277, Val Loss: 1.7121, Val Acc: 0.9256\n",
            "Epoch [2884/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7072, Val Acc: 0.9321\n",
            "Epoch [2885/4000], Train Loss: 1.7111, Train Acc: 0.9284, Val Loss: 1.7111, Val Acc: 0.9256\n",
            "Epoch [2886/4000], Train Loss: 1.7096, Train Acc: 0.9298, Val Loss: 1.7118, Val Acc: 0.9321\n",
            "Epoch [2887/4000], Train Loss: 1.7102, Train Acc: 0.9308, Val Loss: 1.7095, Val Acc: 0.9301\n",
            "Epoch [2888/4000], Train Loss: 1.7085, Train Acc: 0.9303, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [2889/4000], Train Loss: 1.7083, Train Acc: 0.9287, Val Loss: 1.7064, Val Acc: 0.9333\n",
            "Epoch [2890/4000], Train Loss: 1.7104, Train Acc: 0.9295, Val Loss: 1.7087, Val Acc: 0.9308\n",
            "Epoch [2891/4000], Train Loss: 1.7118, Train Acc: 0.9274, Val Loss: 1.7154, Val Acc: 0.9250\n",
            "Epoch [2892/4000], Train Loss: 1.7104, Train Acc: 0.9292, Val Loss: 1.7084, Val Acc: 0.9314\n",
            "Epoch [2893/4000], Train Loss: 1.7093, Train Acc: 0.9308, Val Loss: 1.7109, Val Acc: 0.9301\n",
            "Epoch [2894/4000], Train Loss: 1.7110, Train Acc: 0.9298, Val Loss: 1.7142, Val Acc: 0.9263\n",
            "Epoch [2895/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7197, Val Acc: 0.9186\n",
            "Epoch [2896/4000], Train Loss: 1.7102, Train Acc: 0.9285, Val Loss: 1.7066, Val Acc: 0.9327\n",
            "Epoch [2897/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7103, Val Acc: 0.9333\n",
            "Epoch [2898/4000], Train Loss: 1.7102, Train Acc: 0.9288, Val Loss: 1.7125, Val Acc: 0.9301\n",
            "Epoch [2899/4000], Train Loss: 1.7092, Train Acc: 0.9287, Val Loss: 1.7085, Val Acc: 0.9308\n",
            "Epoch [2900/4000], Train Loss: 1.7099, Train Acc: 0.9285, Val Loss: 1.7113, Val Acc: 0.9282\n",
            "Epoch [2901/4000], Train Loss: 1.7103, Train Acc: 0.9303, Val Loss: 1.7074, Val Acc: 0.9327\n",
            "Epoch [2902/4000], Train Loss: 1.7115, Train Acc: 0.9293, Val Loss: 1.7062, Val Acc: 0.9308\n",
            "Epoch [2903/4000], Train Loss: 1.7084, Train Acc: 0.9308, Val Loss: 1.7111, Val Acc: 0.9276\n",
            "Epoch [2904/4000], Train Loss: 1.7090, Train Acc: 0.9295, Val Loss: 1.7135, Val Acc: 0.9295\n",
            "Epoch [2905/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7125, Val Acc: 0.9314\n",
            "Epoch [2906/4000], Train Loss: 1.7100, Train Acc: 0.9290, Val Loss: 1.7103, Val Acc: 0.9295\n",
            "Epoch [2907/4000], Train Loss: 1.7114, Train Acc: 0.9282, Val Loss: 1.7128, Val Acc: 0.9288\n",
            "Epoch [2908/4000], Train Loss: 1.7099, Train Acc: 0.9280, Val Loss: 1.7094, Val Acc: 0.9295\n",
            "Epoch [2909/4000], Train Loss: 1.7098, Train Acc: 0.9296, Val Loss: 1.7104, Val Acc: 0.9314\n",
            "Epoch [2910/4000], Train Loss: 1.7096, Train Acc: 0.9306, Val Loss: 1.7109, Val Acc: 0.9276\n",
            "Epoch [2911/4000], Train Loss: 1.7107, Train Acc: 0.9287, Val Loss: 1.7052, Val Acc: 0.9321\n",
            "Epoch [2912/4000], Train Loss: 1.7110, Train Acc: 0.9284, Val Loss: 1.7112, Val Acc: 0.9295\n",
            "Epoch [2913/4000], Train Loss: 1.7123, Train Acc: 0.9266, Val Loss: 1.7116, Val Acc: 0.9301\n",
            "Epoch [2914/4000], Train Loss: 1.7120, Train Acc: 0.9295, Val Loss: 1.7114, Val Acc: 0.9269\n",
            "Epoch [2915/4000], Train Loss: 1.7104, Train Acc: 0.9287, Val Loss: 1.7138, Val Acc: 0.9256\n",
            "Epoch [2916/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7101, Val Acc: 0.9263\n",
            "Epoch [2917/4000], Train Loss: 1.7105, Train Acc: 0.9296, Val Loss: 1.7077, Val Acc: 0.9295\n",
            "Epoch [2918/4000], Train Loss: 1.7088, Train Acc: 0.9308, Val Loss: 1.7074, Val Acc: 0.9295\n",
            "Epoch [2919/4000], Train Loss: 1.7080, Train Acc: 0.9300, Val Loss: 1.7116, Val Acc: 0.9282\n",
            "Epoch [2920/4000], Train Loss: 1.7095, Train Acc: 0.9300, Val Loss: 1.7067, Val Acc: 0.9321\n",
            "Epoch [2921/4000], Train Loss: 1.7093, Train Acc: 0.9300, Val Loss: 1.7078, Val Acc: 0.9333\n",
            "Epoch [2922/4000], Train Loss: 1.7095, Train Acc: 0.9296, Val Loss: 1.7061, Val Acc: 0.9321\n",
            "Epoch [2923/4000], Train Loss: 1.7102, Train Acc: 0.9306, Val Loss: 1.7093, Val Acc: 0.9295\n",
            "Epoch [2924/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7143, Val Acc: 0.9282\n",
            "Epoch [2925/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7121, Val Acc: 0.9250\n",
            "Epoch [2926/4000], Train Loss: 1.7102, Train Acc: 0.9290, Val Loss: 1.7075, Val Acc: 0.9327\n",
            "Epoch [2927/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [2928/4000], Train Loss: 1.7088, Train Acc: 0.9295, Val Loss: 1.7125, Val Acc: 0.9301\n",
            "Epoch [2929/4000], Train Loss: 1.7093, Train Acc: 0.9293, Val Loss: 1.7060, Val Acc: 0.9308\n",
            "Epoch [2930/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7121, Val Acc: 0.9237\n",
            "Epoch [2931/4000], Train Loss: 1.7101, Train Acc: 0.9295, Val Loss: 1.7090, Val Acc: 0.9288\n",
            "Epoch [2932/4000], Train Loss: 1.7095, Train Acc: 0.9296, Val Loss: 1.7141, Val Acc: 0.9250\n",
            "Epoch [2933/4000], Train Loss: 1.7100, Train Acc: 0.9295, Val Loss: 1.7103, Val Acc: 0.9282\n",
            "Epoch [2934/4000], Train Loss: 1.7089, Train Acc: 0.9304, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [2935/4000], Train Loss: 1.7103, Train Acc: 0.9272, Val Loss: 1.7084, Val Acc: 0.9301\n",
            "Epoch [2936/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7150, Val Acc: 0.9237\n",
            "Epoch [2937/4000], Train Loss: 1.7098, Train Acc: 0.9285, Val Loss: 1.7090, Val Acc: 0.9321\n",
            "Epoch [2938/4000], Train Loss: 1.7097, Train Acc: 0.9293, Val Loss: 1.7087, Val Acc: 0.9269\n",
            "Epoch [2939/4000], Train Loss: 1.7088, Train Acc: 0.9301, Val Loss: 1.7072, Val Acc: 0.9327\n",
            "Epoch [2940/4000], Train Loss: 1.7108, Train Acc: 0.9288, Val Loss: 1.7101, Val Acc: 0.9327\n",
            "Epoch [2941/4000], Train Loss: 1.7130, Train Acc: 0.9280, Val Loss: 1.7118, Val Acc: 0.9295\n",
            "Epoch [2942/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7120, Val Acc: 0.9288\n",
            "Epoch [2943/4000], Train Loss: 1.7112, Train Acc: 0.9292, Val Loss: 1.7096, Val Acc: 0.9314\n",
            "Epoch [2944/4000], Train Loss: 1.7085, Train Acc: 0.9308, Val Loss: 1.7069, Val Acc: 0.9308\n",
            "Epoch [2945/4000], Train Loss: 1.7089, Train Acc: 0.9288, Val Loss: 1.7088, Val Acc: 0.9288\n",
            "Epoch [2946/4000], Train Loss: 1.7099, Train Acc: 0.9296, Val Loss: 1.7168, Val Acc: 0.9256\n",
            "Epoch [2947/4000], Train Loss: 1.7090, Train Acc: 0.9295, Val Loss: 1.7058, Val Acc: 0.9295\n",
            "Epoch [2948/4000], Train Loss: 1.7105, Train Acc: 0.9280, Val Loss: 1.7081, Val Acc: 0.9327\n",
            "Epoch [2949/4000], Train Loss: 1.7096, Train Acc: 0.9306, Val Loss: 1.7088, Val Acc: 0.9301\n",
            "Epoch [2950/4000], Train Loss: 1.7109, Train Acc: 0.9287, Val Loss: 1.7140, Val Acc: 0.9263\n",
            "Epoch [2951/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7124, Val Acc: 0.9314\n",
            "Epoch [2952/4000], Train Loss: 1.7097, Train Acc: 0.9287, Val Loss: 1.7090, Val Acc: 0.9321\n",
            "Epoch [2953/4000], Train Loss: 1.7116, Train Acc: 0.9269, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [2954/4000], Train Loss: 1.7107, Train Acc: 0.9285, Val Loss: 1.7065, Val Acc: 0.9314\n",
            "Epoch [2955/4000], Train Loss: 1.7092, Train Acc: 0.9304, Val Loss: 1.7103, Val Acc: 0.9263\n",
            "Epoch [2956/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7078, Val Acc: 0.9314\n",
            "Epoch [2957/4000], Train Loss: 1.7094, Train Acc: 0.9298, Val Loss: 1.7086, Val Acc: 0.9282\n",
            "Epoch [2958/4000], Train Loss: 1.7095, Train Acc: 0.9303, Val Loss: 1.7089, Val Acc: 0.9301\n",
            "Epoch [2959/4000], Train Loss: 1.7108, Train Acc: 0.9298, Val Loss: 1.7109, Val Acc: 0.9295\n",
            "Epoch [2960/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7084, Val Acc: 0.9295\n",
            "Epoch [2961/4000], Train Loss: 1.7090, Train Acc: 0.9290, Val Loss: 1.7078, Val Acc: 0.9327\n",
            "Epoch [2962/4000], Train Loss: 1.7090, Train Acc: 0.9296, Val Loss: 1.7108, Val Acc: 0.9314\n",
            "Epoch [2963/4000], Train Loss: 1.7097, Train Acc: 0.9300, Val Loss: 1.7077, Val Acc: 0.9301\n",
            "Epoch [2964/4000], Train Loss: 1.7094, Train Acc: 0.9295, Val Loss: 1.7091, Val Acc: 0.9327\n",
            "Epoch [2965/4000], Train Loss: 1.7085, Train Acc: 0.9303, Val Loss: 1.7211, Val Acc: 0.9205\n",
            "Epoch [2966/4000], Train Loss: 1.7114, Train Acc: 0.9287, Val Loss: 1.7089, Val Acc: 0.9308\n",
            "Epoch [2967/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7158, Val Acc: 0.9276\n",
            "Epoch [2968/4000], Train Loss: 1.7092, Train Acc: 0.9301, Val Loss: 1.7061, Val Acc: 0.9333\n",
            "Epoch [2969/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7126, Val Acc: 0.9308\n",
            "Epoch [2970/4000], Train Loss: 1.7114, Train Acc: 0.9288, Val Loss: 1.7092, Val Acc: 0.9327\n",
            "Epoch [2971/4000], Train Loss: 1.7091, Train Acc: 0.9303, Val Loss: 1.7083, Val Acc: 0.9327\n",
            "Epoch [2972/4000], Train Loss: 1.7091, Train Acc: 0.9303, Val Loss: 1.7076, Val Acc: 0.9327\n",
            "Epoch [2973/4000], Train Loss: 1.7102, Train Acc: 0.9290, Val Loss: 1.7111, Val Acc: 0.9282\n",
            "Epoch [2974/4000], Train Loss: 1.7116, Train Acc: 0.9261, Val Loss: 1.7054, Val Acc: 0.9308\n",
            "Epoch [2975/4000], Train Loss: 1.7087, Train Acc: 0.9309, Val Loss: 1.7081, Val Acc: 0.9333\n",
            "Epoch [2976/4000], Train Loss: 1.7089, Train Acc: 0.9316, Val Loss: 1.7077, Val Acc: 0.9308\n",
            "Epoch [2977/4000], Train Loss: 1.7093, Train Acc: 0.9292, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [2978/4000], Train Loss: 1.7087, Train Acc: 0.9290, Val Loss: 1.7082, Val Acc: 0.9321\n",
            "Epoch [2979/4000], Train Loss: 1.7093, Train Acc: 0.9296, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [2980/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7083, Val Acc: 0.9353\n",
            "Epoch [2981/4000], Train Loss: 1.7102, Train Acc: 0.9282, Val Loss: 1.7061, Val Acc: 0.9314\n",
            "Epoch [2982/4000], Train Loss: 1.7095, Train Acc: 0.9296, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [2983/4000], Train Loss: 1.7098, Train Acc: 0.9298, Val Loss: 1.7087, Val Acc: 0.9327\n",
            "Epoch [2984/4000], Train Loss: 1.7107, Train Acc: 0.9284, Val Loss: 1.7107, Val Acc: 0.9282\n",
            "Epoch [2985/4000], Train Loss: 1.7115, Train Acc: 0.9290, Val Loss: 1.7094, Val Acc: 0.9288\n",
            "Epoch [2986/4000], Train Loss: 1.7097, Train Acc: 0.9306, Val Loss: 1.7070, Val Acc: 0.9327\n",
            "Epoch [2987/4000], Train Loss: 1.7095, Train Acc: 0.9288, Val Loss: 1.7099, Val Acc: 0.9321\n",
            "Epoch [2988/4000], Train Loss: 1.7104, Train Acc: 0.9287, Val Loss: 1.7087, Val Acc: 0.9321\n",
            "Epoch [2989/4000], Train Loss: 1.7115, Train Acc: 0.9279, Val Loss: 1.7143, Val Acc: 0.9282\n",
            "Epoch [2990/4000], Train Loss: 1.7100, Train Acc: 0.9298, Val Loss: 1.7094, Val Acc: 0.9321\n",
            "Epoch [2991/4000], Train Loss: 1.7106, Train Acc: 0.9287, Val Loss: 1.7097, Val Acc: 0.9295\n",
            "Epoch [2992/4000], Train Loss: 1.7097, Train Acc: 0.9295, Val Loss: 1.7061, Val Acc: 0.9321\n",
            "Epoch [2993/4000], Train Loss: 1.7091, Train Acc: 0.9292, Val Loss: 1.7185, Val Acc: 0.9231\n",
            "Epoch [2994/4000], Train Loss: 1.7112, Train Acc: 0.9284, Val Loss: 1.7067, Val Acc: 0.9308\n",
            "Epoch [2995/4000], Train Loss: 1.7103, Train Acc: 0.9279, Val Loss: 1.7116, Val Acc: 0.9295\n",
            "Epoch [2996/4000], Train Loss: 1.7094, Train Acc: 0.9301, Val Loss: 1.7090, Val Acc: 0.9282\n",
            "Epoch [2997/4000], Train Loss: 1.7094, Train Acc: 0.9304, Val Loss: 1.7172, Val Acc: 0.9237\n",
            "Epoch [2998/4000], Train Loss: 1.7115, Train Acc: 0.9295, Val Loss: 1.7115, Val Acc: 0.9308\n",
            "Epoch [2999/4000], Train Loss: 1.7092, Train Acc: 0.9306, Val Loss: 1.7078, Val Acc: 0.9288\n",
            "Epoch [3000/4000], Train Loss: 1.7098, Train Acc: 0.9292, Val Loss: 1.7071, Val Acc: 0.9314\n",
            "Epoch [3001/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7060, Val Acc: 0.9321\n",
            "Epoch [3002/4000], Train Loss: 1.7104, Train Acc: 0.9309, Val Loss: 1.7071, Val Acc: 0.9321\n",
            "Epoch [3003/4000], Train Loss: 1.7084, Train Acc: 0.9304, Val Loss: 1.7101, Val Acc: 0.9288\n",
            "Epoch [3004/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7231, Val Acc: 0.9173\n",
            "Epoch [3005/4000], Train Loss: 1.7103, Train Acc: 0.9293, Val Loss: 1.7088, Val Acc: 0.9327\n",
            "Epoch [3006/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7091, Val Acc: 0.9308\n",
            "Epoch [3007/4000], Train Loss: 1.7121, Train Acc: 0.9290, Val Loss: 1.7108, Val Acc: 0.9295\n",
            "Epoch [3008/4000], Train Loss: 1.7096, Train Acc: 0.9300, Val Loss: 1.7050, Val Acc: 0.9346\n",
            "Epoch [3009/4000], Train Loss: 1.7089, Train Acc: 0.9292, Val Loss: 1.7108, Val Acc: 0.9308\n",
            "Epoch [3010/4000], Train Loss: 1.7106, Train Acc: 0.9293, Val Loss: 1.7170, Val Acc: 0.9269\n",
            "Epoch [3011/4000], Train Loss: 1.7100, Train Acc: 0.9288, Val Loss: 1.7093, Val Acc: 0.9327\n",
            "Epoch [3012/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7087, Val Acc: 0.9308\n",
            "Epoch [3013/4000], Train Loss: 1.7095, Train Acc: 0.9284, Val Loss: 1.7112, Val Acc: 0.9301\n",
            "Epoch [3014/4000], Train Loss: 1.7084, Train Acc: 0.9309, Val Loss: 1.7078, Val Acc: 0.9314\n",
            "Epoch [3015/4000], Train Loss: 1.7094, Train Acc: 0.9303, Val Loss: 1.7110, Val Acc: 0.9340\n",
            "Epoch [3016/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7075, Val Acc: 0.9301\n",
            "Epoch [3017/4000], Train Loss: 1.7109, Train Acc: 0.9287, Val Loss: 1.7070, Val Acc: 0.9327\n",
            "Epoch [3018/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7078, Val Acc: 0.9301\n",
            "Epoch [3019/4000], Train Loss: 1.7109, Train Acc: 0.9285, Val Loss: 1.7094, Val Acc: 0.9327\n",
            "Epoch [3020/4000], Train Loss: 1.7106, Train Acc: 0.9296, Val Loss: 1.7112, Val Acc: 0.9314\n",
            "Epoch [3021/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7112, Val Acc: 0.9301\n",
            "Epoch [3022/4000], Train Loss: 1.7103, Train Acc: 0.9288, Val Loss: 1.7110, Val Acc: 0.9308\n",
            "Epoch [3023/4000], Train Loss: 1.7104, Train Acc: 0.9287, Val Loss: 1.7093, Val Acc: 0.9314\n",
            "Epoch [3024/4000], Train Loss: 1.7096, Train Acc: 0.9280, Val Loss: 1.7130, Val Acc: 0.9288\n",
            "Epoch [3025/4000], Train Loss: 1.7092, Train Acc: 0.9293, Val Loss: 1.7095, Val Acc: 0.9321\n",
            "Epoch [3026/4000], Train Loss: 1.7088, Train Acc: 0.9300, Val Loss: 1.7069, Val Acc: 0.9314\n",
            "Epoch [3027/4000], Train Loss: 1.7102, Train Acc: 0.9292, Val Loss: 1.7120, Val Acc: 0.9244\n",
            "Epoch [3028/4000], Train Loss: 1.7110, Train Acc: 0.9293, Val Loss: 1.7128, Val Acc: 0.9250\n",
            "Epoch [3029/4000], Train Loss: 1.7098, Train Acc: 0.9287, Val Loss: 1.7058, Val Acc: 0.9314\n",
            "Epoch [3030/4000], Train Loss: 1.7101, Train Acc: 0.9295, Val Loss: 1.7075, Val Acc: 0.9308\n",
            "Epoch [3031/4000], Train Loss: 1.7100, Train Acc: 0.9298, Val Loss: 1.7107, Val Acc: 0.9276\n",
            "Epoch [3032/4000], Train Loss: 1.7100, Train Acc: 0.9298, Val Loss: 1.7060, Val Acc: 0.9333\n",
            "Epoch [3033/4000], Train Loss: 1.7113, Train Acc: 0.9292, Val Loss: 1.7129, Val Acc: 0.9269\n",
            "Epoch [3034/4000], Train Loss: 1.7108, Train Acc: 0.9300, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [3035/4000], Train Loss: 1.7095, Train Acc: 0.9300, Val Loss: 1.7119, Val Acc: 0.9301\n",
            "Epoch [3036/4000], Train Loss: 1.7090, Train Acc: 0.9292, Val Loss: 1.7104, Val Acc: 0.9314\n",
            "Epoch [3037/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7169, Val Acc: 0.9186\n",
            "Epoch [3038/4000], Train Loss: 1.7128, Train Acc: 0.9266, Val Loss: 1.7067, Val Acc: 0.9301\n",
            "Epoch [3039/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7067, Val Acc: 0.9314\n",
            "Epoch [3040/4000], Train Loss: 1.7115, Train Acc: 0.9280, Val Loss: 1.7183, Val Acc: 0.9205\n",
            "Epoch [3041/4000], Train Loss: 1.7098, Train Acc: 0.9298, Val Loss: 1.7080, Val Acc: 0.9301\n",
            "Epoch [3042/4000], Train Loss: 1.7089, Train Acc: 0.9304, Val Loss: 1.7188, Val Acc: 0.9212\n",
            "Epoch [3043/4000], Train Loss: 1.7130, Train Acc: 0.9272, Val Loss: 1.7139, Val Acc: 0.9256\n",
            "Epoch [3044/4000], Train Loss: 1.7091, Train Acc: 0.9293, Val Loss: 1.7095, Val Acc: 0.9269\n",
            "Epoch [3045/4000], Train Loss: 1.7095, Train Acc: 0.9287, Val Loss: 1.7064, Val Acc: 0.9308\n",
            "Epoch [3046/4000], Train Loss: 1.7089, Train Acc: 0.9285, Val Loss: 1.7083, Val Acc: 0.9321\n",
            "Epoch [3047/4000], Train Loss: 1.7083, Train Acc: 0.9314, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [3048/4000], Train Loss: 1.7095, Train Acc: 0.9309, Val Loss: 1.7076, Val Acc: 0.9321\n",
            "Epoch [3049/4000], Train Loss: 1.7094, Train Acc: 0.9304, Val Loss: 1.7114, Val Acc: 0.9301\n",
            "Epoch [3050/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7070, Val Acc: 0.9282\n",
            "Epoch [3051/4000], Train Loss: 1.7105, Train Acc: 0.9292, Val Loss: 1.7088, Val Acc: 0.9308\n",
            "Epoch [3052/4000], Train Loss: 1.7117, Train Acc: 0.9276, Val Loss: 1.7176, Val Acc: 0.9237\n",
            "Epoch [3053/4000], Train Loss: 1.7092, Train Acc: 0.9301, Val Loss: 1.7082, Val Acc: 0.9333\n",
            "Epoch [3054/4000], Train Loss: 1.7097, Train Acc: 0.9295, Val Loss: 1.7067, Val Acc: 0.9314\n",
            "Epoch [3055/4000], Train Loss: 1.7105, Train Acc: 0.9295, Val Loss: 1.7078, Val Acc: 0.9314\n",
            "Epoch [3056/4000], Train Loss: 1.7090, Train Acc: 0.9304, Val Loss: 1.7091, Val Acc: 0.9308\n",
            "Epoch [3057/4000], Train Loss: 1.7094, Train Acc: 0.9303, Val Loss: 1.7081, Val Acc: 0.9314\n",
            "Epoch [3058/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7069, Val Acc: 0.9340\n",
            "Epoch [3059/4000], Train Loss: 1.7095, Train Acc: 0.9298, Val Loss: 1.7161, Val Acc: 0.9237\n",
            "Epoch [3060/4000], Train Loss: 1.7109, Train Acc: 0.9287, Val Loss: 1.7049, Val Acc: 0.9327\n",
            "Epoch [3061/4000], Train Loss: 1.7087, Train Acc: 0.9298, Val Loss: 1.7109, Val Acc: 0.9282\n",
            "Epoch [3062/4000], Train Loss: 1.7104, Train Acc: 0.9296, Val Loss: 1.7086, Val Acc: 0.9295\n",
            "Epoch [3063/4000], Train Loss: 1.7111, Train Acc: 0.9292, Val Loss: 1.7114, Val Acc: 0.9301\n",
            "Epoch [3064/4000], Train Loss: 1.7110, Train Acc: 0.9295, Val Loss: 1.7114, Val Acc: 0.9288\n",
            "Epoch [3065/4000], Train Loss: 1.7095, Train Acc: 0.9292, Val Loss: 1.7102, Val Acc: 0.9269\n",
            "Epoch [3066/4000], Train Loss: 1.7095, Train Acc: 0.9292, Val Loss: 1.7078, Val Acc: 0.9327\n",
            "Epoch [3067/4000], Train Loss: 1.7096, Train Acc: 0.9280, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [3068/4000], Train Loss: 1.7096, Train Acc: 0.9284, Val Loss: 1.7106, Val Acc: 0.9295\n",
            "Epoch [3069/4000], Train Loss: 1.7099, Train Acc: 0.9292, Val Loss: 1.7086, Val Acc: 0.9288\n",
            "Epoch [3070/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7091, Val Acc: 0.9288\n",
            "Epoch [3071/4000], Train Loss: 1.7098, Train Acc: 0.9284, Val Loss: 1.7057, Val Acc: 0.9321\n",
            "Epoch [3072/4000], Train Loss: 1.7087, Train Acc: 0.9296, Val Loss: 1.7062, Val Acc: 0.9327\n",
            "Epoch [3073/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7097, Val Acc: 0.9321\n",
            "Epoch [3074/4000], Train Loss: 1.7092, Train Acc: 0.9293, Val Loss: 1.7115, Val Acc: 0.9256\n",
            "Epoch [3075/4000], Train Loss: 1.7097, Train Acc: 0.9295, Val Loss: 1.7123, Val Acc: 0.9244\n",
            "Epoch [3076/4000], Train Loss: 1.7119, Train Acc: 0.9282, Val Loss: 1.7088, Val Acc: 0.9308\n",
            "Epoch [3077/4000], Train Loss: 1.7098, Train Acc: 0.9295, Val Loss: 1.7114, Val Acc: 0.9321\n",
            "Epoch [3078/4000], Train Loss: 1.7101, Train Acc: 0.9279, Val Loss: 1.7087, Val Acc: 0.9288\n",
            "Epoch [3079/4000], Train Loss: 1.7094, Train Acc: 0.9282, Val Loss: 1.7149, Val Acc: 0.9237\n",
            "Epoch [3080/4000], Train Loss: 1.7126, Train Acc: 0.9280, Val Loss: 1.7090, Val Acc: 0.9321\n",
            "Epoch [3081/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7150, Val Acc: 0.9256\n",
            "Epoch [3082/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7081, Val Acc: 0.9333\n",
            "Epoch [3083/4000], Train Loss: 1.7100, Train Acc: 0.9285, Val Loss: 1.7106, Val Acc: 0.9256\n",
            "Epoch [3084/4000], Train Loss: 1.7102, Train Acc: 0.9279, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [3085/4000], Train Loss: 1.7121, Train Acc: 0.9277, Val Loss: 1.7106, Val Acc: 0.9276\n",
            "Epoch [3086/4000], Train Loss: 1.7122, Train Acc: 0.9266, Val Loss: 1.7110, Val Acc: 0.9295\n",
            "Epoch [3087/4000], Train Loss: 1.7082, Train Acc: 0.9304, Val Loss: 1.7066, Val Acc: 0.9314\n",
            "Epoch [3088/4000], Train Loss: 1.7087, Train Acc: 0.9293, Val Loss: 1.7108, Val Acc: 0.9321\n",
            "Epoch [3089/4000], Train Loss: 1.7114, Train Acc: 0.9308, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [3090/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7111, Val Acc: 0.9276\n",
            "Epoch [3091/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [3092/4000], Train Loss: 1.7089, Train Acc: 0.9292, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [3093/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [3094/4000], Train Loss: 1.7088, Train Acc: 0.9300, Val Loss: 1.7076, Val Acc: 0.9295\n",
            "Epoch [3095/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7097, Val Acc: 0.9308\n",
            "Epoch [3096/4000], Train Loss: 1.7107, Train Acc: 0.9287, Val Loss: 1.7101, Val Acc: 0.9327\n",
            "Epoch [3097/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7128, Val Acc: 0.9250\n",
            "Epoch [3098/4000], Train Loss: 1.7096, Train Acc: 0.9279, Val Loss: 1.7093, Val Acc: 0.9321\n",
            "Epoch [3099/4000], Train Loss: 1.7105, Train Acc: 0.9285, Val Loss: 1.7102, Val Acc: 0.9295\n",
            "Epoch [3100/4000], Train Loss: 1.7108, Train Acc: 0.9293, Val Loss: 1.7141, Val Acc: 0.9308\n",
            "Epoch [3101/4000], Train Loss: 1.7115, Train Acc: 0.9292, Val Loss: 1.7091, Val Acc: 0.9276\n",
            "Epoch [3102/4000], Train Loss: 1.7099, Train Acc: 0.9304, Val Loss: 1.7063, Val Acc: 0.9321\n",
            "Epoch [3103/4000], Train Loss: 1.7087, Train Acc: 0.9293, Val Loss: 1.7131, Val Acc: 0.9288\n",
            "Epoch [3104/4000], Train Loss: 1.7118, Train Acc: 0.9272, Val Loss: 1.7095, Val Acc: 0.9321\n",
            "Epoch [3105/4000], Train Loss: 1.7106, Train Acc: 0.9277, Val Loss: 1.7119, Val Acc: 0.9282\n",
            "Epoch [3106/4000], Train Loss: 1.7102, Train Acc: 0.9285, Val Loss: 1.7118, Val Acc: 0.9308\n",
            "Epoch [3107/4000], Train Loss: 1.7099, Train Acc: 0.9284, Val Loss: 1.7073, Val Acc: 0.9308\n",
            "Epoch [3108/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7085, Val Acc: 0.9333\n",
            "Epoch [3109/4000], Train Loss: 1.7095, Train Acc: 0.9300, Val Loss: 1.7098, Val Acc: 0.9314\n",
            "Epoch [3110/4000], Train Loss: 1.7085, Train Acc: 0.9293, Val Loss: 1.7097, Val Acc: 0.9321\n",
            "Epoch [3111/4000], Train Loss: 1.7104, Train Acc: 0.9301, Val Loss: 1.7109, Val Acc: 0.9301\n",
            "Epoch [3112/4000], Train Loss: 1.7093, Train Acc: 0.9313, Val Loss: 1.7118, Val Acc: 0.9282\n",
            "Epoch [3113/4000], Train Loss: 1.7089, Train Acc: 0.9293, Val Loss: 1.7088, Val Acc: 0.9333\n",
            "Epoch [3114/4000], Train Loss: 1.7101, Train Acc: 0.9295, Val Loss: 1.7112, Val Acc: 0.9301\n",
            "Epoch [3115/4000], Train Loss: 1.7097, Train Acc: 0.9287, Val Loss: 1.7104, Val Acc: 0.9288\n",
            "Epoch [3116/4000], Train Loss: 1.7108, Train Acc: 0.9287, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [3117/4000], Train Loss: 1.7100, Train Acc: 0.9301, Val Loss: 1.7192, Val Acc: 0.9237\n",
            "Epoch [3118/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7173, Val Acc: 0.9212\n",
            "Epoch [3119/4000], Train Loss: 1.7110, Train Acc: 0.9280, Val Loss: 1.7126, Val Acc: 0.9282\n",
            "Epoch [3120/4000], Train Loss: 1.7106, Train Acc: 0.9282, Val Loss: 1.7139, Val Acc: 0.9321\n",
            "Epoch [3121/4000], Train Loss: 1.7094, Train Acc: 0.9313, Val Loss: 1.7119, Val Acc: 0.9288\n",
            "Epoch [3122/4000], Train Loss: 1.7109, Train Acc: 0.9274, Val Loss: 1.7083, Val Acc: 0.9327\n",
            "Epoch [3123/4000], Train Loss: 1.7086, Train Acc: 0.9296, Val Loss: 1.7054, Val Acc: 0.9314\n",
            "Epoch [3124/4000], Train Loss: 1.7109, Train Acc: 0.9280, Val Loss: 1.7136, Val Acc: 0.9263\n",
            "Epoch [3125/4000], Train Loss: 1.7088, Train Acc: 0.9303, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [3126/4000], Train Loss: 1.7095, Train Acc: 0.9284, Val Loss: 1.7070, Val Acc: 0.9314\n",
            "Epoch [3127/4000], Train Loss: 1.7093, Train Acc: 0.9304, Val Loss: 1.7074, Val Acc: 0.9327\n",
            "Epoch [3128/4000], Train Loss: 1.7099, Train Acc: 0.9300, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [3129/4000], Train Loss: 1.7094, Train Acc: 0.9304, Val Loss: 1.7090, Val Acc: 0.9301\n",
            "Epoch [3130/4000], Train Loss: 1.7081, Train Acc: 0.9292, Val Loss: 1.7083, Val Acc: 0.9346\n",
            "Epoch [3131/4000], Train Loss: 1.7094, Train Acc: 0.9287, Val Loss: 1.7103, Val Acc: 0.9263\n",
            "Epoch [3132/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7047, Val Acc: 0.9321\n",
            "Epoch [3133/4000], Train Loss: 1.7081, Train Acc: 0.9300, Val Loss: 1.7070, Val Acc: 0.9308\n",
            "Epoch [3134/4000], Train Loss: 1.7108, Train Acc: 0.9293, Val Loss: 1.7144, Val Acc: 0.9282\n",
            "Epoch [3135/4000], Train Loss: 1.7092, Train Acc: 0.9304, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [3136/4000], Train Loss: 1.7090, Train Acc: 0.9309, Val Loss: 1.7074, Val Acc: 0.9314\n",
            "Epoch [3137/4000], Train Loss: 1.7102, Train Acc: 0.9290, Val Loss: 1.7105, Val Acc: 0.9314\n",
            "Epoch [3138/4000], Train Loss: 1.7091, Train Acc: 0.9298, Val Loss: 1.7061, Val Acc: 0.9308\n",
            "Epoch [3139/4000], Train Loss: 1.7093, Train Acc: 0.9300, Val Loss: 1.7061, Val Acc: 0.9314\n",
            "Epoch [3140/4000], Train Loss: 1.7094, Train Acc: 0.9306, Val Loss: 1.7072, Val Acc: 0.9314\n",
            "Epoch [3141/4000], Train Loss: 1.7107, Train Acc: 0.9285, Val Loss: 1.7075, Val Acc: 0.9288\n",
            "Epoch [3142/4000], Train Loss: 1.7104, Train Acc: 0.9287, Val Loss: 1.7201, Val Acc: 0.9205\n",
            "Epoch [3143/4000], Train Loss: 1.7110, Train Acc: 0.9284, Val Loss: 1.7085, Val Acc: 0.9308\n",
            "Epoch [3144/4000], Train Loss: 1.7120, Train Acc: 0.9295, Val Loss: 1.7067, Val Acc: 0.9333\n",
            "Epoch [3145/4000], Train Loss: 1.7089, Train Acc: 0.9303, Val Loss: 1.7081, Val Acc: 0.9333\n",
            "Epoch [3146/4000], Train Loss: 1.7089, Train Acc: 0.9303, Val Loss: 1.7120, Val Acc: 0.9295\n",
            "Epoch [3147/4000], Train Loss: 1.7094, Train Acc: 0.9284, Val Loss: 1.7106, Val Acc: 0.9288\n",
            "Epoch [3148/4000], Train Loss: 1.7098, Train Acc: 0.9306, Val Loss: 1.7085, Val Acc: 0.9346\n",
            "Epoch [3149/4000], Train Loss: 1.7100, Train Acc: 0.9300, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [3150/4000], Train Loss: 1.7110, Train Acc: 0.9296, Val Loss: 1.7145, Val Acc: 0.9263\n",
            "Epoch [3151/4000], Train Loss: 1.7123, Train Acc: 0.9272, Val Loss: 1.7138, Val Acc: 0.9301\n",
            "Epoch [3152/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7109, Val Acc: 0.9314\n",
            "Epoch [3153/4000], Train Loss: 1.7082, Train Acc: 0.9313, Val Loss: 1.7099, Val Acc: 0.9340\n",
            "Epoch [3154/4000], Train Loss: 1.7098, Train Acc: 0.9293, Val Loss: 1.7110, Val Acc: 0.9308\n",
            "Epoch [3155/4000], Train Loss: 1.7097, Train Acc: 0.9298, Val Loss: 1.7099, Val Acc: 0.9327\n",
            "Epoch [3156/4000], Train Loss: 1.7096, Train Acc: 0.9298, Val Loss: 1.7098, Val Acc: 0.9288\n",
            "Epoch [3157/4000], Train Loss: 1.7110, Train Acc: 0.9285, Val Loss: 1.7096, Val Acc: 0.9308\n",
            "Epoch [3158/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7101, Val Acc: 0.9269\n",
            "Epoch [3159/4000], Train Loss: 1.7103, Train Acc: 0.9287, Val Loss: 1.7073, Val Acc: 0.9327\n",
            "Epoch [3160/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7061, Val Acc: 0.9301\n",
            "Epoch [3161/4000], Train Loss: 1.7092, Train Acc: 0.9301, Val Loss: 1.7061, Val Acc: 0.9321\n",
            "Epoch [3162/4000], Train Loss: 1.7093, Train Acc: 0.9304, Val Loss: 1.7098, Val Acc: 0.9308\n",
            "Epoch [3163/4000], Train Loss: 1.7117, Train Acc: 0.9282, Val Loss: 1.7073, Val Acc: 0.9327\n",
            "Epoch [3164/4000], Train Loss: 1.7101, Train Acc: 0.9292, Val Loss: 1.7104, Val Acc: 0.9308\n",
            "Epoch [3165/4000], Train Loss: 1.7115, Train Acc: 0.9284, Val Loss: 1.7134, Val Acc: 0.9263\n",
            "Epoch [3166/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7110, Val Acc: 0.9256\n",
            "Epoch [3167/4000], Train Loss: 1.7101, Train Acc: 0.9284, Val Loss: 1.7155, Val Acc: 0.9301\n",
            "Epoch [3168/4000], Train Loss: 1.7102, Train Acc: 0.9292, Val Loss: 1.7068, Val Acc: 0.9340\n",
            "Epoch [3169/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7123, Val Acc: 0.9333\n",
            "Epoch [3170/4000], Train Loss: 1.7085, Train Acc: 0.9306, Val Loss: 1.7081, Val Acc: 0.9314\n",
            "Epoch [3171/4000], Train Loss: 1.7098, Train Acc: 0.9308, Val Loss: 1.7074, Val Acc: 0.9340\n",
            "Epoch [3172/4000], Train Loss: 1.7091, Train Acc: 0.9292, Val Loss: 1.7056, Val Acc: 0.9308\n",
            "Epoch [3173/4000], Train Loss: 1.7097, Train Acc: 0.9288, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [3174/4000], Train Loss: 1.7095, Train Acc: 0.9288, Val Loss: 1.7067, Val Acc: 0.9314\n",
            "Epoch [3175/4000], Train Loss: 1.7104, Train Acc: 0.9296, Val Loss: 1.7140, Val Acc: 0.9256\n",
            "Epoch [3176/4000], Train Loss: 1.7108, Train Acc: 0.9292, Val Loss: 1.7089, Val Acc: 0.9295\n",
            "Epoch [3177/4000], Train Loss: 1.7128, Train Acc: 0.9276, Val Loss: 1.7097, Val Acc: 0.9308\n",
            "Epoch [3178/4000], Train Loss: 1.7084, Train Acc: 0.9298, Val Loss: 1.7145, Val Acc: 0.9276\n",
            "Epoch [3179/4000], Train Loss: 1.7106, Train Acc: 0.9284, Val Loss: 1.7154, Val Acc: 0.9250\n",
            "Epoch [3180/4000], Train Loss: 1.7110, Train Acc: 0.9279, Val Loss: 1.7118, Val Acc: 0.9301\n",
            "Epoch [3181/4000], Train Loss: 1.7105, Train Acc: 0.9277, Val Loss: 1.7083, Val Acc: 0.9327\n",
            "Epoch [3182/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7070, Val Acc: 0.9327\n",
            "Epoch [3183/4000], Train Loss: 1.7098, Train Acc: 0.9288, Val Loss: 1.7130, Val Acc: 0.9276\n",
            "Epoch [3184/4000], Train Loss: 1.7109, Train Acc: 0.9292, Val Loss: 1.7078, Val Acc: 0.9301\n",
            "Epoch [3185/4000], Train Loss: 1.7086, Train Acc: 0.9296, Val Loss: 1.7082, Val Acc: 0.9288\n",
            "Epoch [3186/4000], Train Loss: 1.7100, Train Acc: 0.9285, Val Loss: 1.7087, Val Acc: 0.9308\n",
            "Epoch [3187/4000], Train Loss: 1.7107, Train Acc: 0.9284, Val Loss: 1.7101, Val Acc: 0.9295\n",
            "Epoch [3188/4000], Train Loss: 1.7089, Train Acc: 0.9296, Val Loss: 1.7099, Val Acc: 0.9282\n",
            "Epoch [3189/4000], Train Loss: 1.7108, Train Acc: 0.9293, Val Loss: 1.7069, Val Acc: 0.9327\n",
            "Epoch [3190/4000], Train Loss: 1.7100, Train Acc: 0.9304, Val Loss: 1.7112, Val Acc: 0.9263\n",
            "Epoch [3191/4000], Train Loss: 1.7134, Train Acc: 0.9269, Val Loss: 1.7104, Val Acc: 0.9295\n",
            "Epoch [3192/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7114, Val Acc: 0.9288\n",
            "Epoch [3193/4000], Train Loss: 1.7091, Train Acc: 0.9280, Val Loss: 1.7084, Val Acc: 0.9327\n",
            "Epoch [3194/4000], Train Loss: 1.7097, Train Acc: 0.9308, Val Loss: 1.7035, Val Acc: 0.9327\n",
            "Epoch [3195/4000], Train Loss: 1.7092, Train Acc: 0.9285, Val Loss: 1.7116, Val Acc: 0.9295\n",
            "Epoch [3196/4000], Train Loss: 1.7087, Train Acc: 0.9298, Val Loss: 1.7128, Val Acc: 0.9282\n",
            "Epoch [3197/4000], Train Loss: 1.7104, Train Acc: 0.9284, Val Loss: 1.7116, Val Acc: 0.9321\n",
            "Epoch [3198/4000], Train Loss: 1.7091, Train Acc: 0.9308, Val Loss: 1.7075, Val Acc: 0.9314\n",
            "Epoch [3199/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7274, Val Acc: 0.9231\n",
            "Epoch [3200/4000], Train Loss: 1.7102, Train Acc: 0.9300, Val Loss: 1.7145, Val Acc: 0.9269\n",
            "Epoch [3201/4000], Train Loss: 1.7121, Train Acc: 0.9276, Val Loss: 1.7050, Val Acc: 0.9321\n",
            "Epoch [3202/4000], Train Loss: 1.7094, Train Acc: 0.9288, Val Loss: 1.7154, Val Acc: 0.9288\n",
            "Epoch [3203/4000], Train Loss: 1.7120, Train Acc: 0.9282, Val Loss: 1.7103, Val Acc: 0.9250\n",
            "Epoch [3204/4000], Train Loss: 1.7088, Train Acc: 0.9293, Val Loss: 1.7083, Val Acc: 0.9308\n",
            "Epoch [3205/4000], Train Loss: 1.7092, Train Acc: 0.9308, Val Loss: 1.7071, Val Acc: 0.9346\n",
            "Epoch [3206/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7154, Val Acc: 0.9212\n",
            "Epoch [3207/4000], Train Loss: 1.7103, Train Acc: 0.9285, Val Loss: 1.7061, Val Acc: 0.9308\n",
            "Epoch [3208/4000], Train Loss: 1.7087, Train Acc: 0.9293, Val Loss: 1.7082, Val Acc: 0.9346\n",
            "Epoch [3209/4000], Train Loss: 1.7095, Train Acc: 0.9285, Val Loss: 1.7077, Val Acc: 0.9340\n",
            "Epoch [3210/4000], Train Loss: 1.7102, Train Acc: 0.9277, Val Loss: 1.7077, Val Acc: 0.9314\n",
            "Epoch [3211/4000], Train Loss: 1.7118, Train Acc: 0.9282, Val Loss: 1.7093, Val Acc: 0.9321\n",
            "Epoch [3212/4000], Train Loss: 1.7090, Train Acc: 0.9306, Val Loss: 1.7078, Val Acc: 0.9308\n",
            "Epoch [3213/4000], Train Loss: 1.7112, Train Acc: 0.9272, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [3214/4000], Train Loss: 1.7096, Train Acc: 0.9303, Val Loss: 1.7105, Val Acc: 0.9288\n",
            "Epoch [3215/4000], Train Loss: 1.7108, Train Acc: 0.9301, Val Loss: 1.7105, Val Acc: 0.9308\n",
            "Epoch [3216/4000], Train Loss: 1.7089, Train Acc: 0.9308, Val Loss: 1.7072, Val Acc: 0.9327\n",
            "Epoch [3217/4000], Train Loss: 1.7115, Train Acc: 0.9277, Val Loss: 1.7089, Val Acc: 0.9295\n",
            "Epoch [3218/4000], Train Loss: 1.7088, Train Acc: 0.9301, Val Loss: 1.7137, Val Acc: 0.9288\n",
            "Epoch [3219/4000], Train Loss: 1.7098, Train Acc: 0.9295, Val Loss: 1.7072, Val Acc: 0.9308\n",
            "Epoch [3220/4000], Train Loss: 1.7095, Train Acc: 0.9300, Val Loss: 1.7082, Val Acc: 0.9269\n",
            "Epoch [3221/4000], Train Loss: 1.7088, Train Acc: 0.9295, Val Loss: 1.7067, Val Acc: 0.9327\n",
            "Epoch [3222/4000], Train Loss: 1.7079, Train Acc: 0.9301, Val Loss: 1.7044, Val Acc: 0.9301\n",
            "Epoch [3223/4000], Train Loss: 1.7094, Train Acc: 0.9311, Val Loss: 1.7140, Val Acc: 0.9224\n",
            "Epoch [3224/4000], Train Loss: 1.7106, Train Acc: 0.9288, Val Loss: 1.7090, Val Acc: 0.9353\n",
            "Epoch [3225/4000], Train Loss: 1.7093, Train Acc: 0.9304, Val Loss: 1.7094, Val Acc: 0.9301\n",
            "Epoch [3226/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7077, Val Acc: 0.9340\n",
            "Epoch [3227/4000], Train Loss: 1.7095, Train Acc: 0.9306, Val Loss: 1.7166, Val Acc: 0.9244\n",
            "Epoch [3228/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7071, Val Acc: 0.9308\n",
            "Epoch [3229/4000], Train Loss: 1.7086, Train Acc: 0.9303, Val Loss: 1.7073, Val Acc: 0.9308\n",
            "Epoch [3230/4000], Train Loss: 1.7091, Train Acc: 0.9303, Val Loss: 1.7127, Val Acc: 0.9301\n",
            "Epoch [3231/4000], Train Loss: 1.7094, Train Acc: 0.9306, Val Loss: 1.7141, Val Acc: 0.9288\n",
            "Epoch [3232/4000], Train Loss: 1.7094, Train Acc: 0.9303, Val Loss: 1.7061, Val Acc: 0.9308\n",
            "Epoch [3233/4000], Train Loss: 1.7086, Train Acc: 0.9298, Val Loss: 1.7079, Val Acc: 0.9308\n",
            "Epoch [3234/4000], Train Loss: 1.7098, Train Acc: 0.9293, Val Loss: 1.7201, Val Acc: 0.9218\n",
            "Epoch [3235/4000], Train Loss: 1.7113, Train Acc: 0.9301, Val Loss: 1.7165, Val Acc: 0.9256\n",
            "Epoch [3236/4000], Train Loss: 1.7099, Train Acc: 0.9280, Val Loss: 1.7103, Val Acc: 0.9295\n",
            "Epoch [3237/4000], Train Loss: 1.7100, Train Acc: 0.9295, Val Loss: 1.7104, Val Acc: 0.9346\n",
            "Epoch [3238/4000], Train Loss: 1.7102, Train Acc: 0.9284, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [3239/4000], Train Loss: 1.7106, Train Acc: 0.9287, Val Loss: 1.7181, Val Acc: 0.9263\n",
            "Epoch [3240/4000], Train Loss: 1.7102, Train Acc: 0.9284, Val Loss: 1.7114, Val Acc: 0.9314\n",
            "Epoch [3241/4000], Train Loss: 1.7079, Train Acc: 0.9308, Val Loss: 1.7078, Val Acc: 0.9301\n",
            "Epoch [3242/4000], Train Loss: 1.7092, Train Acc: 0.9295, Val Loss: 1.7058, Val Acc: 0.9308\n",
            "Epoch [3243/4000], Train Loss: 1.7113, Train Acc: 0.9269, Val Loss: 1.7088, Val Acc: 0.9295\n",
            "Epoch [3244/4000], Train Loss: 1.7111, Train Acc: 0.9292, Val Loss: 1.7092, Val Acc: 0.9282\n",
            "Epoch [3245/4000], Train Loss: 1.7094, Train Acc: 0.9304, Val Loss: 1.7103, Val Acc: 0.9301\n",
            "Epoch [3246/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7090, Val Acc: 0.9308\n",
            "Epoch [3247/4000], Train Loss: 1.7093, Train Acc: 0.9309, Val Loss: 1.7118, Val Acc: 0.9308\n",
            "Epoch [3248/4000], Train Loss: 1.7111, Train Acc: 0.9293, Val Loss: 1.7079, Val Acc: 0.9327\n",
            "Epoch [3249/4000], Train Loss: 1.7085, Train Acc: 0.9293, Val Loss: 1.7066, Val Acc: 0.9301\n",
            "Epoch [3250/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7072, Val Acc: 0.9282\n",
            "Epoch [3251/4000], Train Loss: 1.7096, Train Acc: 0.9279, Val Loss: 1.7175, Val Acc: 0.9218\n",
            "Epoch [3252/4000], Train Loss: 1.7081, Train Acc: 0.9304, Val Loss: 1.7097, Val Acc: 0.9327\n",
            "Epoch [3253/4000], Train Loss: 1.7097, Train Acc: 0.9306, Val Loss: 1.7198, Val Acc: 0.9212\n",
            "Epoch [3254/4000], Train Loss: 1.7089, Train Acc: 0.9301, Val Loss: 1.7100, Val Acc: 0.9321\n",
            "Epoch [3255/4000], Train Loss: 1.7106, Train Acc: 0.9296, Val Loss: 1.7113, Val Acc: 0.9288\n",
            "Epoch [3256/4000], Train Loss: 1.7097, Train Acc: 0.9288, Val Loss: 1.7064, Val Acc: 0.9321\n",
            "Epoch [3257/4000], Train Loss: 1.7098, Train Acc: 0.9288, Val Loss: 1.7095, Val Acc: 0.9295\n",
            "Epoch [3258/4000], Train Loss: 1.7097, Train Acc: 0.9276, Val Loss: 1.7072, Val Acc: 0.9346\n",
            "Epoch [3259/4000], Train Loss: 1.7090, Train Acc: 0.9313, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [3260/4000], Train Loss: 1.7087, Train Acc: 0.9304, Val Loss: 1.7105, Val Acc: 0.9288\n",
            "Epoch [3261/4000], Train Loss: 1.7090, Train Acc: 0.9303, Val Loss: 1.7094, Val Acc: 0.9288\n",
            "Epoch [3262/4000], Train Loss: 1.7108, Train Acc: 0.9285, Val Loss: 1.7092, Val Acc: 0.9301\n",
            "Epoch [3263/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7098, Val Acc: 0.9321\n",
            "Epoch [3264/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7073, Val Acc: 0.9327\n",
            "Epoch [3265/4000], Train Loss: 1.7089, Train Acc: 0.9304, Val Loss: 1.7083, Val Acc: 0.9327\n",
            "Epoch [3266/4000], Train Loss: 1.7093, Train Acc: 0.9300, Val Loss: 1.7071, Val Acc: 0.9327\n",
            "Epoch [3267/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7122, Val Acc: 0.9288\n",
            "Epoch [3268/4000], Train Loss: 1.7103, Train Acc: 0.9282, Val Loss: 1.7043, Val Acc: 0.9308\n",
            "Epoch [3269/4000], Train Loss: 1.7093, Train Acc: 0.9287, Val Loss: 1.7110, Val Acc: 0.9288\n",
            "Epoch [3270/4000], Train Loss: 1.7108, Train Acc: 0.9277, Val Loss: 1.7077, Val Acc: 0.9308\n",
            "Epoch [3271/4000], Train Loss: 1.7088, Train Acc: 0.9300, Val Loss: 1.7122, Val Acc: 0.9256\n",
            "Epoch [3272/4000], Train Loss: 1.7117, Train Acc: 0.9260, Val Loss: 1.7140, Val Acc: 0.9288\n",
            "Epoch [3273/4000], Train Loss: 1.7119, Train Acc: 0.9271, Val Loss: 1.7238, Val Acc: 0.9186\n",
            "Epoch [3274/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7090, Val Acc: 0.9301\n",
            "Epoch [3275/4000], Train Loss: 1.7096, Train Acc: 0.9292, Val Loss: 1.7081, Val Acc: 0.9288\n",
            "Epoch [3276/4000], Train Loss: 1.7097, Train Acc: 0.9290, Val Loss: 1.7067, Val Acc: 0.9333\n",
            "Epoch [3277/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7113, Val Acc: 0.9244\n",
            "Epoch [3278/4000], Train Loss: 1.7098, Train Acc: 0.9298, Val Loss: 1.7062, Val Acc: 0.9301\n",
            "Epoch [3279/4000], Train Loss: 1.7084, Train Acc: 0.9292, Val Loss: 1.7077, Val Acc: 0.9308\n",
            "Epoch [3280/4000], Train Loss: 1.7113, Train Acc: 0.9292, Val Loss: 1.7086, Val Acc: 0.9308\n",
            "Epoch [3281/4000], Train Loss: 1.7092, Train Acc: 0.9293, Val Loss: 1.7076, Val Acc: 0.9308\n",
            "Epoch [3282/4000], Train Loss: 1.7107, Train Acc: 0.9277, Val Loss: 1.7069, Val Acc: 0.9314\n",
            "Epoch [3283/4000], Train Loss: 1.7100, Train Acc: 0.9298, Val Loss: 1.7066, Val Acc: 0.9295\n",
            "Epoch [3284/4000], Train Loss: 1.7107, Train Acc: 0.9285, Val Loss: 1.7060, Val Acc: 0.9314\n",
            "Epoch [3285/4000], Train Loss: 1.7106, Train Acc: 0.9285, Val Loss: 1.7056, Val Acc: 0.9327\n",
            "Epoch [3286/4000], Train Loss: 1.7100, Train Acc: 0.9290, Val Loss: 1.7110, Val Acc: 0.9308\n",
            "Epoch [3287/4000], Train Loss: 1.7077, Train Acc: 0.9309, Val Loss: 1.7080, Val Acc: 0.9288\n",
            "Epoch [3288/4000], Train Loss: 1.7097, Train Acc: 0.9296, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [3289/4000], Train Loss: 1.7099, Train Acc: 0.9288, Val Loss: 1.7115, Val Acc: 0.9269\n",
            "Epoch [3290/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7144, Val Acc: 0.9295\n",
            "Epoch [3291/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7100, Val Acc: 0.9308\n",
            "Epoch [3292/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7099, Val Acc: 0.9269\n",
            "Epoch [3293/4000], Train Loss: 1.7117, Train Acc: 0.9268, Val Loss: 1.7068, Val Acc: 0.9327\n",
            "Epoch [3294/4000], Train Loss: 1.7105, Train Acc: 0.9296, Val Loss: 1.7120, Val Acc: 0.9282\n",
            "Epoch [3295/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7082, Val Acc: 0.9327\n",
            "Epoch [3296/4000], Train Loss: 1.7084, Train Acc: 0.9293, Val Loss: 1.7103, Val Acc: 0.9308\n",
            "Epoch [3297/4000], Train Loss: 1.7100, Train Acc: 0.9298, Val Loss: 1.7105, Val Acc: 0.9288\n",
            "Epoch [3298/4000], Train Loss: 1.7092, Train Acc: 0.9309, Val Loss: 1.7119, Val Acc: 0.9295\n",
            "Epoch [3299/4000], Train Loss: 1.7093, Train Acc: 0.9296, Val Loss: 1.7063, Val Acc: 0.9327\n",
            "Epoch [3300/4000], Train Loss: 1.7092, Train Acc: 0.9296, Val Loss: 1.7079, Val Acc: 0.9301\n",
            "Epoch [3301/4000], Train Loss: 1.7104, Train Acc: 0.9280, Val Loss: 1.7068, Val Acc: 0.9314\n",
            "Epoch [3302/4000], Train Loss: 1.7094, Train Acc: 0.9290, Val Loss: 1.7091, Val Acc: 0.9314\n",
            "Epoch [3303/4000], Train Loss: 1.7108, Train Acc: 0.9284, Val Loss: 1.7103, Val Acc: 0.9314\n",
            "Epoch [3304/4000], Train Loss: 1.7113, Train Acc: 0.9293, Val Loss: 1.7166, Val Acc: 0.9231\n",
            "Epoch [3305/4000], Train Loss: 1.7095, Train Acc: 0.9288, Val Loss: 1.7073, Val Acc: 0.9308\n",
            "Epoch [3306/4000], Train Loss: 1.7087, Train Acc: 0.9301, Val Loss: 1.7097, Val Acc: 0.9308\n",
            "Epoch [3307/4000], Train Loss: 1.7079, Train Acc: 0.9314, Val Loss: 1.7130, Val Acc: 0.9244\n",
            "Epoch [3308/4000], Train Loss: 1.7106, Train Acc: 0.9279, Val Loss: 1.7154, Val Acc: 0.9250\n",
            "Epoch [3309/4000], Train Loss: 1.7114, Train Acc: 0.9280, Val Loss: 1.7156, Val Acc: 0.9301\n",
            "Epoch [3310/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7054, Val Acc: 0.9327\n",
            "Epoch [3311/4000], Train Loss: 1.7097, Train Acc: 0.9280, Val Loss: 1.7087, Val Acc: 0.9327\n",
            "Epoch [3312/4000], Train Loss: 1.7100, Train Acc: 0.9295, Val Loss: 1.7095, Val Acc: 0.9314\n",
            "Epoch [3313/4000], Train Loss: 1.7094, Train Acc: 0.9298, Val Loss: 1.7063, Val Acc: 0.9321\n",
            "Epoch [3314/4000], Train Loss: 1.7095, Train Acc: 0.9303, Val Loss: 1.7115, Val Acc: 0.9282\n",
            "Epoch [3315/4000], Train Loss: 1.7112, Train Acc: 0.9290, Val Loss: 1.7091, Val Acc: 0.9295\n",
            "Epoch [3316/4000], Train Loss: 1.7102, Train Acc: 0.9284, Val Loss: 1.7117, Val Acc: 0.9301\n",
            "Epoch [3317/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7097, Val Acc: 0.9295\n",
            "Epoch [3318/4000], Train Loss: 1.7100, Train Acc: 0.9292, Val Loss: 1.7110, Val Acc: 0.9288\n",
            "Epoch [3319/4000], Train Loss: 1.7101, Train Acc: 0.9296, Val Loss: 1.7062, Val Acc: 0.9308\n",
            "Epoch [3320/4000], Train Loss: 1.7095, Train Acc: 0.9298, Val Loss: 1.7050, Val Acc: 0.9314\n",
            "Epoch [3321/4000], Train Loss: 1.7094, Train Acc: 0.9287, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [3322/4000], Train Loss: 1.7091, Train Acc: 0.9290, Val Loss: 1.7057, Val Acc: 0.9327\n",
            "Epoch [3323/4000], Train Loss: 1.7093, Train Acc: 0.9300, Val Loss: 1.7084, Val Acc: 0.9321\n",
            "Epoch [3324/4000], Train Loss: 1.7092, Train Acc: 0.9287, Val Loss: 1.7195, Val Acc: 0.9256\n",
            "Epoch [3325/4000], Train Loss: 1.7091, Train Acc: 0.9304, Val Loss: 1.7147, Val Acc: 0.9276\n",
            "Epoch [3326/4000], Train Loss: 1.7111, Train Acc: 0.9288, Val Loss: 1.7110, Val Acc: 0.9327\n",
            "Epoch [3327/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7088, Val Acc: 0.9333\n",
            "Epoch [3328/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7084, Val Acc: 0.9327\n",
            "Epoch [3329/4000], Train Loss: 1.7106, Train Acc: 0.9285, Val Loss: 1.7092, Val Acc: 0.9321\n",
            "Epoch [3330/4000], Train Loss: 1.7091, Train Acc: 0.9304, Val Loss: 1.7083, Val Acc: 0.9295\n",
            "Epoch [3331/4000], Train Loss: 1.7102, Train Acc: 0.9280, Val Loss: 1.7094, Val Acc: 0.9308\n",
            "Epoch [3332/4000], Train Loss: 1.7098, Train Acc: 0.9304, Val Loss: 1.7123, Val Acc: 0.9288\n",
            "Epoch [3333/4000], Train Loss: 1.7119, Train Acc: 0.9293, Val Loss: 1.7231, Val Acc: 0.9186\n",
            "Epoch [3334/4000], Train Loss: 1.7122, Train Acc: 0.9279, Val Loss: 1.7083, Val Acc: 0.9314\n",
            "Epoch [3335/4000], Train Loss: 1.7098, Train Acc: 0.9293, Val Loss: 1.7091, Val Acc: 0.9314\n",
            "Epoch [3336/4000], Train Loss: 1.7120, Train Acc: 0.9274, Val Loss: 1.7098, Val Acc: 0.9346\n",
            "Epoch [3337/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7085, Val Acc: 0.9301\n",
            "Epoch [3338/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7083, Val Acc: 0.9333\n",
            "Epoch [3339/4000], Train Loss: 1.7095, Train Acc: 0.9301, Val Loss: 1.7104, Val Acc: 0.9308\n",
            "Epoch [3340/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7083, Val Acc: 0.9333\n",
            "Epoch [3341/4000], Train Loss: 1.7098, Train Acc: 0.9288, Val Loss: 1.7117, Val Acc: 0.9314\n",
            "Epoch [3342/4000], Train Loss: 1.7085, Train Acc: 0.9306, Val Loss: 1.7089, Val Acc: 0.9288\n",
            "Epoch [3343/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7050, Val Acc: 0.9327\n",
            "Epoch [3344/4000], Train Loss: 1.7102, Train Acc: 0.9285, Val Loss: 1.7094, Val Acc: 0.9295\n",
            "Epoch [3345/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7063, Val Acc: 0.9333\n",
            "Epoch [3346/4000], Train Loss: 1.7085, Train Acc: 0.9293, Val Loss: 1.7111, Val Acc: 0.9308\n",
            "Epoch [3347/4000], Train Loss: 1.7090, Train Acc: 0.9303, Val Loss: 1.7053, Val Acc: 0.9301\n",
            "Epoch [3348/4000], Train Loss: 1.7109, Train Acc: 0.9279, Val Loss: 1.7145, Val Acc: 0.9250\n",
            "Epoch [3349/4000], Train Loss: 1.7096, Train Acc: 0.9288, Val Loss: 1.7097, Val Acc: 0.9282\n",
            "Epoch [3350/4000], Train Loss: 1.7097, Train Acc: 0.9306, Val Loss: 1.7140, Val Acc: 0.9269\n",
            "Epoch [3351/4000], Train Loss: 1.7106, Train Acc: 0.9293, Val Loss: 1.7186, Val Acc: 0.9250\n",
            "Epoch [3352/4000], Train Loss: 1.7100, Train Acc: 0.9295, Val Loss: 1.7060, Val Acc: 0.9301\n",
            "Epoch [3353/4000], Train Loss: 1.7085, Train Acc: 0.9304, Val Loss: 1.7106, Val Acc: 0.9282\n",
            "Epoch [3354/4000], Train Loss: 1.7089, Train Acc: 0.9295, Val Loss: 1.7065, Val Acc: 0.9321\n",
            "Epoch [3355/4000], Train Loss: 1.7093, Train Acc: 0.9303, Val Loss: 1.7090, Val Acc: 0.9308\n",
            "Epoch [3356/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7059, Val Acc: 0.9333\n",
            "Epoch [3357/4000], Train Loss: 1.7086, Train Acc: 0.9295, Val Loss: 1.7083, Val Acc: 0.9308\n",
            "Epoch [3358/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7095, Val Acc: 0.9295\n",
            "Epoch [3359/4000], Train Loss: 1.7094, Train Acc: 0.9288, Val Loss: 1.7065, Val Acc: 0.9321\n",
            "Epoch [3360/4000], Train Loss: 1.7093, Train Acc: 0.9285, Val Loss: 1.7123, Val Acc: 0.9269\n",
            "Epoch [3361/4000], Train Loss: 1.7098, Train Acc: 0.9290, Val Loss: 1.7098, Val Acc: 0.9308\n",
            "Epoch [3362/4000], Train Loss: 1.7083, Train Acc: 0.9306, Val Loss: 1.7075, Val Acc: 0.9295\n",
            "Epoch [3363/4000], Train Loss: 1.7107, Train Acc: 0.9293, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [3364/4000], Train Loss: 1.7093, Train Acc: 0.9298, Val Loss: 1.7076, Val Acc: 0.9327\n",
            "Epoch [3365/4000], Train Loss: 1.7087, Train Acc: 0.9300, Val Loss: 1.7091, Val Acc: 0.9295\n",
            "Epoch [3366/4000], Train Loss: 1.7102, Train Acc: 0.9288, Val Loss: 1.7082, Val Acc: 0.9308\n",
            "Epoch [3367/4000], Train Loss: 1.7082, Train Acc: 0.9308, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [3368/4000], Train Loss: 1.7097, Train Acc: 0.9304, Val Loss: 1.7095, Val Acc: 0.9333\n",
            "Epoch [3369/4000], Train Loss: 1.7104, Train Acc: 0.9277, Val Loss: 1.7090, Val Acc: 0.9314\n",
            "Epoch [3370/4000], Train Loss: 1.7096, Train Acc: 0.9288, Val Loss: 1.7098, Val Acc: 0.9333\n",
            "Epoch [3371/4000], Train Loss: 1.7087, Train Acc: 0.9298, Val Loss: 1.7129, Val Acc: 0.9276\n",
            "Epoch [3372/4000], Train Loss: 1.7112, Train Acc: 0.9306, Val Loss: 1.7107, Val Acc: 0.9327\n",
            "Epoch [3373/4000], Train Loss: 1.7098, Train Acc: 0.9287, Val Loss: 1.7097, Val Acc: 0.9282\n",
            "Epoch [3374/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7095, Val Acc: 0.9321\n",
            "Epoch [3375/4000], Train Loss: 1.7081, Train Acc: 0.9292, Val Loss: 1.7072, Val Acc: 0.9301\n",
            "Epoch [3376/4000], Train Loss: 1.7081, Train Acc: 0.9303, Val Loss: 1.7093, Val Acc: 0.9340\n",
            "Epoch [3377/4000], Train Loss: 1.7085, Train Acc: 0.9306, Val Loss: 1.7072, Val Acc: 0.9321\n",
            "Epoch [3378/4000], Train Loss: 1.7086, Train Acc: 0.9298, Val Loss: 1.7108, Val Acc: 0.9295\n",
            "Epoch [3379/4000], Train Loss: 1.7100, Train Acc: 0.9296, Val Loss: 1.7103, Val Acc: 0.9314\n",
            "Epoch [3380/4000], Train Loss: 1.7090, Train Acc: 0.9290, Val Loss: 1.7100, Val Acc: 0.9276\n",
            "Epoch [3381/4000], Train Loss: 1.7107, Train Acc: 0.9290, Val Loss: 1.7162, Val Acc: 0.9250\n",
            "Epoch [3382/4000], Train Loss: 1.7095, Train Acc: 0.9282, Val Loss: 1.7076, Val Acc: 0.9333\n",
            "Epoch [3383/4000], Train Loss: 1.7099, Train Acc: 0.9292, Val Loss: 1.7094, Val Acc: 0.9276\n",
            "Epoch [3384/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7148, Val Acc: 0.9295\n",
            "Epoch [3385/4000], Train Loss: 1.7095, Train Acc: 0.9300, Val Loss: 1.7099, Val Acc: 0.9321\n",
            "Epoch [3386/4000], Train Loss: 1.7099, Train Acc: 0.9304, Val Loss: 1.7120, Val Acc: 0.9282\n",
            "Epoch [3387/4000], Train Loss: 1.7094, Train Acc: 0.9301, Val Loss: 1.7107, Val Acc: 0.9263\n",
            "Epoch [3388/4000], Train Loss: 1.7096, Train Acc: 0.9285, Val Loss: 1.7127, Val Acc: 0.9321\n",
            "Epoch [3389/4000], Train Loss: 1.7091, Train Acc: 0.9300, Val Loss: 1.7067, Val Acc: 0.9321\n",
            "Epoch [3390/4000], Train Loss: 1.7105, Train Acc: 0.9293, Val Loss: 1.7109, Val Acc: 0.9301\n",
            "Epoch [3391/4000], Train Loss: 1.7095, Train Acc: 0.9295, Val Loss: 1.7082, Val Acc: 0.9327\n",
            "Epoch [3392/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7110, Val Acc: 0.9295\n",
            "Epoch [3393/4000], Train Loss: 1.7108, Train Acc: 0.9287, Val Loss: 1.7045, Val Acc: 0.9327\n",
            "Epoch [3394/4000], Train Loss: 1.7109, Train Acc: 0.9285, Val Loss: 1.7102, Val Acc: 0.9314\n",
            "Epoch [3395/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7112, Val Acc: 0.9276\n",
            "Epoch [3396/4000], Train Loss: 1.7083, Train Acc: 0.9293, Val Loss: 1.7096, Val Acc: 0.9314\n",
            "Epoch [3397/4000], Train Loss: 1.7110, Train Acc: 0.9276, Val Loss: 1.7216, Val Acc: 0.9212\n",
            "Epoch [3398/4000], Train Loss: 1.7104, Train Acc: 0.9293, Val Loss: 1.7130, Val Acc: 0.9321\n",
            "Epoch [3399/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [3400/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7100, Val Acc: 0.9314\n",
            "Epoch [3401/4000], Train Loss: 1.7091, Train Acc: 0.9298, Val Loss: 1.7106, Val Acc: 0.9314\n",
            "Epoch [3402/4000], Train Loss: 1.7092, Train Acc: 0.9304, Val Loss: 1.7106, Val Acc: 0.9314\n",
            "Epoch [3403/4000], Train Loss: 1.7113, Train Acc: 0.9280, Val Loss: 1.7099, Val Acc: 0.9295\n",
            "Epoch [3404/4000], Train Loss: 1.7088, Train Acc: 0.9296, Val Loss: 1.7156, Val Acc: 0.9244\n",
            "Epoch [3405/4000], Train Loss: 1.7087, Train Acc: 0.9301, Val Loss: 1.7075, Val Acc: 0.9301\n",
            "Epoch [3406/4000], Train Loss: 1.7093, Train Acc: 0.9298, Val Loss: 1.7081, Val Acc: 0.9295\n",
            "Epoch [3407/4000], Train Loss: 1.7090, Train Acc: 0.9301, Val Loss: 1.7197, Val Acc: 0.9205\n",
            "Epoch [3408/4000], Train Loss: 1.7099, Train Acc: 0.9296, Val Loss: 1.7221, Val Acc: 0.9192\n",
            "Epoch [3409/4000], Train Loss: 1.7114, Train Acc: 0.9287, Val Loss: 1.7069, Val Acc: 0.9314\n",
            "Epoch [3410/4000], Train Loss: 1.7090, Train Acc: 0.9287, Val Loss: 1.7088, Val Acc: 0.9301\n",
            "Epoch [3411/4000], Train Loss: 1.7094, Train Acc: 0.9298, Val Loss: 1.7094, Val Acc: 0.9321\n",
            "Epoch [3412/4000], Train Loss: 1.7108, Train Acc: 0.9285, Val Loss: 1.7050, Val Acc: 0.9346\n",
            "Epoch [3413/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7111, Val Acc: 0.9269\n",
            "Epoch [3414/4000], Train Loss: 1.7088, Train Acc: 0.9296, Val Loss: 1.7104, Val Acc: 0.9340\n",
            "Epoch [3415/4000], Train Loss: 1.7097, Train Acc: 0.9303, Val Loss: 1.7099, Val Acc: 0.9340\n",
            "Epoch [3416/4000], Train Loss: 1.7082, Train Acc: 0.9304, Val Loss: 1.7069, Val Acc: 0.9321\n",
            "Epoch [3417/4000], Train Loss: 1.7089, Train Acc: 0.9293, Val Loss: 1.7126, Val Acc: 0.9308\n",
            "Epoch [3418/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7071, Val Acc: 0.9301\n",
            "Epoch [3419/4000], Train Loss: 1.7116, Train Acc: 0.9288, Val Loss: 1.7096, Val Acc: 0.9333\n",
            "Epoch [3420/4000], Train Loss: 1.7100, Train Acc: 0.9284, Val Loss: 1.7103, Val Acc: 0.9269\n",
            "Epoch [3421/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7070, Val Acc: 0.9321\n",
            "Epoch [3422/4000], Train Loss: 1.7111, Train Acc: 0.9290, Val Loss: 1.7081, Val Acc: 0.9327\n",
            "Epoch [3423/4000], Train Loss: 1.7091, Train Acc: 0.9303, Val Loss: 1.7056, Val Acc: 0.9314\n",
            "Epoch [3424/4000], Train Loss: 1.7103, Train Acc: 0.9295, Val Loss: 1.7142, Val Acc: 0.9256\n",
            "Epoch [3425/4000], Train Loss: 1.7091, Train Acc: 0.9301, Val Loss: 1.7120, Val Acc: 0.9276\n",
            "Epoch [3426/4000], Train Loss: 1.7108, Train Acc: 0.9292, Val Loss: 1.7250, Val Acc: 0.9186\n",
            "Epoch [3427/4000], Train Loss: 1.7120, Train Acc: 0.9274, Val Loss: 1.7115, Val Acc: 0.9308\n",
            "Epoch [3428/4000], Train Loss: 1.7106, Train Acc: 0.9288, Val Loss: 1.7092, Val Acc: 0.9314\n",
            "Epoch [3429/4000], Train Loss: 1.7099, Train Acc: 0.9284, Val Loss: 1.7132, Val Acc: 0.9250\n",
            "Epoch [3430/4000], Train Loss: 1.7107, Train Acc: 0.9274, Val Loss: 1.7072, Val Acc: 0.9314\n",
            "Epoch [3431/4000], Train Loss: 1.7088, Train Acc: 0.9300, Val Loss: 1.7078, Val Acc: 0.9314\n",
            "Epoch [3432/4000], Train Loss: 1.7089, Train Acc: 0.9306, Val Loss: 1.7134, Val Acc: 0.9295\n",
            "Epoch [3433/4000], Train Loss: 1.7084, Train Acc: 0.9313, Val Loss: 1.7087, Val Acc: 0.9333\n",
            "Epoch [3434/4000], Train Loss: 1.7109, Train Acc: 0.9279, Val Loss: 1.7115, Val Acc: 0.9308\n",
            "Epoch [3435/4000], Train Loss: 1.7092, Train Acc: 0.9290, Val Loss: 1.7177, Val Acc: 0.9250\n",
            "Epoch [3436/4000], Train Loss: 1.7099, Train Acc: 0.9300, Val Loss: 1.7154, Val Acc: 0.9288\n",
            "Epoch [3437/4000], Train Loss: 1.7091, Train Acc: 0.9296, Val Loss: 1.7141, Val Acc: 0.9256\n",
            "Epoch [3438/4000], Train Loss: 1.7111, Train Acc: 0.9282, Val Loss: 1.7094, Val Acc: 0.9308\n",
            "Epoch [3439/4000], Train Loss: 1.7090, Train Acc: 0.9296, Val Loss: 1.7130, Val Acc: 0.9263\n",
            "Epoch [3440/4000], Train Loss: 1.7089, Train Acc: 0.9314, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [3441/4000], Train Loss: 1.7096, Train Acc: 0.9295, Val Loss: 1.7061, Val Acc: 0.9301\n",
            "Epoch [3442/4000], Train Loss: 1.7088, Train Acc: 0.9303, Val Loss: 1.7082, Val Acc: 0.9327\n",
            "Epoch [3443/4000], Train Loss: 1.7088, Train Acc: 0.9308, Val Loss: 1.7178, Val Acc: 0.9237\n",
            "Epoch [3444/4000], Train Loss: 1.7097, Train Acc: 0.9296, Val Loss: 1.7068, Val Acc: 0.9308\n",
            "Epoch [3445/4000], Train Loss: 1.7091, Train Acc: 0.9298, Val Loss: 1.7089, Val Acc: 0.9308\n",
            "Epoch [3446/4000], Train Loss: 1.7092, Train Acc: 0.9296, Val Loss: 1.7105, Val Acc: 0.9308\n",
            "Epoch [3447/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7142, Val Acc: 0.9250\n",
            "Epoch [3448/4000], Train Loss: 1.7109, Train Acc: 0.9284, Val Loss: 1.7249, Val Acc: 0.9103\n",
            "Epoch [3449/4000], Train Loss: 1.7117, Train Acc: 0.9290, Val Loss: 1.7073, Val Acc: 0.9314\n",
            "Epoch [3450/4000], Train Loss: 1.7087, Train Acc: 0.9303, Val Loss: 1.7082, Val Acc: 0.9288\n",
            "Epoch [3451/4000], Train Loss: 1.7104, Train Acc: 0.9292, Val Loss: 1.7095, Val Acc: 0.9314\n",
            "Epoch [3452/4000], Train Loss: 1.7094, Train Acc: 0.9290, Val Loss: 1.7132, Val Acc: 0.9256\n",
            "Epoch [3453/4000], Train Loss: 1.7088, Train Acc: 0.9301, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [3454/4000], Train Loss: 1.7095, Train Acc: 0.9292, Val Loss: 1.7069, Val Acc: 0.9327\n",
            "Epoch [3455/4000], Train Loss: 1.7096, Train Acc: 0.9282, Val Loss: 1.7112, Val Acc: 0.9295\n",
            "Epoch [3456/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7132, Val Acc: 0.9288\n",
            "Epoch [3457/4000], Train Loss: 1.7090, Train Acc: 0.9311, Val Loss: 1.7110, Val Acc: 0.9321\n",
            "Epoch [3458/4000], Train Loss: 1.7082, Train Acc: 0.9314, Val Loss: 1.7075, Val Acc: 0.9314\n",
            "Epoch [3459/4000], Train Loss: 1.7090, Train Acc: 0.9296, Val Loss: 1.7064, Val Acc: 0.9308\n",
            "Epoch [3460/4000], Train Loss: 1.7105, Train Acc: 0.9287, Val Loss: 1.7123, Val Acc: 0.9256\n",
            "Epoch [3461/4000], Train Loss: 1.7093, Train Acc: 0.9290, Val Loss: 1.7086, Val Acc: 0.9327\n",
            "Epoch [3462/4000], Train Loss: 1.7090, Train Acc: 0.9301, Val Loss: 1.7184, Val Acc: 0.9237\n",
            "Epoch [3463/4000], Train Loss: 1.7099, Train Acc: 0.9287, Val Loss: 1.7065, Val Acc: 0.9308\n",
            "Epoch [3464/4000], Train Loss: 1.7091, Train Acc: 0.9303, Val Loss: 1.7086, Val Acc: 0.9314\n",
            "Epoch [3465/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7074, Val Acc: 0.9295\n",
            "Epoch [3466/4000], Train Loss: 1.7094, Train Acc: 0.9308, Val Loss: 1.7082, Val Acc: 0.9308\n",
            "Epoch [3467/4000], Train Loss: 1.7106, Train Acc: 0.9293, Val Loss: 1.7069, Val Acc: 0.9301\n",
            "Epoch [3468/4000], Train Loss: 1.7088, Train Acc: 0.9311, Val Loss: 1.7053, Val Acc: 0.9295\n",
            "Epoch [3469/4000], Train Loss: 1.7088, Train Acc: 0.9282, Val Loss: 1.7110, Val Acc: 0.9314\n",
            "Epoch [3470/4000], Train Loss: 1.7087, Train Acc: 0.9303, Val Loss: 1.7070, Val Acc: 0.9340\n",
            "Epoch [3471/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [3472/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7060, Val Acc: 0.9327\n",
            "Epoch [3473/4000], Train Loss: 1.7089, Train Acc: 0.9300, Val Loss: 1.7100, Val Acc: 0.9288\n",
            "Epoch [3474/4000], Train Loss: 1.7091, Train Acc: 0.9285, Val Loss: 1.7116, Val Acc: 0.9295\n",
            "Epoch [3475/4000], Train Loss: 1.7093, Train Acc: 0.9288, Val Loss: 1.7086, Val Acc: 0.9276\n",
            "Epoch [3476/4000], Train Loss: 1.7092, Train Acc: 0.9293, Val Loss: 1.7127, Val Acc: 0.9269\n",
            "Epoch [3477/4000], Train Loss: 1.7095, Train Acc: 0.9304, Val Loss: 1.7093, Val Acc: 0.9282\n",
            "Epoch [3478/4000], Train Loss: 1.7080, Train Acc: 0.9298, Val Loss: 1.7106, Val Acc: 0.9321\n",
            "Epoch [3479/4000], Train Loss: 1.7112, Train Acc: 0.9285, Val Loss: 1.7108, Val Acc: 0.9301\n",
            "Epoch [3480/4000], Train Loss: 1.7099, Train Acc: 0.9301, Val Loss: 1.7172, Val Acc: 0.9199\n",
            "Epoch [3481/4000], Train Loss: 1.7113, Train Acc: 0.9284, Val Loss: 1.7119, Val Acc: 0.9256\n",
            "Epoch [3482/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7084, Val Acc: 0.9308\n",
            "Epoch [3483/4000], Train Loss: 1.7100, Train Acc: 0.9296, Val Loss: 1.7073, Val Acc: 0.9295\n",
            "Epoch [3484/4000], Train Loss: 1.7098, Train Acc: 0.9292, Val Loss: 1.7088, Val Acc: 0.9314\n",
            "Epoch [3485/4000], Train Loss: 1.7100, Train Acc: 0.9276, Val Loss: 1.7070, Val Acc: 0.9321\n",
            "Epoch [3486/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7079, Val Acc: 0.9314\n",
            "Epoch [3487/4000], Train Loss: 1.7094, Train Acc: 0.9288, Val Loss: 1.7098, Val Acc: 0.9333\n",
            "Epoch [3488/4000], Train Loss: 1.7090, Train Acc: 0.9309, Val Loss: 1.7099, Val Acc: 0.9288\n",
            "Epoch [3489/4000], Train Loss: 1.7092, Train Acc: 0.9296, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [3490/4000], Train Loss: 1.7085, Train Acc: 0.9309, Val Loss: 1.7106, Val Acc: 0.9308\n",
            "Epoch [3491/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [3492/4000], Train Loss: 1.7106, Train Acc: 0.9292, Val Loss: 1.7138, Val Acc: 0.9314\n",
            "Epoch [3493/4000], Train Loss: 1.7119, Train Acc: 0.9269, Val Loss: 1.7163, Val Acc: 0.9218\n",
            "Epoch [3494/4000], Train Loss: 1.7102, Train Acc: 0.9280, Val Loss: 1.7080, Val Acc: 0.9308\n",
            "Epoch [3495/4000], Train Loss: 1.7098, Train Acc: 0.9295, Val Loss: 1.7102, Val Acc: 0.9321\n",
            "Epoch [3496/4000], Train Loss: 1.7088, Train Acc: 0.9287, Val Loss: 1.7181, Val Acc: 0.9263\n",
            "Epoch [3497/4000], Train Loss: 1.7091, Train Acc: 0.9290, Val Loss: 1.7071, Val Acc: 0.9321\n",
            "Epoch [3498/4000], Train Loss: 1.7090, Train Acc: 0.9309, Val Loss: 1.7068, Val Acc: 0.9314\n",
            "Epoch [3499/4000], Train Loss: 1.7083, Train Acc: 0.9300, Val Loss: 1.7110, Val Acc: 0.9308\n",
            "Epoch [3500/4000], Train Loss: 1.7096, Train Acc: 0.9290, Val Loss: 1.7081, Val Acc: 0.9321\n",
            "Epoch [3501/4000], Train Loss: 1.7110, Train Acc: 0.9282, Val Loss: 1.7089, Val Acc: 0.9288\n",
            "Epoch [3502/4000], Train Loss: 1.7101, Train Acc: 0.9282, Val Loss: 1.7132, Val Acc: 0.9282\n",
            "Epoch [3503/4000], Train Loss: 1.7096, Train Acc: 0.9300, Val Loss: 1.7086, Val Acc: 0.9321\n",
            "Epoch [3504/4000], Train Loss: 1.7101, Train Acc: 0.9282, Val Loss: 1.7083, Val Acc: 0.9327\n",
            "Epoch [3505/4000], Train Loss: 1.7120, Train Acc: 0.9264, Val Loss: 1.7189, Val Acc: 0.9167\n",
            "Epoch [3506/4000], Train Loss: 1.7099, Train Acc: 0.9285, Val Loss: 1.7131, Val Acc: 0.9295\n",
            "Epoch [3507/4000], Train Loss: 1.7093, Train Acc: 0.9293, Val Loss: 1.7068, Val Acc: 0.9340\n",
            "Epoch [3508/4000], Train Loss: 1.7084, Train Acc: 0.9300, Val Loss: 1.7089, Val Acc: 0.9301\n",
            "Epoch [3509/4000], Train Loss: 1.7094, Train Acc: 0.9301, Val Loss: 1.7065, Val Acc: 0.9321\n",
            "Epoch [3510/4000], Train Loss: 1.7103, Train Acc: 0.9285, Val Loss: 1.7131, Val Acc: 0.9288\n",
            "Epoch [3511/4000], Train Loss: 1.7102, Train Acc: 0.9300, Val Loss: 1.7107, Val Acc: 0.9282\n",
            "Epoch [3512/4000], Train Loss: 1.7086, Train Acc: 0.9293, Val Loss: 1.7076, Val Acc: 0.9321\n",
            "Epoch [3513/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7079, Val Acc: 0.9314\n",
            "Epoch [3514/4000], Train Loss: 1.7109, Train Acc: 0.9287, Val Loss: 1.7103, Val Acc: 0.9314\n",
            "Epoch [3515/4000], Train Loss: 1.7088, Train Acc: 0.9303, Val Loss: 1.7111, Val Acc: 0.9295\n",
            "Epoch [3516/4000], Train Loss: 1.7116, Train Acc: 0.9290, Val Loss: 1.7123, Val Acc: 0.9263\n",
            "Epoch [3517/4000], Train Loss: 1.7097, Train Acc: 0.9293, Val Loss: 1.7088, Val Acc: 0.9301\n",
            "Epoch [3518/4000], Train Loss: 1.7093, Train Acc: 0.9311, Val Loss: 1.7088, Val Acc: 0.9321\n",
            "Epoch [3519/4000], Train Loss: 1.7080, Train Acc: 0.9296, Val Loss: 1.7058, Val Acc: 0.9340\n",
            "Epoch [3520/4000], Train Loss: 1.7101, Train Acc: 0.9293, Val Loss: 1.7078, Val Acc: 0.9308\n",
            "Epoch [3521/4000], Train Loss: 1.7110, Train Acc: 0.9293, Val Loss: 1.7073, Val Acc: 0.9301\n",
            "Epoch [3522/4000], Train Loss: 1.7084, Train Acc: 0.9303, Val Loss: 1.7109, Val Acc: 0.9288\n",
            "Epoch [3523/4000], Train Loss: 1.7133, Train Acc: 0.9266, Val Loss: 1.7069, Val Acc: 0.9308\n",
            "Epoch [3524/4000], Train Loss: 1.7096, Train Acc: 0.9288, Val Loss: 1.7096, Val Acc: 0.9301\n",
            "Epoch [3525/4000], Train Loss: 1.7088, Train Acc: 0.9292, Val Loss: 1.7117, Val Acc: 0.9295\n",
            "Epoch [3526/4000], Train Loss: 1.7113, Train Acc: 0.9277, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [3527/4000], Train Loss: 1.7107, Train Acc: 0.9290, Val Loss: 1.7195, Val Acc: 0.9212\n",
            "Epoch [3528/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7089, Val Acc: 0.9321\n",
            "Epoch [3529/4000], Train Loss: 1.7097, Train Acc: 0.9295, Val Loss: 1.7095, Val Acc: 0.9288\n",
            "Epoch [3530/4000], Train Loss: 1.7098, Train Acc: 0.9293, Val Loss: 1.7104, Val Acc: 0.9308\n",
            "Epoch [3531/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7068, Val Acc: 0.9308\n",
            "Epoch [3532/4000], Train Loss: 1.7115, Train Acc: 0.9285, Val Loss: 1.7071, Val Acc: 0.9282\n",
            "Epoch [3533/4000], Train Loss: 1.7096, Train Acc: 0.9290, Val Loss: 1.7072, Val Acc: 0.9321\n",
            "Epoch [3534/4000], Train Loss: 1.7087, Train Acc: 0.9306, Val Loss: 1.7050, Val Acc: 0.9327\n",
            "Epoch [3535/4000], Train Loss: 1.7108, Train Acc: 0.9284, Val Loss: 1.7131, Val Acc: 0.9263\n",
            "Epoch [3536/4000], Train Loss: 1.7093, Train Acc: 0.9292, Val Loss: 1.7096, Val Acc: 0.9301\n",
            "Epoch [3537/4000], Train Loss: 1.7082, Train Acc: 0.9300, Val Loss: 1.7090, Val Acc: 0.9314\n",
            "Epoch [3538/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7090, Val Acc: 0.9321\n",
            "Epoch [3539/4000], Train Loss: 1.7103, Train Acc: 0.9301, Val Loss: 1.7194, Val Acc: 0.9231\n",
            "Epoch [3540/4000], Train Loss: 1.7097, Train Acc: 0.9301, Val Loss: 1.7175, Val Acc: 0.9263\n",
            "Epoch [3541/4000], Train Loss: 1.7093, Train Acc: 0.9293, Val Loss: 1.7123, Val Acc: 0.9308\n",
            "Epoch [3542/4000], Train Loss: 1.7114, Train Acc: 0.9271, Val Loss: 1.7095, Val Acc: 0.9288\n",
            "Epoch [3543/4000], Train Loss: 1.7091, Train Acc: 0.9279, Val Loss: 1.7094, Val Acc: 0.9301\n",
            "Epoch [3544/4000], Train Loss: 1.7104, Train Acc: 0.9280, Val Loss: 1.7120, Val Acc: 0.9295\n",
            "Epoch [3545/4000], Train Loss: 1.7097, Train Acc: 0.9293, Val Loss: 1.7078, Val Acc: 0.9308\n",
            "Epoch [3546/4000], Train Loss: 1.7089, Train Acc: 0.9303, Val Loss: 1.7076, Val Acc: 0.9314\n",
            "Epoch [3547/4000], Train Loss: 1.7079, Train Acc: 0.9306, Val Loss: 1.7114, Val Acc: 0.9288\n",
            "Epoch [3548/4000], Train Loss: 1.7090, Train Acc: 0.9309, Val Loss: 1.7110, Val Acc: 0.9276\n",
            "Epoch [3549/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7132, Val Acc: 0.9301\n",
            "Epoch [3550/4000], Train Loss: 1.7093, Train Acc: 0.9308, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [3551/4000], Train Loss: 1.7097, Train Acc: 0.9298, Val Loss: 1.7108, Val Acc: 0.9321\n",
            "Epoch [3552/4000], Train Loss: 1.7104, Train Acc: 0.9303, Val Loss: 1.7094, Val Acc: 0.9308\n",
            "Epoch [3553/4000], Train Loss: 1.7089, Train Acc: 0.9313, Val Loss: 1.7103, Val Acc: 0.9321\n",
            "Epoch [3554/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7105, Val Acc: 0.9314\n",
            "Epoch [3555/4000], Train Loss: 1.7103, Train Acc: 0.9308, Val Loss: 1.7118, Val Acc: 0.9321\n",
            "Epoch [3556/4000], Train Loss: 1.7089, Train Acc: 0.9304, Val Loss: 1.7128, Val Acc: 0.9256\n",
            "Epoch [3557/4000], Train Loss: 1.7092, Train Acc: 0.9295, Val Loss: 1.7098, Val Acc: 0.9282\n",
            "Epoch [3558/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7075, Val Acc: 0.9321\n",
            "Epoch [3559/4000], Train Loss: 1.7112, Train Acc: 0.9288, Val Loss: 1.7125, Val Acc: 0.9295\n",
            "Epoch [3560/4000], Train Loss: 1.7091, Train Acc: 0.9292, Val Loss: 1.7093, Val Acc: 0.9327\n",
            "Epoch [3561/4000], Train Loss: 1.7107, Train Acc: 0.9288, Val Loss: 1.7095, Val Acc: 0.9321\n",
            "Epoch [3562/4000], Train Loss: 1.7087, Train Acc: 0.9300, Val Loss: 1.7066, Val Acc: 0.9333\n",
            "Epoch [3563/4000], Train Loss: 1.7103, Train Acc: 0.9301, Val Loss: 1.7088, Val Acc: 0.9327\n",
            "Epoch [3564/4000], Train Loss: 1.7096, Train Acc: 0.9292, Val Loss: 1.7069, Val Acc: 0.9288\n",
            "Epoch [3565/4000], Train Loss: 1.7083, Train Acc: 0.9300, Val Loss: 1.7094, Val Acc: 0.9340\n",
            "Epoch [3566/4000], Train Loss: 1.7106, Train Acc: 0.9287, Val Loss: 1.7068, Val Acc: 0.9301\n",
            "Epoch [3567/4000], Train Loss: 1.7087, Train Acc: 0.9290, Val Loss: 1.7049, Val Acc: 0.9321\n",
            "Epoch [3568/4000], Train Loss: 1.7088, Train Acc: 0.9295, Val Loss: 1.7109, Val Acc: 0.9288\n",
            "Epoch [3569/4000], Train Loss: 1.7099, Train Acc: 0.9306, Val Loss: 1.7128, Val Acc: 0.9263\n",
            "Epoch [3570/4000], Train Loss: 1.7109, Train Acc: 0.9292, Val Loss: 1.7097, Val Acc: 0.9314\n",
            "Epoch [3571/4000], Train Loss: 1.7088, Train Acc: 0.9295, Val Loss: 1.7099, Val Acc: 0.9295\n",
            "Epoch [3572/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7076, Val Acc: 0.9288\n",
            "Epoch [3573/4000], Train Loss: 1.7085, Train Acc: 0.9306, Val Loss: 1.7066, Val Acc: 0.9314\n",
            "Epoch [3574/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7064, Val Acc: 0.9321\n",
            "Epoch [3575/4000], Train Loss: 1.7101, Train Acc: 0.9306, Val Loss: 1.7128, Val Acc: 0.9237\n",
            "Epoch [3576/4000], Train Loss: 1.7096, Train Acc: 0.9313, Val Loss: 1.7107, Val Acc: 0.9301\n",
            "Epoch [3577/4000], Train Loss: 1.7095, Train Acc: 0.9295, Val Loss: 1.7071, Val Acc: 0.9333\n",
            "Epoch [3578/4000], Train Loss: 1.7093, Train Acc: 0.9288, Val Loss: 1.7128, Val Acc: 0.9276\n",
            "Epoch [3579/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7099, Val Acc: 0.9340\n",
            "Epoch [3580/4000], Train Loss: 1.7098, Train Acc: 0.9288, Val Loss: 1.7105, Val Acc: 0.9301\n",
            "Epoch [3581/4000], Train Loss: 1.7098, Train Acc: 0.9290, Val Loss: 1.7085, Val Acc: 0.9301\n",
            "Epoch [3582/4000], Train Loss: 1.7095, Train Acc: 0.9295, Val Loss: 1.7078, Val Acc: 0.9340\n",
            "Epoch [3583/4000], Train Loss: 1.7089, Train Acc: 0.9301, Val Loss: 1.7102, Val Acc: 0.9288\n",
            "Epoch [3584/4000], Train Loss: 1.7083, Train Acc: 0.9292, Val Loss: 1.7083, Val Acc: 0.9321\n",
            "Epoch [3585/4000], Train Loss: 1.7096, Train Acc: 0.9287, Val Loss: 1.7261, Val Acc: 0.9128\n",
            "Epoch [3586/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7062, Val Acc: 0.9321\n",
            "Epoch [3587/4000], Train Loss: 1.7108, Train Acc: 0.9279, Val Loss: 1.7100, Val Acc: 0.9301\n",
            "Epoch [3588/4000], Train Loss: 1.7086, Train Acc: 0.9304, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [3589/4000], Train Loss: 1.7123, Train Acc: 0.9276, Val Loss: 1.7107, Val Acc: 0.9308\n",
            "Epoch [3590/4000], Train Loss: 1.7078, Train Acc: 0.9303, Val Loss: 1.7054, Val Acc: 0.9314\n",
            "Epoch [3591/4000], Train Loss: 1.7087, Train Acc: 0.9306, Val Loss: 1.7065, Val Acc: 0.9308\n",
            "Epoch [3592/4000], Train Loss: 1.7079, Train Acc: 0.9303, Val Loss: 1.7050, Val Acc: 0.9308\n",
            "Epoch [3593/4000], Train Loss: 1.7101, Train Acc: 0.9301, Val Loss: 1.7090, Val Acc: 0.9333\n",
            "Epoch [3594/4000], Train Loss: 1.7109, Train Acc: 0.9292, Val Loss: 1.7102, Val Acc: 0.9288\n",
            "Epoch [3595/4000], Train Loss: 1.7091, Train Acc: 0.9293, Val Loss: 1.7114, Val Acc: 0.9295\n",
            "Epoch [3596/4000], Train Loss: 1.7101, Train Acc: 0.9308, Val Loss: 1.7064, Val Acc: 0.9314\n",
            "Epoch [3597/4000], Train Loss: 1.7113, Train Acc: 0.9287, Val Loss: 1.7068, Val Acc: 0.9333\n",
            "Epoch [3598/4000], Train Loss: 1.7102, Train Acc: 0.9287, Val Loss: 1.7092, Val Acc: 0.9314\n",
            "Epoch [3599/4000], Train Loss: 1.7078, Train Acc: 0.9304, Val Loss: 1.7103, Val Acc: 0.9288\n",
            "Epoch [3600/4000], Train Loss: 1.7111, Train Acc: 0.9292, Val Loss: 1.7114, Val Acc: 0.9288\n",
            "Epoch [3601/4000], Train Loss: 1.7103, Train Acc: 0.9288, Val Loss: 1.7238, Val Acc: 0.9135\n",
            "Epoch [3602/4000], Train Loss: 1.7106, Train Acc: 0.9300, Val Loss: 1.7114, Val Acc: 0.9276\n",
            "Epoch [3603/4000], Train Loss: 1.7098, Train Acc: 0.9287, Val Loss: 1.7123, Val Acc: 0.9314\n",
            "Epoch [3604/4000], Train Loss: 1.7117, Train Acc: 0.9279, Val Loss: 1.7105, Val Acc: 0.9321\n",
            "Epoch [3605/4000], Train Loss: 1.7096, Train Acc: 0.9288, Val Loss: 1.7092, Val Acc: 0.9340\n",
            "Epoch [3606/4000], Train Loss: 1.7089, Train Acc: 0.9292, Val Loss: 1.7171, Val Acc: 0.9244\n",
            "Epoch [3607/4000], Train Loss: 1.7097, Train Acc: 0.9284, Val Loss: 1.7119, Val Acc: 0.9282\n",
            "Epoch [3608/4000], Train Loss: 1.7096, Train Acc: 0.9303, Val Loss: 1.7104, Val Acc: 0.9301\n",
            "Epoch [3609/4000], Train Loss: 1.7094, Train Acc: 0.9287, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [3610/4000], Train Loss: 1.7115, Train Acc: 0.9287, Val Loss: 1.7095, Val Acc: 0.9301\n",
            "Epoch [3611/4000], Train Loss: 1.7133, Train Acc: 0.9279, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [3612/4000], Train Loss: 1.7082, Train Acc: 0.9295, Val Loss: 1.7086, Val Acc: 0.9321\n",
            "Epoch [3613/4000], Train Loss: 1.7080, Train Acc: 0.9301, Val Loss: 1.7096, Val Acc: 0.9288\n",
            "Epoch [3614/4000], Train Loss: 1.7088, Train Acc: 0.9296, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [3615/4000], Train Loss: 1.7095, Train Acc: 0.9295, Val Loss: 1.7075, Val Acc: 0.9301\n",
            "Epoch [3616/4000], Train Loss: 1.7101, Train Acc: 0.9298, Val Loss: 1.7114, Val Acc: 0.9288\n",
            "Epoch [3617/4000], Train Loss: 1.7093, Train Acc: 0.9285, Val Loss: 1.7093, Val Acc: 0.9256\n",
            "Epoch [3618/4000], Train Loss: 1.7097, Train Acc: 0.9290, Val Loss: 1.7101, Val Acc: 0.9314\n",
            "Epoch [3619/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7084, Val Acc: 0.9327\n",
            "Epoch [3620/4000], Train Loss: 1.7089, Train Acc: 0.9304, Val Loss: 1.7079, Val Acc: 0.9314\n",
            "Epoch [3621/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7101, Val Acc: 0.9295\n",
            "Epoch [3622/4000], Train Loss: 1.7089, Train Acc: 0.9306, Val Loss: 1.7066, Val Acc: 0.9308\n",
            "Epoch [3623/4000], Train Loss: 1.7093, Train Acc: 0.9293, Val Loss: 1.7073, Val Acc: 0.9327\n",
            "Epoch [3624/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7067, Val Acc: 0.9327\n",
            "Epoch [3625/4000], Train Loss: 1.7093, Train Acc: 0.9298, Val Loss: 1.7080, Val Acc: 0.9327\n",
            "Epoch [3626/4000], Train Loss: 1.7098, Train Acc: 0.9287, Val Loss: 1.7114, Val Acc: 0.9288\n",
            "Epoch [3627/4000], Train Loss: 1.7083, Train Acc: 0.9311, Val Loss: 1.7093, Val Acc: 0.9333\n",
            "Epoch [3628/4000], Train Loss: 1.7088, Train Acc: 0.9296, Val Loss: 1.7100, Val Acc: 0.9295\n",
            "Epoch [3629/4000], Train Loss: 1.7097, Train Acc: 0.9303, Val Loss: 1.7081, Val Acc: 0.9321\n",
            "Epoch [3630/4000], Train Loss: 1.7079, Train Acc: 0.9304, Val Loss: 1.7111, Val Acc: 0.9256\n",
            "Epoch [3631/4000], Train Loss: 1.7104, Train Acc: 0.9296, Val Loss: 1.7080, Val Acc: 0.9295\n",
            "Epoch [3632/4000], Train Loss: 1.7079, Train Acc: 0.9303, Val Loss: 1.7108, Val Acc: 0.9269\n",
            "Epoch [3633/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7135, Val Acc: 0.9256\n",
            "Epoch [3634/4000], Train Loss: 1.7099, Train Acc: 0.9284, Val Loss: 1.7062, Val Acc: 0.9340\n",
            "Epoch [3635/4000], Train Loss: 1.7087, Train Acc: 0.9296, Val Loss: 1.7122, Val Acc: 0.9321\n",
            "Epoch [3636/4000], Train Loss: 1.7109, Train Acc: 0.9295, Val Loss: 1.7132, Val Acc: 0.9250\n",
            "Epoch [3637/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7148, Val Acc: 0.9218\n",
            "Epoch [3638/4000], Train Loss: 1.7099, Train Acc: 0.9301, Val Loss: 1.7071, Val Acc: 0.9327\n",
            "Epoch [3639/4000], Train Loss: 1.7089, Train Acc: 0.9303, Val Loss: 1.7069, Val Acc: 0.9295\n",
            "Epoch [3640/4000], Train Loss: 1.7089, Train Acc: 0.9313, Val Loss: 1.7086, Val Acc: 0.9308\n",
            "Epoch [3641/4000], Train Loss: 1.7095, Train Acc: 0.9296, Val Loss: 1.7097, Val Acc: 0.9282\n",
            "Epoch [3642/4000], Train Loss: 1.7103, Train Acc: 0.9303, Val Loss: 1.7049, Val Acc: 0.9321\n",
            "Epoch [3643/4000], Train Loss: 1.7083, Train Acc: 0.9304, Val Loss: 1.7079, Val Acc: 0.9282\n",
            "Epoch [3644/4000], Train Loss: 1.7091, Train Acc: 0.9292, Val Loss: 1.7143, Val Acc: 0.9295\n",
            "Epoch [3645/4000], Train Loss: 1.7082, Train Acc: 0.9308, Val Loss: 1.7060, Val Acc: 0.9314\n",
            "Epoch [3646/4000], Train Loss: 1.7081, Train Acc: 0.9306, Val Loss: 1.7133, Val Acc: 0.9308\n",
            "Epoch [3647/4000], Train Loss: 1.7138, Train Acc: 0.9258, Val Loss: 1.7091, Val Acc: 0.9314\n",
            "Epoch [3648/4000], Train Loss: 1.7098, Train Acc: 0.9292, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [3649/4000], Train Loss: 1.7090, Train Acc: 0.9303, Val Loss: 1.7151, Val Acc: 0.9231\n",
            "Epoch [3650/4000], Train Loss: 1.7096, Train Acc: 0.9290, Val Loss: 1.7100, Val Acc: 0.9314\n",
            "Epoch [3651/4000], Train Loss: 1.7083, Train Acc: 0.9301, Val Loss: 1.7085, Val Acc: 0.9314\n",
            "Epoch [3652/4000], Train Loss: 1.7101, Train Acc: 0.9284, Val Loss: 1.7124, Val Acc: 0.9301\n",
            "Epoch [3653/4000], Train Loss: 1.7080, Train Acc: 0.9313, Val Loss: 1.7085, Val Acc: 0.9327\n",
            "Epoch [3654/4000], Train Loss: 1.7090, Train Acc: 0.9304, Val Loss: 1.7103, Val Acc: 0.9327\n",
            "Epoch [3655/4000], Train Loss: 1.7099, Train Acc: 0.9306, Val Loss: 1.7151, Val Acc: 0.9321\n",
            "Epoch [3656/4000], Train Loss: 1.7095, Train Acc: 0.9303, Val Loss: 1.7096, Val Acc: 0.9269\n",
            "Epoch [3657/4000], Train Loss: 1.7091, Train Acc: 0.9285, Val Loss: 1.7071, Val Acc: 0.9333\n",
            "Epoch [3658/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7102, Val Acc: 0.9263\n",
            "Epoch [3659/4000], Train Loss: 1.7099, Train Acc: 0.9304, Val Loss: 1.7063, Val Acc: 0.9301\n",
            "Epoch [3660/4000], Train Loss: 1.7094, Train Acc: 0.9282, Val Loss: 1.7091, Val Acc: 0.9301\n",
            "Epoch [3661/4000], Train Loss: 1.7101, Train Acc: 0.9284, Val Loss: 1.7089, Val Acc: 0.9308\n",
            "Epoch [3662/4000], Train Loss: 1.7084, Train Acc: 0.9288, Val Loss: 1.7053, Val Acc: 0.9314\n",
            "Epoch [3663/4000], Train Loss: 1.7100, Train Acc: 0.9292, Val Loss: 1.7068, Val Acc: 0.9314\n",
            "Epoch [3664/4000], Train Loss: 1.7103, Train Acc: 0.9293, Val Loss: 1.7092, Val Acc: 0.9269\n",
            "Epoch [3665/4000], Train Loss: 1.7101, Train Acc: 0.9295, Val Loss: 1.7060, Val Acc: 0.9314\n",
            "Epoch [3666/4000], Train Loss: 1.7090, Train Acc: 0.9304, Val Loss: 1.7119, Val Acc: 0.9269\n",
            "Epoch [3667/4000], Train Loss: 1.7088, Train Acc: 0.9301, Val Loss: 1.7074, Val Acc: 0.9321\n",
            "Epoch [3668/4000], Train Loss: 1.7087, Train Acc: 0.9311, Val Loss: 1.7071, Val Acc: 0.9321\n",
            "Epoch [3669/4000], Train Loss: 1.7101, Train Acc: 0.9287, Val Loss: 1.7091, Val Acc: 0.9327\n",
            "Epoch [3670/4000], Train Loss: 1.7084, Train Acc: 0.9295, Val Loss: 1.7077, Val Acc: 0.9288\n",
            "Epoch [3671/4000], Train Loss: 1.7085, Train Acc: 0.9298, Val Loss: 1.7100, Val Acc: 0.9282\n",
            "Epoch [3672/4000], Train Loss: 1.7093, Train Acc: 0.9308, Val Loss: 1.7112, Val Acc: 0.9282\n",
            "Epoch [3673/4000], Train Loss: 1.7092, Train Acc: 0.9300, Val Loss: 1.7118, Val Acc: 0.9282\n",
            "Epoch [3674/4000], Train Loss: 1.7114, Train Acc: 0.9284, Val Loss: 1.7079, Val Acc: 0.9321\n",
            "Epoch [3675/4000], Train Loss: 1.7087, Train Acc: 0.9309, Val Loss: 1.7096, Val Acc: 0.9282\n",
            "Epoch [3676/4000], Train Loss: 1.7100, Train Acc: 0.9296, Val Loss: 1.7100, Val Acc: 0.9295\n",
            "Epoch [3677/4000], Train Loss: 1.7082, Train Acc: 0.9300, Val Loss: 1.7087, Val Acc: 0.9340\n",
            "Epoch [3678/4000], Train Loss: 1.7104, Train Acc: 0.9288, Val Loss: 1.7083, Val Acc: 0.9269\n",
            "Epoch [3679/4000], Train Loss: 1.7103, Train Acc: 0.9292, Val Loss: 1.7077, Val Acc: 0.9301\n",
            "Epoch [3680/4000], Train Loss: 1.7093, Train Acc: 0.9288, Val Loss: 1.7090, Val Acc: 0.9327\n",
            "Epoch [3681/4000], Train Loss: 1.7086, Train Acc: 0.9317, Val Loss: 1.7087, Val Acc: 0.9295\n",
            "Epoch [3682/4000], Train Loss: 1.7095, Train Acc: 0.9303, Val Loss: 1.7076, Val Acc: 0.9314\n",
            "Epoch [3683/4000], Train Loss: 1.7087, Train Acc: 0.9304, Val Loss: 1.7112, Val Acc: 0.9269\n",
            "Epoch [3684/4000], Train Loss: 1.7081, Train Acc: 0.9306, Val Loss: 1.7109, Val Acc: 0.9301\n",
            "Epoch [3685/4000], Train Loss: 1.7104, Train Acc: 0.9288, Val Loss: 1.7079, Val Acc: 0.9314\n",
            "Epoch [3686/4000], Train Loss: 1.7076, Train Acc: 0.9306, Val Loss: 1.7062, Val Acc: 0.9314\n",
            "Epoch [3687/4000], Train Loss: 1.7089, Train Acc: 0.9296, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [3688/4000], Train Loss: 1.7086, Train Acc: 0.9306, Val Loss: 1.7078, Val Acc: 0.9321\n",
            "Epoch [3689/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7089, Val Acc: 0.9333\n",
            "Epoch [3690/4000], Train Loss: 1.7081, Train Acc: 0.9311, Val Loss: 1.7078, Val Acc: 0.9288\n",
            "Epoch [3691/4000], Train Loss: 1.7103, Train Acc: 0.9303, Val Loss: 1.7095, Val Acc: 0.9301\n",
            "Epoch [3692/4000], Train Loss: 1.7094, Train Acc: 0.9301, Val Loss: 1.7078, Val Acc: 0.9308\n",
            "Epoch [3693/4000], Train Loss: 1.7103, Train Acc: 0.9269, Val Loss: 1.7074, Val Acc: 0.9333\n",
            "Epoch [3694/4000], Train Loss: 1.7098, Train Acc: 0.9298, Val Loss: 1.7123, Val Acc: 0.9269\n",
            "Epoch [3695/4000], Train Loss: 1.7094, Train Acc: 0.9296, Val Loss: 1.7098, Val Acc: 0.9288\n",
            "Epoch [3696/4000], Train Loss: 1.7088, Train Acc: 0.9301, Val Loss: 1.7189, Val Acc: 0.9212\n",
            "Epoch [3697/4000], Train Loss: 1.7094, Train Acc: 0.9301, Val Loss: 1.7096, Val Acc: 0.9276\n",
            "Epoch [3698/4000], Train Loss: 1.7098, Train Acc: 0.9292, Val Loss: 1.7084, Val Acc: 0.9327\n",
            "Epoch [3699/4000], Train Loss: 1.7091, Train Acc: 0.9279, Val Loss: 1.7091, Val Acc: 0.9276\n",
            "Epoch [3700/4000], Train Loss: 1.7100, Train Acc: 0.9284, Val Loss: 1.7118, Val Acc: 0.9282\n",
            "Epoch [3701/4000], Train Loss: 1.7087, Train Acc: 0.9292, Val Loss: 1.7095, Val Acc: 0.9288\n",
            "Epoch [3702/4000], Train Loss: 1.7085, Train Acc: 0.9306, Val Loss: 1.7069, Val Acc: 0.9321\n",
            "Epoch [3703/4000], Train Loss: 1.7093, Train Acc: 0.9287, Val Loss: 1.7087, Val Acc: 0.9301\n",
            "Epoch [3704/4000], Train Loss: 1.7097, Train Acc: 0.9301, Val Loss: 1.7097, Val Acc: 0.9301\n",
            "Epoch [3705/4000], Train Loss: 1.7090, Train Acc: 0.9295, Val Loss: 1.7084, Val Acc: 0.9340\n",
            "Epoch [3706/4000], Train Loss: 1.7089, Train Acc: 0.9308, Val Loss: 1.7083, Val Acc: 0.9301\n",
            "Epoch [3707/4000], Train Loss: 1.7101, Train Acc: 0.9288, Val Loss: 1.7052, Val Acc: 0.9301\n",
            "Epoch [3708/4000], Train Loss: 1.7086, Train Acc: 0.9292, Val Loss: 1.7092, Val Acc: 0.9314\n",
            "Epoch [3709/4000], Train Loss: 1.7094, Train Acc: 0.9284, Val Loss: 1.7192, Val Acc: 0.9244\n",
            "Epoch [3710/4000], Train Loss: 1.7083, Train Acc: 0.9303, Val Loss: 1.7071, Val Acc: 0.9295\n",
            "Epoch [3711/4000], Train Loss: 1.7090, Train Acc: 0.9295, Val Loss: 1.7059, Val Acc: 0.9314\n",
            "Epoch [3712/4000], Train Loss: 1.7089, Train Acc: 0.9296, Val Loss: 1.7088, Val Acc: 0.9301\n",
            "Epoch [3713/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7189, Val Acc: 0.9192\n",
            "Epoch [3714/4000], Train Loss: 1.7120, Train Acc: 0.9288, Val Loss: 1.7068, Val Acc: 0.9333\n",
            "Epoch [3715/4000], Train Loss: 1.7100, Train Acc: 0.9303, Val Loss: 1.7074, Val Acc: 0.9314\n",
            "Epoch [3716/4000], Train Loss: 1.7114, Train Acc: 0.9287, Val Loss: 1.7086, Val Acc: 0.9288\n",
            "Epoch [3717/4000], Train Loss: 1.7084, Train Acc: 0.9301, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [3718/4000], Train Loss: 1.7095, Train Acc: 0.9295, Val Loss: 1.7093, Val Acc: 0.9276\n",
            "Epoch [3719/4000], Train Loss: 1.7107, Train Acc: 0.9288, Val Loss: 1.7057, Val Acc: 0.9314\n",
            "Epoch [3720/4000], Train Loss: 1.7099, Train Acc: 0.9292, Val Loss: 1.7140, Val Acc: 0.9231\n",
            "Epoch [3721/4000], Train Loss: 1.7088, Train Acc: 0.9288, Val Loss: 1.7100, Val Acc: 0.9288\n",
            "Epoch [3722/4000], Train Loss: 1.7093, Train Acc: 0.9311, Val Loss: 1.7129, Val Acc: 0.9250\n",
            "Epoch [3723/4000], Train Loss: 1.7105, Train Acc: 0.9290, Val Loss: 1.7116, Val Acc: 0.9282\n",
            "Epoch [3724/4000], Train Loss: 1.7093, Train Acc: 0.9303, Val Loss: 1.7143, Val Acc: 0.9276\n",
            "Epoch [3725/4000], Train Loss: 1.7073, Train Acc: 0.9316, Val Loss: 1.7111, Val Acc: 0.9295\n",
            "Epoch [3726/4000], Train Loss: 1.7092, Train Acc: 0.9298, Val Loss: 1.7095, Val Acc: 0.9288\n",
            "Epoch [3727/4000], Train Loss: 1.7086, Train Acc: 0.9295, Val Loss: 1.7106, Val Acc: 0.9301\n",
            "Epoch [3728/4000], Train Loss: 1.7102, Train Acc: 0.9293, Val Loss: 1.7080, Val Acc: 0.9333\n",
            "Epoch [3729/4000], Train Loss: 1.7100, Train Acc: 0.9284, Val Loss: 1.7094, Val Acc: 0.9321\n",
            "Epoch [3730/4000], Train Loss: 1.7095, Train Acc: 0.9292, Val Loss: 1.7119, Val Acc: 0.9282\n",
            "Epoch [3731/4000], Train Loss: 1.7085, Train Acc: 0.9296, Val Loss: 1.7128, Val Acc: 0.9301\n",
            "Epoch [3732/4000], Train Loss: 1.7112, Train Acc: 0.9282, Val Loss: 1.7050, Val Acc: 0.9340\n",
            "Epoch [3733/4000], Train Loss: 1.7089, Train Acc: 0.9295, Val Loss: 1.7054, Val Acc: 0.9327\n",
            "Epoch [3734/4000], Train Loss: 1.7093, Train Acc: 0.9314, Val Loss: 1.7113, Val Acc: 0.9282\n",
            "Epoch [3735/4000], Train Loss: 1.7085, Train Acc: 0.9301, Val Loss: 1.7081, Val Acc: 0.9308\n",
            "Epoch [3736/4000], Train Loss: 1.7089, Train Acc: 0.9308, Val Loss: 1.7092, Val Acc: 0.9321\n",
            "Epoch [3737/4000], Train Loss: 1.7088, Train Acc: 0.9303, Val Loss: 1.7071, Val Acc: 0.9288\n",
            "Epoch [3738/4000], Train Loss: 1.7085, Train Acc: 0.9300, Val Loss: 1.7106, Val Acc: 0.9288\n",
            "Epoch [3739/4000], Train Loss: 1.7106, Train Acc: 0.9295, Val Loss: 1.7152, Val Acc: 0.9269\n",
            "Epoch [3740/4000], Train Loss: 1.7099, Train Acc: 0.9295, Val Loss: 1.7104, Val Acc: 0.9333\n",
            "Epoch [3741/4000], Train Loss: 1.7087, Train Acc: 0.9285, Val Loss: 1.7231, Val Acc: 0.9212\n",
            "Epoch [3742/4000], Train Loss: 1.7109, Train Acc: 0.9276, Val Loss: 1.7091, Val Acc: 0.9314\n",
            "Epoch [3743/4000], Train Loss: 1.7091, Train Acc: 0.9288, Val Loss: 1.7103, Val Acc: 0.9314\n",
            "Epoch [3744/4000], Train Loss: 1.7080, Train Acc: 0.9306, Val Loss: 1.7051, Val Acc: 0.9333\n",
            "Epoch [3745/4000], Train Loss: 1.7083, Train Acc: 0.9314, Val Loss: 1.7097, Val Acc: 0.9321\n",
            "Epoch [3746/4000], Train Loss: 1.7087, Train Acc: 0.9298, Val Loss: 1.7070, Val Acc: 0.9321\n",
            "Epoch [3747/4000], Train Loss: 1.7086, Train Acc: 0.9298, Val Loss: 1.7090, Val Acc: 0.9282\n",
            "Epoch [3748/4000], Train Loss: 1.7096, Train Acc: 0.9284, Val Loss: 1.7141, Val Acc: 0.9308\n",
            "Epoch [3749/4000], Train Loss: 1.7114, Train Acc: 0.9284, Val Loss: 1.7051, Val Acc: 0.9321\n",
            "Epoch [3750/4000], Train Loss: 1.7080, Train Acc: 0.9298, Val Loss: 1.7063, Val Acc: 0.9295\n",
            "Epoch [3751/4000], Train Loss: 1.7103, Train Acc: 0.9290, Val Loss: 1.7081, Val Acc: 0.9327\n",
            "Epoch [3752/4000], Train Loss: 1.7113, Train Acc: 0.9272, Val Loss: 1.7226, Val Acc: 0.9135\n",
            "Epoch [3753/4000], Train Loss: 1.7095, Train Acc: 0.9287, Val Loss: 1.7070, Val Acc: 0.9288\n",
            "Epoch [3754/4000], Train Loss: 1.7082, Train Acc: 0.9300, Val Loss: 1.7105, Val Acc: 0.9282\n",
            "Epoch [3755/4000], Train Loss: 1.7098, Train Acc: 0.9290, Val Loss: 1.7114, Val Acc: 0.9282\n",
            "Epoch [3756/4000], Train Loss: 1.7086, Train Acc: 0.9301, Val Loss: 1.7071, Val Acc: 0.9301\n",
            "Epoch [3757/4000], Train Loss: 1.7083, Train Acc: 0.9300, Val Loss: 1.7087, Val Acc: 0.9333\n",
            "Epoch [3758/4000], Train Loss: 1.7075, Train Acc: 0.9309, Val Loss: 1.7097, Val Acc: 0.9276\n",
            "Epoch [3759/4000], Train Loss: 1.7111, Train Acc: 0.9280, Val Loss: 1.7080, Val Acc: 0.9308\n",
            "Epoch [3760/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7084, Val Acc: 0.9321\n",
            "Epoch [3761/4000], Train Loss: 1.7081, Train Acc: 0.9308, Val Loss: 1.7086, Val Acc: 0.9276\n",
            "Epoch [3762/4000], Train Loss: 1.7091, Train Acc: 0.9300, Val Loss: 1.7072, Val Acc: 0.9282\n",
            "Epoch [3763/4000], Train Loss: 1.7096, Train Acc: 0.9298, Val Loss: 1.7071, Val Acc: 0.9308\n",
            "Epoch [3764/4000], Train Loss: 1.7124, Train Acc: 0.9271, Val Loss: 1.7065, Val Acc: 0.9314\n",
            "Epoch [3765/4000], Train Loss: 1.7088, Train Acc: 0.9303, Val Loss: 1.7118, Val Acc: 0.9288\n",
            "Epoch [3766/4000], Train Loss: 1.7109, Train Acc: 0.9290, Val Loss: 1.7098, Val Acc: 0.9321\n",
            "Epoch [3767/4000], Train Loss: 1.7092, Train Acc: 0.9285, Val Loss: 1.7173, Val Acc: 0.9231\n",
            "Epoch [3768/4000], Train Loss: 1.7096, Train Acc: 0.9298, Val Loss: 1.7059, Val Acc: 0.9308\n",
            "Epoch [3769/4000], Train Loss: 1.7090, Train Acc: 0.9300, Val Loss: 1.7075, Val Acc: 0.9308\n",
            "Epoch [3770/4000], Train Loss: 1.7105, Train Acc: 0.9298, Val Loss: 1.7121, Val Acc: 0.9282\n",
            "Epoch [3771/4000], Train Loss: 1.7112, Train Acc: 0.9292, Val Loss: 1.7069, Val Acc: 0.9301\n",
            "Epoch [3772/4000], Train Loss: 1.7098, Train Acc: 0.9288, Val Loss: 1.7082, Val Acc: 0.9327\n",
            "Epoch [3773/4000], Train Loss: 1.7098, Train Acc: 0.9301, Val Loss: 1.7059, Val Acc: 0.9308\n",
            "Epoch [3774/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7084, Val Acc: 0.9308\n",
            "Epoch [3775/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7077, Val Acc: 0.9333\n",
            "Epoch [3776/4000], Train Loss: 1.7081, Train Acc: 0.9290, Val Loss: 1.7069, Val Acc: 0.9314\n",
            "Epoch [3777/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7072, Val Acc: 0.9314\n",
            "Epoch [3778/4000], Train Loss: 1.7080, Train Acc: 0.9301, Val Loss: 1.7070, Val Acc: 0.9340\n",
            "Epoch [3779/4000], Train Loss: 1.7094, Train Acc: 0.9293, Val Loss: 1.7064, Val Acc: 0.9301\n",
            "Epoch [3780/4000], Train Loss: 1.7093, Train Acc: 0.9288, Val Loss: 1.7093, Val Acc: 0.9301\n",
            "Epoch [3781/4000], Train Loss: 1.7090, Train Acc: 0.9296, Val Loss: 1.7058, Val Acc: 0.9327\n",
            "Epoch [3782/4000], Train Loss: 1.7085, Train Acc: 0.9296, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [3783/4000], Train Loss: 1.7090, Train Acc: 0.9303, Val Loss: 1.7128, Val Acc: 0.9250\n",
            "Epoch [3784/4000], Train Loss: 1.7085, Train Acc: 0.9295, Val Loss: 1.7076, Val Acc: 0.9308\n",
            "Epoch [3785/4000], Train Loss: 1.7089, Train Acc: 0.9298, Val Loss: 1.7065, Val Acc: 0.9346\n",
            "Epoch [3786/4000], Train Loss: 1.7090, Train Acc: 0.9296, Val Loss: 1.7073, Val Acc: 0.9301\n",
            "Epoch [3787/4000], Train Loss: 1.7084, Train Acc: 0.9293, Val Loss: 1.7067, Val Acc: 0.9321\n",
            "Epoch [3788/4000], Train Loss: 1.7089, Train Acc: 0.9300, Val Loss: 1.7105, Val Acc: 0.9288\n",
            "Epoch [3789/4000], Train Loss: 1.7090, Train Acc: 0.9306, Val Loss: 1.7077, Val Acc: 0.9308\n",
            "Epoch [3790/4000], Train Loss: 1.7095, Train Acc: 0.9280, Val Loss: 1.7081, Val Acc: 0.9288\n",
            "Epoch [3791/4000], Train Loss: 1.7083, Train Acc: 0.9316, Val Loss: 1.7131, Val Acc: 0.9276\n",
            "Epoch [3792/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7082, Val Acc: 0.9314\n",
            "Epoch [3793/4000], Train Loss: 1.7089, Train Acc: 0.9306, Val Loss: 1.7066, Val Acc: 0.9321\n",
            "Epoch [3794/4000], Train Loss: 1.7080, Train Acc: 0.9298, Val Loss: 1.7095, Val Acc: 0.9301\n",
            "Epoch [3795/4000], Train Loss: 1.7088, Train Acc: 0.9311, Val Loss: 1.7080, Val Acc: 0.9327\n",
            "Epoch [3796/4000], Train Loss: 1.7095, Train Acc: 0.9293, Val Loss: 1.7105, Val Acc: 0.9276\n",
            "Epoch [3797/4000], Train Loss: 1.7090, Train Acc: 0.9300, Val Loss: 1.7054, Val Acc: 0.9301\n",
            "Epoch [3798/4000], Train Loss: 1.7105, Train Acc: 0.9293, Val Loss: 1.7071, Val Acc: 0.9353\n",
            "Epoch [3799/4000], Train Loss: 1.7131, Train Acc: 0.9258, Val Loss: 1.7144, Val Acc: 0.9212\n",
            "Epoch [3800/4000], Train Loss: 1.7092, Train Acc: 0.9300, Val Loss: 1.7070, Val Acc: 0.9321\n",
            "Epoch [3801/4000], Train Loss: 1.7080, Train Acc: 0.9313, Val Loss: 1.7100, Val Acc: 0.9308\n",
            "Epoch [3802/4000], Train Loss: 1.7111, Train Acc: 0.9285, Val Loss: 1.7109, Val Acc: 0.9308\n",
            "Epoch [3803/4000], Train Loss: 1.7105, Train Acc: 0.9277, Val Loss: 1.7145, Val Acc: 0.9250\n",
            "Epoch [3804/4000], Train Loss: 1.7094, Train Acc: 0.9300, Val Loss: 1.7069, Val Acc: 0.9295\n",
            "Epoch [3805/4000], Train Loss: 1.7079, Train Acc: 0.9304, Val Loss: 1.7126, Val Acc: 0.9276\n",
            "Epoch [3806/4000], Train Loss: 1.7093, Train Acc: 0.9306, Val Loss: 1.7073, Val Acc: 0.9295\n",
            "Epoch [3807/4000], Train Loss: 1.7080, Train Acc: 0.9317, Val Loss: 1.7104, Val Acc: 0.9295\n",
            "Epoch [3808/4000], Train Loss: 1.7091, Train Acc: 0.9288, Val Loss: 1.7095, Val Acc: 0.9308\n",
            "Epoch [3809/4000], Train Loss: 1.7091, Train Acc: 0.9296, Val Loss: 1.7083, Val Acc: 0.9301\n",
            "Epoch [3810/4000], Train Loss: 1.7112, Train Acc: 0.9290, Val Loss: 1.7126, Val Acc: 0.9276\n",
            "Epoch [3811/4000], Train Loss: 1.7098, Train Acc: 0.9300, Val Loss: 1.7116, Val Acc: 0.9314\n",
            "Epoch [3812/4000], Train Loss: 1.7079, Train Acc: 0.9311, Val Loss: 1.7116, Val Acc: 0.9269\n",
            "Epoch [3813/4000], Train Loss: 1.7082, Train Acc: 0.9308, Val Loss: 1.7094, Val Acc: 0.9276\n",
            "Epoch [3814/4000], Train Loss: 1.7085, Train Acc: 0.9306, Val Loss: 1.7079, Val Acc: 0.9314\n",
            "Epoch [3815/4000], Train Loss: 1.7084, Train Acc: 0.9295, Val Loss: 1.7096, Val Acc: 0.9321\n",
            "Epoch [3816/4000], Train Loss: 1.7086, Train Acc: 0.9308, Val Loss: 1.7049, Val Acc: 0.9295\n",
            "Epoch [3817/4000], Train Loss: 1.7090, Train Acc: 0.9296, Val Loss: 1.7066, Val Acc: 0.9327\n",
            "Epoch [3818/4000], Train Loss: 1.7096, Train Acc: 0.9298, Val Loss: 1.7065, Val Acc: 0.9308\n",
            "Epoch [3819/4000], Train Loss: 1.7102, Train Acc: 0.9295, Val Loss: 1.7083, Val Acc: 0.9301\n",
            "Epoch [3820/4000], Train Loss: 1.7096, Train Acc: 0.9292, Val Loss: 1.7073, Val Acc: 0.9321\n",
            "Epoch [3821/4000], Train Loss: 1.7086, Train Acc: 0.9311, Val Loss: 1.7094, Val Acc: 0.9301\n",
            "Epoch [3822/4000], Train Loss: 1.7093, Train Acc: 0.9303, Val Loss: 1.7068, Val Acc: 0.9301\n",
            "Epoch [3823/4000], Train Loss: 1.7096, Train Acc: 0.9298, Val Loss: 1.7079, Val Acc: 0.9269\n",
            "Epoch [3824/4000], Train Loss: 1.7086, Train Acc: 0.9295, Val Loss: 1.7112, Val Acc: 0.9333\n",
            "Epoch [3825/4000], Train Loss: 1.7088, Train Acc: 0.9298, Val Loss: 1.7106, Val Acc: 0.9282\n",
            "Epoch [3826/4000], Train Loss: 1.7087, Train Acc: 0.9298, Val Loss: 1.7066, Val Acc: 0.9321\n",
            "Epoch [3827/4000], Train Loss: 1.7093, Train Acc: 0.9292, Val Loss: 1.7056, Val Acc: 0.9301\n",
            "Epoch [3828/4000], Train Loss: 1.7116, Train Acc: 0.9288, Val Loss: 1.7071, Val Acc: 0.9327\n",
            "Epoch [3829/4000], Train Loss: 1.7106, Train Acc: 0.9282, Val Loss: 1.7061, Val Acc: 0.9308\n",
            "Epoch [3830/4000], Train Loss: 1.7095, Train Acc: 0.9300, Val Loss: 1.7107, Val Acc: 0.9263\n",
            "Epoch [3831/4000], Train Loss: 1.7092, Train Acc: 0.9298, Val Loss: 1.7095, Val Acc: 0.9282\n",
            "Epoch [3832/4000], Train Loss: 1.7083, Train Acc: 0.9304, Val Loss: 1.7072, Val Acc: 0.9308\n",
            "Epoch [3833/4000], Train Loss: 1.7099, Train Acc: 0.9303, Val Loss: 1.7167, Val Acc: 0.9263\n",
            "Epoch [3834/4000], Train Loss: 1.7115, Train Acc: 0.9276, Val Loss: 1.7066, Val Acc: 0.9321\n",
            "Epoch [3835/4000], Train Loss: 1.7090, Train Acc: 0.9298, Val Loss: 1.7076, Val Acc: 0.9295\n",
            "Epoch [3836/4000], Train Loss: 1.7082, Train Acc: 0.9306, Val Loss: 1.7055, Val Acc: 0.9327\n",
            "Epoch [3837/4000], Train Loss: 1.7091, Train Acc: 0.9285, Val Loss: 1.7152, Val Acc: 0.9314\n",
            "Epoch [3838/4000], Train Loss: 1.7086, Train Acc: 0.9300, Val Loss: 1.7079, Val Acc: 0.9282\n",
            "Epoch [3839/4000], Train Loss: 1.7099, Train Acc: 0.9306, Val Loss: 1.7111, Val Acc: 0.9321\n",
            "Epoch [3840/4000], Train Loss: 1.7095, Train Acc: 0.9295, Val Loss: 1.7092, Val Acc: 0.9327\n",
            "Epoch [3841/4000], Train Loss: 1.7089, Train Acc: 0.9306, Val Loss: 1.7100, Val Acc: 0.9282\n",
            "Epoch [3842/4000], Train Loss: 1.7087, Train Acc: 0.9296, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [3843/4000], Train Loss: 1.7102, Train Acc: 0.9296, Val Loss: 1.7075, Val Acc: 0.9327\n",
            "Epoch [3844/4000], Train Loss: 1.7103, Train Acc: 0.9285, Val Loss: 1.7129, Val Acc: 0.9288\n",
            "Epoch [3845/4000], Train Loss: 1.7104, Train Acc: 0.9298, Val Loss: 1.7101, Val Acc: 0.9308\n",
            "Epoch [3846/4000], Train Loss: 1.7104, Train Acc: 0.9284, Val Loss: 1.7161, Val Acc: 0.9282\n",
            "Epoch [3847/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7089, Val Acc: 0.9276\n",
            "Epoch [3848/4000], Train Loss: 1.7096, Train Acc: 0.9285, Val Loss: 1.7073, Val Acc: 0.9340\n",
            "Epoch [3849/4000], Train Loss: 1.7093, Train Acc: 0.9304, Val Loss: 1.7083, Val Acc: 0.9321\n",
            "Epoch [3850/4000], Train Loss: 1.7086, Train Acc: 0.9304, Val Loss: 1.7117, Val Acc: 0.9288\n",
            "Epoch [3851/4000], Train Loss: 1.7094, Train Acc: 0.9288, Val Loss: 1.7077, Val Acc: 0.9333\n",
            "Epoch [3852/4000], Train Loss: 1.7081, Train Acc: 0.9301, Val Loss: 1.7112, Val Acc: 0.9263\n",
            "Epoch [3853/4000], Train Loss: 1.7080, Train Acc: 0.9313, Val Loss: 1.7104, Val Acc: 0.9308\n",
            "Epoch [3854/4000], Train Loss: 1.7079, Train Acc: 0.9303, Val Loss: 1.7066, Val Acc: 0.9327\n",
            "Epoch [3855/4000], Train Loss: 1.7084, Train Acc: 0.9303, Val Loss: 1.7078, Val Acc: 0.9340\n",
            "Epoch [3856/4000], Train Loss: 1.7085, Train Acc: 0.9304, Val Loss: 1.7082, Val Acc: 0.9301\n",
            "Epoch [3857/4000], Train Loss: 1.7092, Train Acc: 0.9288, Val Loss: 1.7144, Val Acc: 0.9250\n",
            "Epoch [3858/4000], Train Loss: 1.7097, Train Acc: 0.9295, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [3859/4000], Train Loss: 1.7088, Train Acc: 0.9311, Val Loss: 1.7070, Val Acc: 0.9295\n",
            "Epoch [3860/4000], Train Loss: 1.7083, Train Acc: 0.9317, Val Loss: 1.7060, Val Acc: 0.9314\n",
            "Epoch [3861/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7091, Val Acc: 0.9314\n",
            "Epoch [3862/4000], Train Loss: 1.7097, Train Acc: 0.9296, Val Loss: 1.7116, Val Acc: 0.9256\n",
            "Epoch [3863/4000], Train Loss: 1.7093, Train Acc: 0.9295, Val Loss: 1.7159, Val Acc: 0.9288\n",
            "Epoch [3864/4000], Train Loss: 1.7096, Train Acc: 0.9303, Val Loss: 1.7098, Val Acc: 0.9327\n",
            "Epoch [3865/4000], Train Loss: 1.7089, Train Acc: 0.9309, Val Loss: 1.7071, Val Acc: 0.9314\n",
            "Epoch [3866/4000], Train Loss: 1.7104, Train Acc: 0.9290, Val Loss: 1.7088, Val Acc: 0.9321\n",
            "Epoch [3867/4000], Train Loss: 1.7081, Train Acc: 0.9301, Val Loss: 1.7117, Val Acc: 0.9282\n",
            "Epoch [3868/4000], Train Loss: 1.7090, Train Acc: 0.9314, Val Loss: 1.7094, Val Acc: 0.9321\n",
            "Epoch [3869/4000], Train Loss: 1.7088, Train Acc: 0.9298, Val Loss: 1.7082, Val Acc: 0.9321\n",
            "Epoch [3870/4000], Train Loss: 1.7100, Train Acc: 0.9303, Val Loss: 1.7098, Val Acc: 0.9295\n",
            "Epoch [3871/4000], Train Loss: 1.7088, Train Acc: 0.9295, Val Loss: 1.7076, Val Acc: 0.9314\n",
            "Epoch [3872/4000], Train Loss: 1.7100, Train Acc: 0.9293, Val Loss: 1.7066, Val Acc: 0.9321\n",
            "Epoch [3873/4000], Train Loss: 1.7089, Train Acc: 0.9290, Val Loss: 1.7093, Val Acc: 0.9308\n",
            "Epoch [3874/4000], Train Loss: 1.7091, Train Acc: 0.9296, Val Loss: 1.7157, Val Acc: 0.9256\n",
            "Epoch [3875/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7080, Val Acc: 0.9314\n",
            "Epoch [3876/4000], Train Loss: 1.7086, Train Acc: 0.9309, Val Loss: 1.7075, Val Acc: 0.9333\n",
            "Epoch [3877/4000], Train Loss: 1.7089, Train Acc: 0.9296, Val Loss: 1.7044, Val Acc: 0.9327\n",
            "Epoch [3878/4000], Train Loss: 1.7112, Train Acc: 0.9279, Val Loss: 1.7091, Val Acc: 0.9321\n",
            "Epoch [3879/4000], Train Loss: 1.7095, Train Acc: 0.9309, Val Loss: 1.7115, Val Acc: 0.9282\n",
            "Epoch [3880/4000], Train Loss: 1.7104, Train Acc: 0.9284, Val Loss: 1.7064, Val Acc: 0.9295\n",
            "Epoch [3881/4000], Train Loss: 1.7085, Train Acc: 0.9304, Val Loss: 1.7083, Val Acc: 0.9321\n",
            "Epoch [3882/4000], Train Loss: 1.7093, Train Acc: 0.9303, Val Loss: 1.7083, Val Acc: 0.9314\n",
            "Epoch [3883/4000], Train Loss: 1.7118, Train Acc: 0.9284, Val Loss: 1.7157, Val Acc: 0.9282\n",
            "Epoch [3884/4000], Train Loss: 1.7089, Train Acc: 0.9298, Val Loss: 1.7085, Val Acc: 0.9301\n",
            "Epoch [3885/4000], Train Loss: 1.7086, Train Acc: 0.9304, Val Loss: 1.7070, Val Acc: 0.9301\n",
            "Epoch [3886/4000], Train Loss: 1.7089, Train Acc: 0.9300, Val Loss: 1.7076, Val Acc: 0.9333\n",
            "Epoch [3887/4000], Train Loss: 1.7095, Train Acc: 0.9296, Val Loss: 1.7097, Val Acc: 0.9321\n",
            "Epoch [3888/4000], Train Loss: 1.7088, Train Acc: 0.9290, Val Loss: 1.7125, Val Acc: 0.9276\n",
            "Epoch [3889/4000], Train Loss: 1.7103, Train Acc: 0.9272, Val Loss: 1.7094, Val Acc: 0.9301\n",
            "Epoch [3890/4000], Train Loss: 1.7095, Train Acc: 0.9316, Val Loss: 1.7098, Val Acc: 0.9333\n",
            "Epoch [3891/4000], Train Loss: 1.7093, Train Acc: 0.9293, Val Loss: 1.7080, Val Acc: 0.9321\n",
            "Epoch [3892/4000], Train Loss: 1.7089, Train Acc: 0.9300, Val Loss: 1.7060, Val Acc: 0.9308\n",
            "Epoch [3893/4000], Train Loss: 1.7094, Train Acc: 0.9295, Val Loss: 1.7056, Val Acc: 0.9340\n",
            "Epoch [3894/4000], Train Loss: 1.7098, Train Acc: 0.9285, Val Loss: 1.7123, Val Acc: 0.9269\n",
            "Epoch [3895/4000], Train Loss: 1.7086, Train Acc: 0.9314, Val Loss: 1.7085, Val Acc: 0.9314\n",
            "Epoch [3896/4000], Train Loss: 1.7094, Train Acc: 0.9292, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [3897/4000], Train Loss: 1.7095, Train Acc: 0.9292, Val Loss: 1.7113, Val Acc: 0.9308\n",
            "Epoch [3898/4000], Train Loss: 1.7097, Train Acc: 0.9295, Val Loss: 1.7081, Val Acc: 0.9333\n",
            "Epoch [3899/4000], Train Loss: 1.7104, Train Acc: 0.9287, Val Loss: 1.7073, Val Acc: 0.9308\n",
            "Epoch [3900/4000], Train Loss: 1.7101, Train Acc: 0.9296, Val Loss: 1.7057, Val Acc: 0.9327\n",
            "Epoch [3901/4000], Train Loss: 1.7079, Train Acc: 0.9298, Val Loss: 1.7158, Val Acc: 0.9167\n",
            "Epoch [3902/4000], Train Loss: 1.7099, Train Acc: 0.9293, Val Loss: 1.7067, Val Acc: 0.9308\n",
            "Epoch [3903/4000], Train Loss: 1.7113, Train Acc: 0.9292, Val Loss: 1.7122, Val Acc: 0.9269\n",
            "Epoch [3904/4000], Train Loss: 1.7127, Train Acc: 0.9264, Val Loss: 1.7148, Val Acc: 0.9276\n",
            "Epoch [3905/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7138, Val Acc: 0.9237\n",
            "Epoch [3906/4000], Train Loss: 1.7094, Train Acc: 0.9288, Val Loss: 1.7044, Val Acc: 0.9308\n",
            "Epoch [3907/4000], Train Loss: 1.7091, Train Acc: 0.9300, Val Loss: 1.7057, Val Acc: 0.9295\n",
            "Epoch [3908/4000], Train Loss: 1.7083, Train Acc: 0.9300, Val Loss: 1.7067, Val Acc: 0.9288\n",
            "Epoch [3909/4000], Train Loss: 1.7091, Train Acc: 0.9309, Val Loss: 1.7081, Val Acc: 0.9333\n",
            "Epoch [3910/4000], Train Loss: 1.7074, Train Acc: 0.9317, Val Loss: 1.7095, Val Acc: 0.9314\n",
            "Epoch [3911/4000], Train Loss: 1.7083, Train Acc: 0.9293, Val Loss: 1.7101, Val Acc: 0.9308\n",
            "Epoch [3912/4000], Train Loss: 1.7085, Train Acc: 0.9296, Val Loss: 1.7122, Val Acc: 0.9288\n",
            "Epoch [3913/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7070, Val Acc: 0.9314\n",
            "Epoch [3914/4000], Train Loss: 1.7078, Train Acc: 0.9303, Val Loss: 1.7086, Val Acc: 0.9321\n",
            "Epoch [3915/4000], Train Loss: 1.7104, Train Acc: 0.9293, Val Loss: 1.7035, Val Acc: 0.9340\n",
            "Epoch [3916/4000], Train Loss: 1.7092, Train Acc: 0.9282, Val Loss: 1.7085, Val Acc: 0.9321\n",
            "Epoch [3917/4000], Train Loss: 1.7095, Train Acc: 0.9290, Val Loss: 1.7086, Val Acc: 0.9301\n",
            "Epoch [3918/4000], Train Loss: 1.7092, Train Acc: 0.9301, Val Loss: 1.7069, Val Acc: 0.9301\n",
            "Epoch [3919/4000], Train Loss: 1.7094, Train Acc: 0.9295, Val Loss: 1.7058, Val Acc: 0.9321\n",
            "Epoch [3920/4000], Train Loss: 1.7093, Train Acc: 0.9292, Val Loss: 1.7104, Val Acc: 0.9308\n",
            "Epoch [3921/4000], Train Loss: 1.7094, Train Acc: 0.9290, Val Loss: 1.7098, Val Acc: 0.9301\n",
            "Epoch [3922/4000], Train Loss: 1.7090, Train Acc: 0.9290, Val Loss: 1.7124, Val Acc: 0.9276\n",
            "Epoch [3923/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7075, Val Acc: 0.9295\n",
            "Epoch [3924/4000], Train Loss: 1.7094, Train Acc: 0.9288, Val Loss: 1.7065, Val Acc: 0.9327\n",
            "Epoch [3925/4000], Train Loss: 1.7080, Train Acc: 0.9308, Val Loss: 1.7142, Val Acc: 0.9263\n",
            "Epoch [3926/4000], Train Loss: 1.7103, Train Acc: 0.9298, Val Loss: 1.7052, Val Acc: 0.9321\n",
            "Epoch [3927/4000], Train Loss: 1.7101, Train Acc: 0.9303, Val Loss: 1.7057, Val Acc: 0.9288\n",
            "Epoch [3928/4000], Train Loss: 1.7098, Train Acc: 0.9300, Val Loss: 1.7066, Val Acc: 0.9359\n",
            "Epoch [3929/4000], Train Loss: 1.7101, Train Acc: 0.9303, Val Loss: 1.7081, Val Acc: 0.9314\n",
            "Epoch [3930/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7105, Val Acc: 0.9263\n",
            "Epoch [3931/4000], Train Loss: 1.7107, Train Acc: 0.9292, Val Loss: 1.7118, Val Acc: 0.9282\n",
            "Epoch [3932/4000], Train Loss: 1.7083, Train Acc: 0.9316, Val Loss: 1.7092, Val Acc: 0.9301\n",
            "Epoch [3933/4000], Train Loss: 1.7089, Train Acc: 0.9313, Val Loss: 1.7071, Val Acc: 0.9301\n",
            "Epoch [3934/4000], Train Loss: 1.7106, Train Acc: 0.9285, Val Loss: 1.7100, Val Acc: 0.9250\n",
            "Epoch [3935/4000], Train Loss: 1.7086, Train Acc: 0.9306, Val Loss: 1.7060, Val Acc: 0.9314\n",
            "Epoch [3936/4000], Train Loss: 1.7077, Train Acc: 0.9313, Val Loss: 1.7117, Val Acc: 0.9282\n",
            "Epoch [3937/4000], Train Loss: 1.7080, Train Acc: 0.9300, Val Loss: 1.7072, Val Acc: 0.9314\n",
            "Epoch [3938/4000], Train Loss: 1.7099, Train Acc: 0.9288, Val Loss: 1.7071, Val Acc: 0.9340\n",
            "Epoch [3939/4000], Train Loss: 1.7089, Train Acc: 0.9303, Val Loss: 1.7115, Val Acc: 0.9269\n",
            "Epoch [3940/4000], Train Loss: 1.7083, Train Acc: 0.9304, Val Loss: 1.7077, Val Acc: 0.9308\n",
            "Epoch [3941/4000], Train Loss: 1.7092, Train Acc: 0.9304, Val Loss: 1.7072, Val Acc: 0.9308\n",
            "Epoch [3942/4000], Train Loss: 1.7091, Train Acc: 0.9296, Val Loss: 1.7138, Val Acc: 0.9244\n",
            "Epoch [3943/4000], Train Loss: 1.7086, Train Acc: 0.9298, Val Loss: 1.7094, Val Acc: 0.9308\n",
            "Epoch [3944/4000], Train Loss: 1.7096, Train Acc: 0.9308, Val Loss: 1.7110, Val Acc: 0.9231\n",
            "Epoch [3945/4000], Train Loss: 1.7091, Train Acc: 0.9300, Val Loss: 1.7076, Val Acc: 0.9346\n",
            "Epoch [3946/4000], Train Loss: 1.7096, Train Acc: 0.9295, Val Loss: 1.7082, Val Acc: 0.9321\n",
            "Epoch [3947/4000], Train Loss: 1.7089, Train Acc: 0.9292, Val Loss: 1.7068, Val Acc: 0.9301\n",
            "Epoch [3948/4000], Train Loss: 1.7096, Train Acc: 0.9296, Val Loss: 1.7135, Val Acc: 0.9263\n",
            "Epoch [3949/4000], Train Loss: 1.7101, Train Acc: 0.9295, Val Loss: 1.7058, Val Acc: 0.9308\n",
            "Epoch [3950/4000], Train Loss: 1.7097, Train Acc: 0.9300, Val Loss: 1.7126, Val Acc: 0.9269\n",
            "Epoch [3951/4000], Train Loss: 1.7098, Train Acc: 0.9311, Val Loss: 1.7083, Val Acc: 0.9301\n",
            "Epoch [3952/4000], Train Loss: 1.7096, Train Acc: 0.9293, Val Loss: 1.7086, Val Acc: 0.9308\n",
            "Epoch [3953/4000], Train Loss: 1.7090, Train Acc: 0.9292, Val Loss: 1.7062, Val Acc: 0.9308\n",
            "Epoch [3954/4000], Train Loss: 1.7107, Train Acc: 0.9296, Val Loss: 1.7158, Val Acc: 0.9308\n",
            "Epoch [3955/4000], Train Loss: 1.7101, Train Acc: 0.9293, Val Loss: 1.7096, Val Acc: 0.9295\n",
            "Epoch [3956/4000], Train Loss: 1.7096, Train Acc: 0.9288, Val Loss: 1.7076, Val Acc: 0.9301\n",
            "Epoch [3957/4000], Train Loss: 1.7081, Train Acc: 0.9298, Val Loss: 1.7066, Val Acc: 0.9333\n",
            "Epoch [3958/4000], Train Loss: 1.7077, Train Acc: 0.9313, Val Loss: 1.7115, Val Acc: 0.9250\n",
            "Epoch [3959/4000], Train Loss: 1.7097, Train Acc: 0.9292, Val Loss: 1.7048, Val Acc: 0.9301\n",
            "Epoch [3960/4000], Train Loss: 1.7091, Train Acc: 0.9300, Val Loss: 1.7079, Val Acc: 0.9295\n",
            "Epoch [3961/4000], Train Loss: 1.7080, Train Acc: 0.9301, Val Loss: 1.7097, Val Acc: 0.9308\n",
            "Epoch [3962/4000], Train Loss: 1.7088, Train Acc: 0.9295, Val Loss: 1.7096, Val Acc: 0.9333\n",
            "Epoch [3963/4000], Train Loss: 1.7082, Train Acc: 0.9304, Val Loss: 1.7054, Val Acc: 0.9340\n",
            "Epoch [3964/4000], Train Loss: 1.7087, Train Acc: 0.9313, Val Loss: 1.7043, Val Acc: 0.9327\n",
            "Epoch [3965/4000], Train Loss: 1.7100, Train Acc: 0.9287, Val Loss: 1.7071, Val Acc: 0.9314\n",
            "Epoch [3966/4000], Train Loss: 1.7087, Train Acc: 0.9306, Val Loss: 1.7056, Val Acc: 0.9295\n",
            "Epoch [3967/4000], Train Loss: 1.7081, Train Acc: 0.9319, Val Loss: 1.7064, Val Acc: 0.9333\n",
            "Epoch [3968/4000], Train Loss: 1.7086, Train Acc: 0.9311, Val Loss: 1.7108, Val Acc: 0.9301\n",
            "Epoch [3969/4000], Train Loss: 1.7085, Train Acc: 0.9309, Val Loss: 1.7112, Val Acc: 0.9276\n",
            "Epoch [3970/4000], Train Loss: 1.7099, Train Acc: 0.9313, Val Loss: 1.7098, Val Acc: 0.9269\n",
            "Epoch [3971/4000], Train Loss: 1.7102, Train Acc: 0.9288, Val Loss: 1.7084, Val Acc: 0.9308\n",
            "Epoch [3972/4000], Train Loss: 1.7089, Train Acc: 0.9296, Val Loss: 1.7046, Val Acc: 0.9327\n",
            "Epoch [3973/4000], Train Loss: 1.7087, Train Acc: 0.9295, Val Loss: 1.7095, Val Acc: 0.9295\n",
            "Epoch [3974/4000], Train Loss: 1.7099, Train Acc: 0.9290, Val Loss: 1.7053, Val Acc: 0.9314\n",
            "Epoch [3975/4000], Train Loss: 1.7100, Train Acc: 0.9306, Val Loss: 1.7072, Val Acc: 0.9295\n",
            "Epoch [3976/4000], Train Loss: 1.7107, Train Acc: 0.9290, Val Loss: 1.7079, Val Acc: 0.9308\n",
            "Epoch [3977/4000], Train Loss: 1.7084, Train Acc: 0.9303, Val Loss: 1.7102, Val Acc: 0.9282\n",
            "Epoch [3978/4000], Train Loss: 1.7101, Train Acc: 0.9290, Val Loss: 1.7106, Val Acc: 0.9288\n",
            "Epoch [3979/4000], Train Loss: 1.7088, Train Acc: 0.9295, Val Loss: 1.7081, Val Acc: 0.9301\n",
            "Epoch [3980/4000], Train Loss: 1.7084, Train Acc: 0.9313, Val Loss: 1.7076, Val Acc: 0.9276\n",
            "Epoch [3981/4000], Train Loss: 1.7103, Train Acc: 0.9288, Val Loss: 1.7189, Val Acc: 0.9250\n",
            "Epoch [3982/4000], Train Loss: 1.7116, Train Acc: 0.9264, Val Loss: 1.7127, Val Acc: 0.9288\n",
            "Epoch [3983/4000], Train Loss: 1.7081, Train Acc: 0.9309, Val Loss: 1.7058, Val Acc: 0.9314\n",
            "Epoch [3984/4000], Train Loss: 1.7095, Train Acc: 0.9308, Val Loss: 1.7072, Val Acc: 0.9295\n",
            "Epoch [3985/4000], Train Loss: 1.7078, Train Acc: 0.9313, Val Loss: 1.7176, Val Acc: 0.9269\n",
            "Epoch [3986/4000], Train Loss: 1.7094, Train Acc: 0.9295, Val Loss: 1.7064, Val Acc: 0.9333\n",
            "Epoch [3987/4000], Train Loss: 1.7082, Train Acc: 0.9288, Val Loss: 1.7090, Val Acc: 0.9282\n",
            "Epoch [3988/4000], Train Loss: 1.7096, Train Acc: 0.9301, Val Loss: 1.7086, Val Acc: 0.9282\n",
            "Epoch [3989/4000], Train Loss: 1.7095, Train Acc: 0.9285, Val Loss: 1.7109, Val Acc: 0.9282\n",
            "Epoch [3990/4000], Train Loss: 1.7089, Train Acc: 0.9296, Val Loss: 1.7064, Val Acc: 0.9314\n",
            "Epoch [3991/4000], Train Loss: 1.7104, Train Acc: 0.9287, Val Loss: 1.7088, Val Acc: 0.9321\n",
            "Epoch [3992/4000], Train Loss: 1.7092, Train Acc: 0.9292, Val Loss: 1.7091, Val Acc: 0.9301\n",
            "Epoch [3993/4000], Train Loss: 1.7088, Train Acc: 0.9287, Val Loss: 1.7053, Val Acc: 0.9340\n",
            "Epoch [3994/4000], Train Loss: 1.7091, Train Acc: 0.9308, Val Loss: 1.7087, Val Acc: 0.9295\n",
            "Epoch [3995/4000], Train Loss: 1.7088, Train Acc: 0.9296, Val Loss: 1.7083, Val Acc: 0.9321\n",
            "Epoch [3996/4000], Train Loss: 1.7108, Train Acc: 0.9282, Val Loss: 1.7055, Val Acc: 0.9327\n",
            "Epoch [3997/4000], Train Loss: 1.7099, Train Acc: 0.9298, Val Loss: 1.7051, Val Acc: 0.9314\n",
            "Epoch [3998/4000], Train Loss: 1.7088, Train Acc: 0.9313, Val Loss: 1.7092, Val Acc: 0.9314\n",
            "Epoch [3999/4000], Train Loss: 1.7091, Train Acc: 0.9308, Val Loss: 1.7078, Val Acc: 0.9295\n",
            "Epoch [4000/4000], Train Loss: 1.7105, Train Acc: 0.9284, Val Loss: 1.7118, Val Acc: 0.9282\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "optim = optim.Adam(Model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "train_model(Model,torch.nn.CrossEntropyLoss(), optim,Noise_0_train_loader, Noise_0_test_loader,num_epochs=4000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = \"model.pt\"\n",
        "\n",
        "# Save the model\n",
        "torch.save(Model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATH = \"model.pt\"\n",
        "\n",
        "# Load the model\n",
        "Model.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b32HVbvcPSjV"
      },
      "source": [
        "## Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FKRoQ3NnBNw8"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.utilities.types import OptimizerLRScheduler\n",
        "\n",
        "\n",
        "class LightningMLP(L.LightningModule):\n",
        "    def __init__(self, model, loss, optimizer):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = torch.tensor(inputs).to(torch.float32),torch.tensor(targets).to(torch.long)\n",
        "        inputs, targets = inputs.to(device),targets.to(device)\n",
        "        output = self.model(inputs)\n",
        "        loss = self.loss(output, targets)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self,batch,batch_idx):\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = torch.tensor(inputs).to(torch.float32),torch.tensor(targets).to(torch.long)\n",
        "        inputs, targets = inputs.to(device),targets.to(device)\n",
        "        output = self.model(inputs)\n",
        "        loss = self.loss(output, targets)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "0c0de4b95e4545c0bbe9eafea153125b",
            "bd0232e4017240debb92efe3100b71a3",
            "32fad7dacf594cb488bf6a261f933f8b",
            "613424989e984c8888528d3a4d61a318",
            "1ab03e476b624c699cc85fb8d9ec71a1",
            "95bf24f9ca0b4baab139e7716dffb754",
            "3e901c104db34d8a88a632d8d897022b",
            "a2d642748e7c4d81a2ab1ec8d9720926",
            "5217d52572ba41ceba0130b5ac47af5b",
            "2a39fbb734d74e8b9b9f44a0627b35a2",
            "abbe93dc21094598aa884f59e3b4aaab"
          ]
        },
        "id": "jvabAADABNw8",
        "outputId": "54d61c56-84f6-4338-9e38-1cedc2838bff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Deeptanshu Barman\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type  | Params\n",
            "--------------------------------\n",
            "0 | model | MyMLP | 6.5 K \n",
            "--------------------------------\n",
            "6.5 K     Trainable params\n",
            "0         Non-trainable params\n",
            "6.5 K     Total params\n",
            "0.026     Total estimated model params size (MB)\n",
            "C:\\Users\\Deeptanshu Barman\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dcf7890b2984588abf21669aafd8363",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\temp\\ipykernel_16032\\3998816417.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs, targets = torch.tensor(inputs).to(torch.float32),torch.tensor(targets).to(torch.long)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Boolean value of Tensor with more than one value is ambiguous",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m mymodel \u001b[38;5;241m=\u001b[39m LightningMLP(Model, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(Model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m))\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmymodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNoise_0_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNoise_0_test_loader\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m         closure()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\core\\module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1254\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1289\u001b[0m \n\u001b[0;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1291\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\core\\optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 183\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    186\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[20], line 16\u001b[0m, in \u001b[0;36mLightningMLP.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device),targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:1169\u001b[0m, in \u001b[0;36mCrossEntropyLoss.__init__\u001b[1;34m(self, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m   1168\u001b[0m              reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, label_smoothing: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index \u001b[38;5;241m=\u001b[39m ignore_index\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing \u001b[38;5;241m=\u001b[39m label_smoothing\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:30\u001b[0m, in \u001b[0;36m_WeightedLoss.__init__\u001b[1;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_WeightedLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, weight)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight: Optional[Tensor]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[1;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m(_Loss, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[1;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[0;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ],
      "source": [
        "mymodel = LightningMLP(Model, torch.nn.CrossEntropyLoss, torch.optim.Adam(Model.parameters(), lr=0.001))\n",
        "trainer = L.Trainer(max_epochs = 10)\n",
        "trainer.fit(mymodel, Noise_0_train_loader, Noise_0_test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c0de4b95e4545c0bbe9eafea153125b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd0232e4017240debb92efe3100b71a3",
              "IPY_MODEL_32fad7dacf594cb488bf6a261f933f8b",
              "IPY_MODEL_613424989e984c8888528d3a4d61a318"
            ],
            "layout": "IPY_MODEL_1ab03e476b624c699cc85fb8d9ec71a1"
          }
        },
        "1ab03e476b624c699cc85fb8d9ec71a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2a39fbb734d74e8b9b9f44a0627b35a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32fad7dacf594cb488bf6a261f933f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2d642748e7c4d81a2ab1ec8d9720926",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5217d52572ba41ceba0130b5ac47af5b",
            "value": 98
          }
        },
        "3e901c104db34d8a88a632d8d897022b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5217d52572ba41ceba0130b5ac47af5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "613424989e984c8888528d3a4d61a318": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a39fbb734d74e8b9b9f44a0627b35a2",
            "placeholder": "​",
            "style": "IPY_MODEL_abbe93dc21094598aa884f59e3b4aaab",
            "value": " 98/98 [00:00&lt;00:00, 280.28it/s, v_num=3]"
          }
        },
        "95bf24f9ca0b4baab139e7716dffb754": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2d642748e7c4d81a2ab1ec8d9720926": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abbe93dc21094598aa884f59e3b4aaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd0232e4017240debb92efe3100b71a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95bf24f9ca0b4baab139e7716dffb754",
            "placeholder": "​",
            "style": "IPY_MODEL_3e901c104db34d8a88a632d8d897022b",
            "value": "Epoch 9: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
