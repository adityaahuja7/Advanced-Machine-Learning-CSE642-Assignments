{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noise_0_dataframe = pd.read_csv(\"Data/Assignment1/data_0_noise\")\n",
    "Noise_Low_dataframe = pd.read_csv(\"Data/Assignment1/data_Low_noise\")\n",
    "Noise_High_dataframe = pd.read_csv(\"Data/Assignment1/data_High_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=Noise_Low_dataframe\n",
    "validation_dataframe=Noise_Low_dataframe\n",
    "target_columns=\"era\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode = \"era\"\n",
    "class_index = list(dataframe[to_encode].unique())\n",
    "def encode(value, class_index = class_index):\n",
    "    return class_index.index(value)\n",
    "\n",
    "dataframe[to_encode] = dataframe[to_encode].apply(encode)\n",
    "validation_dataframe[to_encode] = validation_dataframe[to_encode].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, noise, transform=None, target_transform=None, drop=None, target=None, encoderdecoder=False):\n",
    "        self.dataframe = dataframe\n",
    "        if drop != None:\n",
    "            self.X = dataframe.drop(drop, axis=1).values\n",
    "        else:\n",
    "            self.X = dataframe.values\n",
    "\n",
    "        if not encoderdecoder:\n",
    "            self.y = dataframe[target].values\n",
    "        else:\n",
    "            self.y = self.X\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.noise = noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, label = self.X[idx], self.y[idx]\n",
    "        return item, label\n",
    "\n",
    "    def get_noise(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(dataframe, \"0\",drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\",\"data_type\"],target=target_columns)\n",
    "Noise_train, Noise_test = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "Noise_train_loader = DataLoader(Noise_train, batch_size=128, shuffle=True)\n",
    "Noise_test_loader = DataLoader(Noise_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(torch.nn.Module):\n",
    "    def __init__(self,ModuleList):\n",
    "        super(MyMLP, self).__init__()\n",
    "        module_list = torch.nn.ModuleList(ModuleList)\n",
    "        self.layers = torch.nn.Sequential(*module_list)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.layers(X)\n",
    "        X = self.softmax(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubTabEncoderDecoder(torch.nn.Module):\n",
    "    def __init__(self,encoder_sizes,decoder_sizes,activation = torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.encoder_sizes = encoder_sizes\n",
    "        self.decoder_sizes = decoder_sizes\n",
    "        linear_encoder = [torch.nn.Linear(encoder_sizes[i],encoder_sizes[i+1]) for i in range(len(encoder_sizes)-1)]\n",
    "        linear_decoder = [torch.nn.Linear(decoder_sizes[i],decoder_sizes[i+1]) for i in range(len(decoder_sizes)-1)]\n",
    "        self.encoder = torch.nn.Sequential(*[l for layer in linear_encoder for l in (layer, activation)])\n",
    "        self.decoder = torch.nn.Sequential(*[l for layer in linear_decoder for l in (layer, activation)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SubTabEncoderDecoder([24,32,16],[16,32,24])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,optimizer,train_loader,val_loader,num_epochs = 100,verbos = True, lr = 0.001):\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        current_train_loss = 0\n",
    "        current_accuracy = []\n",
    "        for data, target in tqdm(train_loader,desc = \"Training Epoch \"+str(epoch)):\n",
    "            data, target = data.to(device).float(), target.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_train_loss += loss.item()\n",
    "            current_accuracy.append(torch.sum(torch.argmax(output,dim=1) == target).item()/len(target))\n",
    "            \n",
    "        if (verbos):\n",
    "            print(\"Epoch: \",epoch,\" Loss: \",current_train_loss/len(train_loader))\n",
    "            print(\"Accuracy: \",np.mean(current_accuracy))\n",
    "            \n",
    "        train_loss.append(current_train_loss/len(train_loader))\n",
    "        model.eval()\n",
    "        current_val_loss = 0\n",
    "        current_val_accuracy = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device).float(), target.to(device).long()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                current_val_loss += loss.item()\n",
    "                current_val_accuracy.append(torch.sum(torch.argmax(output,dim=1) == target).item()/len(target))\n",
    "            val_loss.append(current_val_loss/len(val_loader))\n",
    "            if (verbos):\n",
    "                print(\"Validation Loss: \",current_val_loss/len(val_loader))\n",
    "                print(\"Validation Accuracy: \",np.mean(current_val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()\n",
    "torch.save(encoder, \"encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 5.4385e-02,  8.8948e-02,  4.4324e-02,  1.4275e-01, -5.7704e-02,\n",
      "         -2.7618e-01,  7.2716e-02,  8.1099e-02,  2.0728e-01,  2.6289e-01,\n",
      "          4.9600e-01, -7.6064e-02, -2.2410e-03, -8.9613e-02,  1.0795e-01,\n",
      "          1.6437e-01, -2.9043e-01,  1.4872e-01,  2.6488e-01,  9.9153e-02,\n",
      "          4.2092e-01,  1.4136e-01,  4.8081e-02, -4.3162e-02],\n",
      "        [-1.6995e-01,  4.2841e-02, -8.1688e-02, -6.8102e-02,  4.0787e-02,\n",
      "          3.1971e-01,  3.5013e-01,  1.4828e-01,  3.4598e-01, -1.2330e-01,\n",
      "         -3.8470e-02,  2.1789e-01, -1.3521e-01,  1.6085e-01, -9.4290e-02,\n",
      "          3.1923e-01,  2.3472e-01,  9.9152e-02,  7.9975e-03,  4.6274e-01,\n",
      "         -1.2566e-01, -5.0012e-02,  4.3919e-02,  4.3021e-01],\n",
      "        [-1.0232e-01, -4.2814e-03,  2.1391e-01,  4.5100e-03, -1.0470e-01,\n",
      "          2.6734e-02,  1.2554e-01,  2.1296e-01, -2.4479e-02, -1.5362e-01,\n",
      "          1.4884e-02, -5.5587e-02,  1.8573e-01,  8.2411e-02,  1.6707e-01,\n",
      "          5.8941e-02,  2.2441e-02,  2.6793e-01,  7.2289e-02,  7.3340e-03,\n",
      "          9.7594e-02,  9.6551e-02, -3.8792e-02, -3.2696e-02],\n",
      "        [ 8.3963e-02,  1.2356e-01, -5.8431e-02,  1.1891e-01, -1.9283e-01,\n",
      "          9.5178e-02,  2.5615e-01, -2.9308e-02,  8.3568e-02,  1.5608e-01,\n",
      "          1.5373e-01,  2.3505e-01,  1.0554e-02,  3.3235e-02,  2.6236e-01,\n",
      "          4.1843e-01, -1.6318e-01, -6.4946e-02,  1.1990e-01, -1.3774e-01,\n",
      "         -1.2096e-01,  1.8253e-01, -3.6415e-02,  1.0241e-02],\n",
      "        [ 9.5450e-02,  8.8223e-02, -1.8284e-01, -1.8759e-01, -9.3602e-02,\n",
      "          2.2072e-01,  1.0989e-01, -1.5723e-01, -8.6629e-02,  3.2281e-01,\n",
      "         -2.9247e-02,  1.4996e-01,  3.5719e-02, -2.0836e-01, -5.1188e-02,\n",
      "          1.4695e-01, -2.4660e-03, -1.8845e-02,  1.2118e-01,  1.9070e-01,\n",
      "          9.6476e-02,  2.0334e-01,  5.0431e-03,  1.5881e-01],\n",
      "        [-8.3216e-02,  1.3512e-01, -2.1769e-02, -8.3982e-02, -1.0468e-01,\n",
      "          5.3272e-02,  3.4784e-01,  2.4992e-01,  1.9574e-01,  1.0713e-01,\n",
      "         -4.6733e-02,  5.5384e-01, -1.3602e-01,  2.2185e-01,  1.0238e-01,\n",
      "         -2.8381e-01,  3.6332e-02, -3.3633e-01,  1.3448e-01,  5.4557e-02,\n",
      "          2.4235e-01,  6.4496e-01, -2.9455e-02,  3.4250e-02],\n",
      "        [ 2.4167e-01, -4.2408e-02,  1.8609e-01,  1.8928e-01, -2.0171e-01,\n",
      "          3.5986e-02,  4.2576e-02,  2.8560e-01, -1.4670e-02,  2.6238e-01,\n",
      "          2.4997e-01,  7.2037e-02, -7.3571e-03,  2.4349e-01, -3.4734e-02,\n",
      "          1.8479e-01,  6.0610e-01,  1.4354e-01,  5.0517e-01, -7.7250e-02,\n",
      "         -2.3254e-01, -2.9934e-04,  2.7926e-02, -4.3986e-02],\n",
      "        [ 5.0984e-02,  2.1622e-01,  7.9681e-02,  1.8749e-01, -1.4033e-01,\n",
      "         -6.0002e-02,  5.8869e-02,  2.4085e-02, -2.1017e-01,  6.1826e-02,\n",
      "         -1.6685e-02, -5.3577e-02,  1.0728e-01, -3.6895e-02, -1.5436e-01,\n",
      "          1.4687e-01, -1.8717e-01, -1.0262e-01,  5.1990e-01,  2.4760e-01,\n",
      "          1.2607e-01,  1.1171e-02, -8.2401e-03,  2.1155e-01],\n",
      "        [-1.4928e-01, -1.6569e-01, -6.2134e-02, -1.9842e-01,  3.3908e-02,\n",
      "         -1.9619e-01, -5.2848e-02, -1.3556e-01, -8.5812e-02, -8.3383e-03,\n",
      "         -2.2060e-01,  8.3628e-02, -1.8067e-01,  1.4459e-01,  8.7097e-02,\n",
      "         -4.8203e-02,  7.4186e-03,  9.4903e-03,  6.7443e-03, -6.3458e-02,\n",
      "         -3.1520e-02,  1.0991e-01, -7.5017e-02,  1.0653e-01],\n",
      "        [ 1.4753e-02, -6.2849e-02,  1.1376e-01,  8.4664e-02, -1.4087e-01,\n",
      "          1.6401e-01,  2.3574e-01,  1.4339e-02, -1.9001e-01, -2.5325e-01,\n",
      "          1.2131e-01,  1.2060e-01,  2.2775e-02, -1.2211e-02, -1.4926e-01,\n",
      "         -9.7465e-02, -1.4504e-02, -3.6353e-02,  3.5280e-01, -1.2648e-01,\n",
      "          4.1544e-02,  3.3525e-01,  8.0238e-02,  1.7250e-01],\n",
      "        [ 7.4454e-02, -1.8851e-01,  6.9267e-02,  2.7940e-04, -1.1160e-01,\n",
      "          1.2259e-01,  2.3560e-01, -3.3226e-02,  3.2813e-01,  2.4542e-01,\n",
      "          2.1466e-01,  2.0969e-01,  1.6655e-01,  2.5487e-01,  7.4937e-02,\n",
      "         -2.2033e-02, -2.4558e-01, -4.8578e-01,  3.5446e-01,  1.4569e-01,\n",
      "         -3.1735e-02, -2.4726e-02,  9.2896e-02,  2.5087e-01],\n",
      "        [ 3.3707e-01,  3.8648e-01,  3.4778e-01,  1.6253e-01, -1.3816e-01,\n",
      "         -1.1683e-01, -1.4005e-01,  9.6928e-02,  1.8165e-01,  7.9266e-02,\n",
      "          1.2499e-01,  7.3703e-02, -6.3573e-02,  1.0492e-01,  1.5812e-01,\n",
      "          4.1762e-02, -3.0704e-01,  2.9475e-01,  5.0441e-02,  1.1850e-01,\n",
      "          4.5219e-02,  4.4815e-02,  6.0448e-02,  5.0380e-02],\n",
      "        [ 1.7935e-02, -8.6086e-02, -7.7906e-02, -1.2817e-01,  1.3245e-01,\n",
      "          4.5875e-02,  2.8342e-01,  1.0111e-01, -8.8022e-02,  4.7543e-02,\n",
      "         -2.1956e-01,  1.5785e-01,  1.6527e-02, -5.1751e-02,  1.9384e-01,\n",
      "          5.4009e-01, -3.8080e-01,  2.4336e-02,  1.1306e-01, -2.7942e-01,\n",
      "          2.0362e-01, -6.2293e-02,  6.0822e-02,  5.3640e-01],\n",
      "        [ 1.8837e-01,  1.2192e-01,  3.8142e-02,  1.1061e-01,  1.5892e-01,\n",
      "          1.9052e-01,  3.4359e-01,  2.7403e-01, -8.4265e-02,  2.7562e-01,\n",
      "          1.1986e-01,  2.2377e-01,  3.9900e-03,  5.1255e-02, -1.5290e-02,\n",
      "          4.0357e-01, -1.2074e-01, -1.4072e-01, -1.8108e-02, -4.3998e-01,\n",
      "          6.1315e-02,  1.7376e-02,  1.5545e-02, -1.5023e-01],\n",
      "        [-5.8168e-02, -1.9084e-01, -6.6016e-02,  1.4041e-01, -9.6436e-02,\n",
      "         -2.1532e-02,  2.5110e-02,  5.8951e-02, -2.8441e-02, -1.8734e-01,\n",
      "          4.8746e-02,  5.3899e-02, -2.1680e-01,  1.0603e-01, -1.7931e-01,\n",
      "         -1.1347e-01, -1.2515e-02, -8.4640e-02,  9.2891e-02, -7.0985e-02,\n",
      "          2.4346e-02, -4.4222e-02, -2.5940e-02, -1.8637e-01],\n",
      "        [-6.5472e-02, -6.8672e-02, -1.5334e-01, -1.0814e-01,  5.1525e-02,\n",
      "          7.6990e-02,  3.6127e-01,  2.4362e-01,  3.9203e-03,  9.9452e-02,\n",
      "          3.1226e-01, -9.1083e-02, -4.4470e-02,  3.2643e-02,  6.1325e-01,\n",
      "         -3.0764e-01, -5.7565e-02,  5.0734e-01,  4.6206e-01,  2.0315e-01,\n",
      "         -2.5306e-01,  4.2985e-02, -1.5174e-02,  1.7547e-02],\n",
      "        [-1.2903e-01, -2.4939e-02,  7.3643e-02, -9.6408e-02,  7.3819e-02,\n",
      "         -1.0536e-01,  2.4710e-01,  5.8900e-02, -1.4971e-01,  1.7525e-02,\n",
      "          5.2803e-01,  1.7387e-01, -2.7563e-02,  1.7826e-01, -1.6236e-01,\n",
      "          1.4220e-01, -2.1034e-03, -1.7703e-01,  9.6719e-02, -1.7178e-01,\n",
      "         -5.1976e-02,  8.1392e-02, -4.3424e-02,  3.4695e-02],\n",
      "        [ 1.6509e-01,  6.2164e-02,  6.5406e-04,  1.3905e-01, -1.7013e-02,\n",
      "          1.3373e-01,  3.2727e-01,  8.1208e-02,  2.8655e-01,  2.0676e-01,\n",
      "          1.4374e-01, -2.0018e-01, -5.5623e-03, -8.1045e-02,  2.6865e-02,\n",
      "         -1.0929e-01, -1.8086e-01,  3.4747e-01, -1.9195e-01, -4.5225e-02,\n",
      "         -2.0389e-01,  2.1405e-01, -6.5557e-02,  4.8547e-01],\n",
      "        [ 1.3084e-01,  1.3829e-01,  2.1675e-01,  2.5870e-01, -9.7081e-02,\n",
      "          1.2110e-01,  4.0670e-01, -7.2795e-02, -2.7521e-02,  2.7667e-01,\n",
      "          3.3716e-01, -2.6237e-01, -1.0044e-02,  3.4745e-02, -1.2486e-01,\n",
      "         -4.9173e-03,  2.9516e-01,  2.3707e-02, -3.5530e-01,  7.6525e-02,\n",
      "          2.0724e-02,  2.7835e-01,  9.0275e-02,  1.9425e-01],\n",
      "        [ 2.6243e-03, -2.1638e-02,  1.2724e-01,  3.2963e-02,  1.9367e-01,\n",
      "         -1.5240e-01,  1.4457e-01,  4.4123e-01,  1.4932e-02,  2.6941e-01,\n",
      "          1.2562e-01, -1.4149e-01, -2.0099e-02,  1.5874e-01, -1.9972e-01,\n",
      "          2.8163e-02,  4.1176e-01, -1.9575e-01,  2.2299e-01, -1.2195e-01,\n",
      "          3.1462e-01,  8.7010e-03, -2.6287e-02,  2.2345e-01],\n",
      "        [ 1.3267e-01,  1.3738e-03,  1.4961e-01,  1.7438e-01, -1.6572e-01,\n",
      "          2.4801e-01,  3.8808e-01,  7.4540e-03,  3.5109e-01, -2.1853e-01,\n",
      "          2.2446e-01,  2.7657e-01, -2.4796e-02, -5.6796e-02,  1.8519e-01,\n",
      "          8.6896e-02,  1.3419e-01,  2.0377e-02,  2.6830e-01,  1.6210e-01,\n",
      "          3.9117e-01,  6.4757e-02,  2.8898e-03,  2.7157e-02],\n",
      "        [ 9.8540e-02,  2.2283e-01,  1.1427e-01, -6.4428e-02, -4.8360e-02,\n",
      "         -5.5306e-02, -3.5762e-02, -6.2699e-02,  1.9898e-01,  1.8672e-01,\n",
      "          8.4887e-02, -1.5634e-02,  5.8582e-02,  5.4896e-02,  1.8477e-01,\n",
      "          1.3806e-01,  4.3047e-01, -1.4727e-01,  2.0346e-02, -2.3764e-01,\n",
      "         -2.0303e-01,  9.2889e-02,  3.0884e-02,  3.8431e-01],\n",
      "        [ 1.8544e-01,  1.6329e-01, -1.5732e-01,  1.2461e-01, -1.7007e-01,\n",
      "         -2.3115e-02,  1.5305e-01,  2.4152e-01, -2.2309e-02, -1.7264e-02,\n",
      "          1.7739e-01,  2.9651e-01,  2.1840e-01,  1.8341e-01, -1.8381e-01,\n",
      "         -1.9746e-01,  2.7502e-01, -9.0085e-02, -1.9775e-01,  2.7869e-01,\n",
      "          1.2126e-01, -6.9456e-02,  4.2434e-02, -5.0011e-02],\n",
      "        [ 1.3160e-01, -2.5601e-02,  3.8136e-02,  1.4220e-02, -1.0887e-01,\n",
      "         -4.3867e-02,  2.3493e-02,  4.8594e-01, -9.3871e-02, -3.2259e-02,\n",
      "          1.8201e-02, -1.8574e-01, -5.3541e-02,  5.6423e-02,  1.4083e-01,\n",
      "         -6.6919e-02, -1.0542e-01,  6.1621e-02, -9.4726e-02,  2.2683e-01,\n",
      "         -1.6335e-02,  2.0759e-01,  7.1556e-02, -4.9616e-02],\n",
      "        [ 6.3494e-02,  4.2844e-02,  2.4722e-02,  4.9518e-02, -7.8240e-02,\n",
      "          9.3062e-02,  1.5649e-01, -1.2253e-01, -3.7573e-02,  3.5975e-03,\n",
      "          1.1142e-01,  4.8397e-01, -8.8229e-02, -8.2359e-02,  3.6799e-01,\n",
      "         -2.1629e-01,  1.9085e-02, -1.3012e-01,  6.0586e-02, -4.2853e-02,\n",
      "          2.9152e-01, -2.1653e-01, -2.5315e-02,  3.8442e-01],\n",
      "        [ 3.1679e-02, -1.6967e-02,  7.6925e-02, -1.1462e-02, -7.6675e-02,\n",
      "          5.6462e-02,  1.8862e-01, -1.6420e-01,  1.8324e-01,  2.0084e-01,\n",
      "          1.3589e-01,  4.4362e-02, -4.2958e-02, -1.3539e-01,  7.0020e-02,\n",
      "          4.8287e-01,  8.8107e-02,  9.8467e-02, -2.4245e-01,  1.7382e-01,\n",
      "          7.1664e-02,  4.8194e-01,  3.8914e-03, -1.1237e-01],\n",
      "        [ 8.1838e-02, -1.0543e-01,  2.3963e-01,  1.9669e-02,  5.9198e-02,\n",
      "          2.5386e-02,  2.6880e-01, -1.3454e-01, -2.4776e-01,  2.1271e-01,\n",
      "          4.5087e-02,  9.9224e-02, -5.8442e-03, -1.0278e-02,  3.9521e-01,\n",
      "          1.1476e-01,  1.4462e-01, -2.7336e-01,  1.6652e-01,  3.1012e-01,\n",
      "          8.6135e-03,  6.6628e-02,  5.8262e-02, -1.0262e-01],\n",
      "        [-1.0230e-03, -1.9489e-01,  8.9090e-02,  1.3764e-01,  1.7240e-02,\n",
      "          1.9847e-01, -1.7081e-01,  3.8229e-02, -3.2150e-02,  2.5597e-01,\n",
      "         -1.7281e-01,  2.5308e-01,  7.7970e-02,  1.3400e-01,  4.0112e-01,\n",
      "          1.0682e-01, -4.8488e-02,  6.6624e-04, -6.4875e-02,  9.7236e-02,\n",
      "          1.9389e-01,  4.1880e-01, -2.3874e-02,  1.7010e-01],\n",
      "        [ 7.0058e-03, -1.0094e-01,  6.6758e-02, -6.6005e-02, -1.9135e-02,\n",
      "          3.9149e-01,  4.5555e-01,  1.0815e-01, -1.7584e-01,  2.9700e-01,\n",
      "         -2.4340e-02,  8.8118e-02, -1.5946e-02,  1.4569e-02,  2.1679e-01,\n",
      "          2.4158e-01, -2.2423e-01, -1.1422e-01, -2.3774e-01, -5.4315e-02,\n",
      "          2.2158e-01, -1.3446e-02, -1.9881e-02,  2.0115e-01],\n",
      "        [-1.1253e-01, -1.5383e-01, -1.1857e-01,  1.0388e-01,  8.6805e-02,\n",
      "         -1.7576e-01, -1.0004e-01, -3.5409e-02, -1.5101e-01, -1.3927e-01,\n",
      "          1.0365e-01, -1.6674e-01, -4.1678e-02,  1.4952e-01, -1.2330e-01,\n",
      "         -4.5201e-02, -1.1214e-01, -1.9967e-01,  7.7979e-03,  1.0135e-01,\n",
      "         -7.7542e-02,  1.1135e-01,  7.1484e-02, -1.6985e-01],\n",
      "        [ 1.7260e-01,  1.9773e-01,  1.1927e-01,  1.7471e-02,  3.2626e-02,\n",
      "          1.2118e-01,  2.0962e-01, -8.7449e-02, -5.3461e-02,  1.7387e-01,\n",
      "          1.0265e-01, -9.0114e-02, -9.8807e-02,  1.6032e-01, -9.8625e-02,\n",
      "          1.1111e-01, -5.6872e-02,  9.4058e-02,  2.5664e-01,  2.6821e-01,\n",
      "         -1.6508e-01,  2.0345e-01, -1.2135e-01,  1.8855e-01],\n",
      "        [ 1.8681e-02, -1.2154e-01, -7.2342e-02,  5.2793e-02,  1.4928e-01,\n",
      "         -2.6289e-01,  7.8836e-02,  7.8528e-02, -2.0232e-01,  6.5315e-02,\n",
      "          3.4305e-01,  1.4537e-01, -2.6474e-02,  2.1090e-01, -2.0515e-01,\n",
      "          2.6942e-01, -6.3197e-02,  1.7516e-01, -5.9024e-03,  3.6191e-01,\n",
      "         -7.6388e-02,  3.0290e-01, -1.3583e-01,  4.1042e-01]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-0.0148,  0.0222,  0.0593, -0.0463,  0.1540, -0.0121, -0.0387,  0.1552,\n",
      "        -0.2104,  0.3677,  0.1121,  0.0535,  0.2676,  0.1845, -0.1093,  0.0074,\n",
      "         0.3630,  0.0633,  0.0730, -0.0921, -0.0140,  0.1511,  0.2011,  0.1961,\n",
      "         0.2244,  0.1643,  0.1930, -0.1035,  0.1301, -0.1514,  0.0817,  0.4256],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-2.8122e-02, -9.2304e-02, -1.8003e-01,  2.6888e-01,  1.3343e-01,\n",
      "          1.5739e-01, -8.0063e-02,  1.8170e-01, -1.3680e-01,  2.8660e-01,\n",
      "          7.1910e-02,  2.2238e-01, -1.4900e-01, -2.6810e-02,  1.6242e-01,\n",
      "         -4.2967e-01,  1.5398e-01, -1.1340e-01,  5.3083e-02, -4.1914e-01,\n",
      "          2.2303e-01,  1.8868e-01, -2.8519e-02, -2.0281e-01,  1.4200e-01,\n",
      "          2.7149e-01,  1.8698e-01,  1.6041e-01, -2.9928e-01, -1.1841e-01,\n",
      "          7.7823e-02,  1.8190e-01],\n",
      "        [ 1.1483e-02,  2.9020e-01, -3.3414e-02,  1.3380e-01,  8.7141e-02,\n",
      "         -8.7564e-02, -1.5607e-01, -5.0522e-02,  1.3267e-01,  6.8699e-02,\n",
      "          2.3859e-01, -2.6892e-01,  4.0041e-01,  1.1532e-01, -1.5485e-01,\n",
      "         -1.8963e-02,  1.8053e-01,  3.1859e-01, -5.9230e-02,  2.4596e-01,\n",
      "          3.4713e-01,  3.3209e-01, -7.8594e-02, -2.3883e-01,  2.7752e-01,\n",
      "         -8.9033e-02, -2.4389e-01, -2.0993e-01,  1.4533e-01, -1.5622e-02,\n",
      "         -1.3485e-01, -3.4924e-02],\n",
      "        [ 7.3121e-02, -1.3637e-01, -1.6056e-01,  1.5306e-01,  1.4871e-01,\n",
      "          5.7521e-01,  1.9036e-01,  2.6147e-01,  1.4301e-01,  2.7698e-01,\n",
      "          4.6548e-01, -1.6630e-01, -1.3244e-01,  6.7873e-02, -2.9013e-02,\n",
      "          1.3741e-01,  1.0670e-01, -1.2005e-01, -1.2189e-01,  5.0509e-01,\n",
      "         -4.1807e-02, -1.8299e-02, -7.9029e-02,  6.0371e-02, -3.7967e-01,\n",
      "         -4.5772e-03, -1.1349e-01, -1.5083e-01, -2.2204e-01, -1.3679e-01,\n",
      "          2.1582e-01, -2.5469e-02],\n",
      "        [ 1.8657e-01, -1.2853e-01, -2.1181e-01, -1.4728e-01,  3.5796e-01,\n",
      "          3.6457e-01, -3.4594e-01, -8.1927e-02,  8.8850e-02, -1.7936e-01,\n",
      "          3.9918e-01, -1.5020e-01, -8.5481e-02, -2.0043e-01, -8.7161e-02,\n",
      "          4.8921e-02,  1.6058e-01,  2.3298e-01,  1.4426e-01,  4.8319e-02,\n",
      "         -2.3339e-01,  9.2978e-02,  1.4392e-01,  1.9288e-01,  2.8832e-01,\n",
      "          1.0372e-01,  3.1685e-01,  1.8280e-01,  1.6772e-01,  1.4101e-01,\n",
      "         -8.8388e-02,  1.9285e-01],\n",
      "        [ 2.0023e-01, -1.2265e-01,  1.1991e-01,  3.1553e-01,  2.1134e-01,\n",
      "         -2.6649e-02,  2.0584e-02, -3.5841e-02,  3.8947e-02, -2.9502e-01,\n",
      "         -1.9112e-01,  1.2942e-01,  2.7484e-01,  1.5641e-01,  1.3633e-01,\n",
      "          1.0080e-01, -3.2172e-01,  1.4845e-02,  3.6411e-02,  7.8006e-02,\n",
      "         -8.9287e-02,  1.0665e-01, -1.1468e-01,  1.3139e-01, -1.7592e-01,\n",
      "          1.9240e-01,  1.5131e-01,  3.0740e-01,  2.5340e-01,  9.6842e-02,\n",
      "          1.0992e-01, -9.0703e-02],\n",
      "        [-9.7316e-02,  2.9755e-01, -2.0378e-02, -3.7086e-01,  4.4214e-01,\n",
      "         -1.2304e-01,  2.2105e-02,  3.3609e-01,  5.9560e-02,  1.9742e-01,\n",
      "         -9.5612e-02, -6.7798e-02,  2.6040e-01, -2.5418e-01,  8.0976e-02,\n",
      "         -5.1041e-02, -3.2439e-01, -4.2846e-02,  7.4587e-02,  3.2447e-01,\n",
      "          3.9942e-02, -7.4345e-02, -6.5724e-02, -1.1773e-01, -1.1150e-01,\n",
      "          1.5330e-01, -6.0619e-02,  1.3005e-01,  1.0471e-01,  8.2663e-02,\n",
      "          2.2069e-01,  9.2686e-02],\n",
      "        [ 1.0357e-01,  3.0654e-01,  8.2127e-02,  1.9905e-01,  1.0395e-01,\n",
      "          3.8272e-02, -1.4739e-01, -7.6699e-02, -9.5977e-02, -2.6468e-01,\n",
      "          2.4414e-01, -8.6734e-02,  6.0146e-02,  2.3529e-01,  2.4989e-02,\n",
      "         -2.2548e-01,  3.3634e-01, -2.8865e-01, -1.3863e-01, -5.1604e-02,\n",
      "         -3.0142e-02, -3.5374e-01,  1.5806e-01,  4.4500e-02, -1.2430e-01,\n",
      "          3.1405e-01,  1.1220e-01, -1.6810e-01,  2.7476e-01, -7.7068e-02,\n",
      "         -9.3705e-02,  1.7310e-01],\n",
      "        [ 4.3410e-02,  9.1672e-02,  2.0796e-01, -2.1484e-01, -1.1041e-01,\n",
      "          1.5187e-01, -9.8752e-02, -3.9159e-02, -3.6153e-02, -4.3675e-02,\n",
      "         -8.7361e-02,  7.3972e-02, -3.4329e-01,  5.2876e-02, -6.1989e-02,\n",
      "         -1.2262e-01, -1.8312e-01,  1.5418e-01,  2.2431e-01,  2.2655e-01,\n",
      "          4.2686e-01, -1.0258e-01,  3.1338e-01,  1.4913e-01, -3.1837e-03,\n",
      "          1.1846e-01, -1.0466e-01, -7.5683e-02, -2.6160e-02, -3.5503e-02,\n",
      "          1.6955e-01, -1.0835e-01],\n",
      "        [ 3.2164e-01,  1.0326e-01,  4.0031e-02,  1.0886e-01,  2.5044e-02,\n",
      "         -1.0875e-01, -7.8150e-04,  1.2344e-01, -6.1155e-02,  7.0877e-02,\n",
      "          1.9914e-03, -1.4153e-02,  9.6317e-04, -2.2333e-01, -9.3469e-02,\n",
      "          6.2192e-01, -1.1093e-02, -3.3242e-01, -2.4227e-01, -1.9770e-01,\n",
      "          2.4662e-01, -1.5419e-01,  1.1619e-01,  4.5626e-02,  1.1619e-01,\n",
      "          1.0068e-01,  2.3043e-01,  1.8342e-04, -2.1085e-01,  5.8011e-02,\n",
      "          1.2810e-01,  1.0743e-01],\n",
      "        [ 6.7455e-02, -5.9114e-02,  6.1561e-02,  1.7241e-01,  5.0366e-02,\n",
      "         -1.3238e-01, -1.6308e-01,  3.0116e-01,  1.4310e-02,  1.3459e-01,\n",
      "          2.6889e-01,  2.0011e-01,  7.3273e-02,  1.4235e-01, -2.6022e-02,\n",
      "         -3.2658e-02,  1.4357e-01,  4.7913e-01,  2.7529e-01, -1.0692e-01,\n",
      "         -8.2155e-02, -2.8486e-01, -1.7391e-01,  1.0316e-01,  1.6794e-02,\n",
      "         -1.8409e-01,  6.7870e-02, -3.0808e-01,  3.4073e-01,  1.7345e-01,\n",
      "          2.0640e-01,  5.6146e-02],\n",
      "        [ 7.0363e-02,  3.7829e-02, -6.9944e-03, -3.6525e-02, -7.2924e-02,\n",
      "         -1.9348e-01,  1.4718e-01, -1.6247e-01, -1.0872e-01, -4.5557e-02,\n",
      "         -1.4406e-01, -8.9696e-02, -1.1243e-01, -1.8399e-01, -7.8629e-03,\n",
      "         -1.7293e-01, -1.4539e-01,  2.0957e-02, -1.2556e-01, -5.9699e-02,\n",
      "          7.7661e-02,  7.7664e-02, -1.3955e-01,  1.0578e-01, -9.4482e-02,\n",
      "          3.4448e-03,  1.4945e-01,  1.0046e-01,  1.0331e-01,  9.4697e-02,\n",
      "         -1.5873e-01,  3.1047e-02],\n",
      "        [-2.9978e-01, -7.8321e-02,  1.6829e-01,  1.0316e-01,  3.2947e-01,\n",
      "          2.9179e-02,  9.7135e-02, -9.9426e-02, -6.7565e-02,  3.7604e-01,\n",
      "         -3.2961e-01, -3.9874e-01, -2.9476e-02,  1.0646e-01,  1.0757e-01,\n",
      "          6.9734e-02,  2.8085e-01, -1.4470e-01,  3.4557e-01, -7.7017e-02,\n",
      "          8.6714e-02,  6.1730e-02,  1.5975e-01,  2.3640e-02,  2.1462e-01,\n",
      "         -3.6302e-02,  4.8768e-01, -6.5251e-02,  2.7706e-01,  4.5300e-02,\n",
      "          1.0456e-01, -1.0303e-02],\n",
      "        [ 4.4877e-03,  1.2735e-01, -1.0455e-01,  1.0995e-02, -1.5771e-01,\n",
      "         -6.6911e-02,  2.4141e-01,  2.3289e-01,  6.8810e-02, -2.2187e-01,\n",
      "          5.7599e-02,  2.0574e-01, -1.2543e-02, -1.9533e-02,  1.3086e-01,\n",
      "         -5.8102e-02,  2.6631e-01, -2.2683e-01,  1.0665e-01,  3.6316e-01,\n",
      "         -2.8446e-01,  2.8271e-01,  2.8486e-01,  1.0574e-01,  2.8051e-01,\n",
      "         -6.3854e-02,  3.1357e-01,  6.0475e-03, -3.7011e-02, -1.2535e-01,\n",
      "          1.4613e-01,  2.9478e-01],\n",
      "        [ 1.5825e-01,  1.4260e-01,  6.2718e-02, -1.5224e-02, -1.2319e-01,\n",
      "         -2.3587e-02,  2.3523e-02, -7.1272e-02, -1.2487e-01,  3.1137e-01,\n",
      "         -1.1478e-01,  6.6367e-03,  5.8663e-02, -9.6077e-02,  4.3578e-02,\n",
      "          2.6989e-01,  3.6725e-01,  4.9410e-01,  2.5707e-01, -5.6289e-02,\n",
      "         -6.8886e-02,  2.6873e-01, -2.6460e-03,  1.8157e-01, -5.0964e-01,\n",
      "          2.4129e-01, -1.6653e-01, -7.6051e-02, -2.9125e-01,  1.4259e-01,\n",
      "         -7.4479e-02,  3.5013e-01],\n",
      "        [-3.9064e-01,  1.5559e-01,  3.5984e-02,  8.1721e-02, -3.0312e-01,\n",
      "          4.1011e-01, -4.2998e-02,  5.7679e-02,  1.8086e-02,  2.7894e-01,\n",
      "          8.3383e-02,  1.3079e-01,  2.4969e-01,  1.7193e-03,  2.9213e-02,\n",
      "          1.7998e-01, -5.2661e-02,  6.6074e-02, -1.5316e-01, -2.1769e-01,\n",
      "         -7.5219e-02, -1.5553e-03,  1.8545e-01,  2.5253e-01,  1.5435e-01,\n",
      "         -1.7230e-01, -4.2353e-02,  3.6824e-01, -4.7417e-02, -1.3102e-01,\n",
      "         -4.5982e-02,  1.9687e-01],\n",
      "        [ 3.4789e-01, -3.2020e-01, -2.0770e-03, -2.4470e-01,  1.0827e-01,\n",
      "          3.2069e-02, -2.7413e-01,  1.9683e-01,  1.6679e-01,  3.4798e-01,\n",
      "         -3.4421e-01,  8.4474e-02,  3.0519e-01,  1.1133e-01,  1.0717e-01,\n",
      "         -1.7244e-01,  2.6613e-01, -1.3775e-01, -3.6294e-02,  1.7234e-01,\n",
      "          1.6605e-01, -8.9449e-02,  9.4902e-02,  9.7168e-02,  3.3180e-01,\n",
      "          7.1351e-04, -1.3444e-02,  1.4538e-01,  6.7586e-02,  1.2939e-01,\n",
      "         -1.8824e-01,  2.2117e-01]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-0.0738, -0.0722, -0.0030,  0.2579,  0.1021, -0.0564,  0.1942,  0.1934,\n",
      "         0.1221,  0.1723, -0.0236, -0.0062, -0.0588,  0.0805,  0.1772,  0.1682],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoder = torch.load(\"encoder.pth\")\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    print(param)\n",
    "\n",
    "modules = [encoder]\n",
    "modules.append(torch.nn.Linear(16, 16))\n",
    "modules.append(torch.nn.Linear(16,12))\n",
    "model = MyMLP(modules)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 1950/1950 [00:05<00:00, 359.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  2.3669971600556985\n",
      "Accuracy:  0.2544471153846154\n",
      "Validation Loss:  2.2934760622313766\n",
      "Validation Accuracy:  0.3462634477459016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1950/1950 [00:04<00:00, 409.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  2.270648723504482\n",
      "Accuracy:  0.3690665064102564\n",
      "Validation Loss:  2.2456896818075025\n",
      "Validation Accuracy:  0.393890881147541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1950/1950 [00:04<00:00, 400.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  Loss:  2.2306870327240382\n",
      "Accuracy:  0.40306490384615384\n",
      "Validation Loss:  2.2215047082940087\n",
      "Validation Accuracy:  0.4067622950819672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1950/1950 [00:05<00:00, 388.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  Loss:  2.217719182479076\n",
      "Accuracy:  0.4103485576923077\n",
      "Validation Loss:  2.2130843142016987\n",
      "Validation Accuracy:  0.41446273053278687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1950/1950 [00:05<00:00, 383.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4  Loss:  2.2093342067033817\n",
      "Accuracy:  0.41665865384615386\n",
      "Validation Loss:  2.2059392660367685\n",
      "Validation Accuracy:  0.419921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1950/1950 [00:04<00:00, 410.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  Loss:  2.2041846643350063\n",
      "Accuracy:  0.4198076923076923\n",
      "Validation Loss:  2.2023264575200003\n",
      "Validation Accuracy:  0.42333183913934425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1950/1950 [00:04<00:00, 431.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6  Loss:  2.200918201911144\n",
      "Accuracy:  0.42232772435897437\n",
      "Validation Loss:  2.198693265191844\n",
      "Validation Accuracy:  0.42482069672131145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1950/1950 [00:04<00:00, 415.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7  Loss:  2.198523012063442\n",
      "Accuracy:  0.42396233974358977\n",
      "Validation Loss:  2.1962737934511214\n",
      "Validation Accuracy:  0.4276063012295082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1950/1950 [00:04<00:00, 392.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  Loss:  2.1965549081411115\n",
      "Accuracy:  0.42582131410256413\n",
      "Validation Loss:  2.1965509394153218\n",
      "Validation Accuracy:  0.42477266905737704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1950/1950 [00:04<00:00, 430.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9  Loss:  2.194837774619078\n",
      "Accuracy:  0.42692708333333335\n",
      "Validation Loss:  2.193482826479146\n",
      "Validation Accuracy:  0.42755827356557374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1950/1950 [00:04<00:00, 405.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  Loss:  2.193172877629598\n",
      "Accuracy:  0.428349358974359\n",
      "Validation Loss:  2.1920139418273674\n",
      "Validation Accuracy:  0.42971951844262296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1950/1950 [00:04<00:00, 394.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11  Loss:  2.1909453099813216\n",
      "Accuracy:  0.43107772435897435\n",
      "Validation Loss:  2.190336556219664\n",
      "Validation Accuracy:  0.4326652151639344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1950/1950 [00:04<00:00, 398.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12  Loss:  2.189569254899636\n",
      "Accuracy:  0.4326522435897436\n",
      "Validation Loss:  2.188042845393791\n",
      "Validation Accuracy:  0.433257556352459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1950/1950 [00:04<00:00, 399.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13  Loss:  2.1883527655479234\n",
      "Accuracy:  0.43404647435897437\n",
      "Validation Loss:  2.187578053259459\n",
      "Validation Accuracy:  0.43458632172131145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1950/1950 [00:04<00:00, 431.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14  Loss:  2.187247787255507\n",
      "Accuracy:  0.43513221153846154\n",
      "Validation Loss:  2.186283449169065\n",
      "Validation Accuracy:  0.4357870133196721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1950/1950 [00:04<00:00, 409.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15  Loss:  2.186188565034133\n",
      "Accuracy:  0.4359815705128205\n",
      "Validation Loss:  2.1862297155818\n",
      "Validation Accuracy:  0.4359951331967213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 1950/1950 [00:05<00:00, 388.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16  Loss:  2.1851418550197894\n",
      "Accuracy:  0.43720753205128204\n",
      "Validation Loss:  2.184450410917157\n",
      "Validation Accuracy:  0.4373078893442623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 1950/1950 [00:04<00:00, 413.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17  Loss:  2.1841817497595764\n",
      "Accuracy:  0.43778044871794874\n",
      "Validation Loss:  2.1839540576348537\n",
      "Validation Accuracy:  0.43748399077868855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 1950/1950 [00:04<00:00, 399.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18  Loss:  2.1832224002251257\n",
      "Accuracy:  0.4387580128205128\n",
      "Validation Loss:  2.182413242879461\n",
      "Validation Accuracy:  0.4390368852459016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 1950/1950 [00:05<00:00, 346.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19  Loss:  2.1822624233441474\n",
      "Accuracy:  0.43969551282051283\n",
      "Validation Loss:  2.181828617072496\n",
      "Validation Accuracy:  0.43993340163934425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 1950/1950 [00:05<00:00, 339.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20  Loss:  2.1812717882792154\n",
      "Accuracy:  0.4403605769230769\n",
      "Validation Loss:  2.18109512231389\n",
      "Validation Accuracy:  0.4395491803278688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 1950/1950 [00:05<00:00, 369.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21  Loss:  2.180479074258071\n",
      "Accuracy:  0.4410857371794872\n",
      "Validation Loss:  2.1797690376883647\n",
      "Validation Accuracy:  0.4415023053278688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 1950/1950 [00:05<00:00, 332.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  22  Loss:  2.1796536935904087\n",
      "Accuracy:  0.44181891025641024\n",
      "Validation Loss:  2.179684088855493\n",
      "Validation Accuracy:  0.44180648053278687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 1950/1950 [00:05<00:00, 368.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23  Loss:  2.1789707562862297\n",
      "Accuracy:  0.44247195512820514\n",
      "Validation Loss:  2.178694630255465\n",
      "Validation Accuracy:  0.44313524590163933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 1950/1950 [00:04<00:00, 421.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  24  Loss:  2.178314090508681\n",
      "Accuracy:  0.44302483974358975\n",
      "Validation Loss:  2.1785296274990333\n",
      "Validation Accuracy:  0.44220671106557374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 1950/1950 [00:04<00:00, 411.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  25  Loss:  2.177713516797775\n",
      "Accuracy:  0.4437139423076923\n",
      "Validation Loss:  2.178016988468952\n",
      "Validation Accuracy:  0.44334336577868855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 1950/1950 [00:04<00:00, 394.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  26  Loss:  2.177171616921058\n",
      "Accuracy:  0.4442067307692308\n",
      "Validation Loss:  2.177335857856469\n",
      "Validation Accuracy:  0.4444159836065574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 1950/1950 [00:04<00:00, 396.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  27  Loss:  2.1766752434999517\n",
      "Accuracy:  0.444599358974359\n",
      "Validation Loss:  2.1768032590873907\n",
      "Validation Accuracy:  0.4427510245901639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 1950/1950 [00:04<00:00, 410.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  28  Loss:  2.176154643450028\n",
      "Accuracy:  0.4453205128205128\n",
      "Validation Loss:  2.1758969592266397\n",
      "Validation Accuracy:  0.44603291495901637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 1950/1950 [00:04<00:00, 426.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  29  Loss:  2.175803512671055\n",
      "Accuracy:  0.4455889423076923\n",
      "Validation Loss:  2.1755455351266706\n",
      "Validation Accuracy:  0.44580878586065575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 1950/1950 [00:04<00:00, 401.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  30  Loss:  2.175504060402895\n",
      "Accuracy:  0.4454607371794872\n",
      "Validation Loss:  2.1762741618469112\n",
      "Validation Accuracy:  0.44431992827868855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 1950/1950 [00:04<00:00, 390.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  31  Loss:  2.1750646017759276\n",
      "Accuracy:  0.44609375\n",
      "Validation Loss:  2.175939687451378\n",
      "Validation Accuracy:  0.4439196977459016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|██████████| 1950/1950 [00:05<00:00, 375.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  32  Loss:  2.1748364347066635\n",
      "Accuracy:  0.44603365384615384\n",
      "Validation Loss:  2.1751119650778223\n",
      "Validation Accuracy:  0.4456326844262295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 1950/1950 [00:04<00:00, 416.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  33  Loss:  2.174640842584463\n",
      "Accuracy:  0.4462459935897436\n",
      "Validation Loss:  2.17448107287532\n",
      "Validation Accuracy:  0.44598488729508196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 1950/1950 [00:04<00:00, 398.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  34  Loss:  2.1744514791782086\n",
      "Accuracy:  0.44634214743589745\n",
      "Validation Loss:  2.1759921077822075\n",
      "Validation Accuracy:  0.44337538422131145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 1950/1950 [00:04<00:00, 400.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  35  Loss:  2.1741723953149257\n",
      "Accuracy:  0.4463741987179487\n",
      "Validation Loss:  2.174782210197605\n",
      "Validation Accuracy:  0.4449442879098361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 1950/1950 [00:04<00:00, 401.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  36  Loss:  2.174058597026727\n",
      "Accuracy:  0.4466826923076923\n",
      "Validation Loss:  2.173992547343989\n",
      "Validation Accuracy:  0.4463370901639344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|██████████| 1950/1950 [00:04<00:00, 428.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  37  Loss:  2.1739250069398146\n",
      "Accuracy:  0.44649439102564104\n",
      "Validation Loss:  2.174544637809034\n",
      "Validation Accuracy:  0.44603291495901637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 1950/1950 [00:04<00:00, 391.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  38  Loss:  2.1737858371245555\n",
      "Accuracy:  0.44685897435897437\n",
      "Validation Loss:  2.174075817964116\n",
      "Validation Accuracy:  0.44651319159836067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 1950/1950 [00:04<00:00, 392.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  39  Loss:  2.173456114255465\n",
      "Accuracy:  0.4470633012820513\n",
      "Validation Loss:  2.173505510951652\n",
      "Validation Accuracy:  0.4464171362704918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 1950/1950 [00:04<00:00, 426.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  40  Loss:  2.1733987299601236\n",
      "Accuracy:  0.4471995192307692\n",
      "Validation Loss:  2.173035312871464\n",
      "Validation Accuracy:  0.44742571721311475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 1950/1950 [00:04<00:00, 408.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  41  Loss:  2.173200314350617\n",
      "Accuracy:  0.4472676282051282\n",
      "Validation Loss:  2.1730696788576784\n",
      "Validation Accuracy:  0.44672131147540983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|██████████| 1950/1950 [00:04<00:00, 401.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  42  Loss:  2.173050876152821\n",
      "Accuracy:  0.44724358974358974\n",
      "Validation Loss:  2.1736425664581236\n",
      "Validation Accuracy:  0.4468974129098361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|██████████| 1950/1950 [00:05<00:00, 349.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  43  Loss:  2.172976403969985\n",
      "Accuracy:  0.4472956730769231\n",
      "Validation Loss:  2.173555323334991\n",
      "Validation Accuracy:  0.44625704405737704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|██████████| 1950/1950 [00:05<00:00, 386.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  44  Loss:  2.1728582017849654\n",
      "Accuracy:  0.4474679487179487\n",
      "Validation Loss:  2.17245708674681\n",
      "Validation Accuracy:  0.4480500768442623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|██████████| 1950/1950 [00:04<00:00, 410.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  45  Loss:  2.172590602361239\n",
      "Accuracy:  0.44774839743589745\n",
      "Validation Loss:  2.1725079050806704\n",
      "Validation Accuracy:  0.4471855788934426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46: 100%|██████████| 1950/1950 [00:05<00:00, 380.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  46  Loss:  2.1725989708533655\n",
      "Accuracy:  0.44756410256410256\n",
      "Validation Loss:  2.1722887730012173\n",
      "Validation Accuracy:  0.44854636270491804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47:  72%|███████▏  | 1397/1950 [00:03<00:01, 393.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNoise_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNoise_test_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[168], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, num_epochs, verbos, lr)\u001b[0m\n\u001b[0;32m     10\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[153], line 9\u001b[0m, in \u001b[0;36mMyMLP.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m----> 9\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(X)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,torch.nn.CrossEntropyLoss(),torch.optim.Adam,Noise_train_loader,Noise_test_loader,num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
