{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noise_0_dataframe = pd.read_csv(\"Data/Assignment1/data_0_noise\")\n",
    "Noise_Low_dataframe = pd.read_csv(\"Data/Assignment1/data_Low_noise\")\n",
    "Noise_High_dataframe = pd.read_csv(\"Data/Assignment1/data_High_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=Noise_High_dataframe\n",
    "validation_dataframe=Noise_High_dataframe\n",
    "target_columns=\"target_10_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode = \"target_10_val\"\n",
    "class_index = list(dataframe[to_encode].unique())\n",
    "def encode(value, class_index = class_index):\n",
    "    return class_index.index(value)\n",
    "\n",
    "dataframe[to_encode] = dataframe[to_encode].apply(encode)\n",
    "validation_dataframe[to_encode] = validation_dataframe[to_encode].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, noise, transform=None, target_transform=None, drop=None, target=None):\n",
    "        self.dataframe = dataframe\n",
    "        if drop != None:\n",
    "            self.X = dataframe.drop(drop, axis=1).values\n",
    "        else:\n",
    "            self.X = dataframe.values\n",
    "        \n",
    "        self.y = dataframe[target].values\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.noise = noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, label = self.X[idx], self.y[idx]\n",
    "        return item, label\n",
    "\n",
    "    def get_noise(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(dataframe, \"0\",drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\",\"data_type\"],target=target_columns)\n",
    "Noise_train, Noise_test = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "Noise_train_loader = DataLoader(Noise_train, batch_size=128, shuffle=True)\n",
    "Noise_test_loader = DataLoader(Noise_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(torch.nn.Module):\n",
    "    def __init__(self,encoder,latent_dim,linear,subset_size,overlap,activation = torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.linear = [torch.nn.Linear(linear[i],linear[i+1]) for i in range(len(linear)-1)]\n",
    "        self.linear = torch.nn.Sequential(*[l for layer in self.linear for l in (layer, activation)])\n",
    "        self.subset_size = subset_size\n",
    "        self.overlap = overlap\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "        self.att_layer = torch.nn.Linear(latent_dim,1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        subsets = []\n",
    "        num_columns = X.shape[1]\n",
    "        for i in range(0, num_columns-self.subset_size,self.subset_size-self.overlap):\n",
    "            subsets.append(X[:,i:i+self.subset_size])\n",
    "        \n",
    "        preds = 0\n",
    "        h = 0\n",
    "        for subset in subsets:\n",
    "            pred = self.encoder(subset)\n",
    "            h = self.att_layer(pred)\n",
    "            pred = self.linear(pred)\n",
    "            pred = self.softmax(pred)\n",
    "            preds += pred\n",
    "        \n",
    "        h = h/len(subsets)\n",
    "        h = self.sigmoid(h)     \n",
    "        preds = preds/len(subsets)\n",
    "        \n",
    "        return preds,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetAutoencoder (torch.nn.Module):\n",
    "    def __init__(self, encoder_sizes,decoder_sizes,activation = torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        linear_encoder = [torch.nn.Linear(encoder_sizes[i],encoder_sizes[i+1]) for i in range(len(encoder_sizes)-1)]\n",
    "        linear_decoder = [torch.nn.Linear(decoder_sizes[i],decoder_sizes[i+1]) for i in range(len(decoder_sizes)-1)]\n",
    "        self.encoder = torch.nn.Sequential(*[l for layer in linear_encoder for l in (layer, activation)])\n",
    "        self.decoder = torch.nn.Sequential(*[l for layer in linear_decoder for l in (layer, activation)])\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.decoder(X)\n",
    "        return X\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self,reg,no_of_class=12):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.reg=reg\n",
    "        self.no_of_class=no_of_class\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        p=torch.Tensor(inputs[0]).to(device).t()\n",
    "        # Converting to 64 X 12 to 64 X 1\n",
    "        # _,p=torch.max(p,1)\n",
    "        targets=torch.eye(self.no_of_class).to(device)[targets].t()\n",
    "        tou=torch.Tensor(inputs[1]).to(device)\n",
    "        tou=tou.t()\n",
    "        # print(\"tou\",tou)\n",
    "        # print(tou)\n",
    "        loss_a= torch.t(targets)@torch.log(tou*(p-targets)+targets)\n",
    "        loss_b=self.reg*torch.log(tou)\n",
    "        loss_a=loss_a.diag().t()\n",
    "        # print(\"Loss a\",loss_a)\n",
    "        # print(\"Loss b\",loss_b)\n",
    "        loss=loss_a+loss_b\n",
    "        # print(\"loss\",loss.shape)\n",
    "        # print(\"return\",loss.mean().shape)\n",
    "        return -loss.mean()\n",
    "\n",
    "    def custom_p(p):\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsets(current_batch,subset_size,overlap):\n",
    "        subsets = []\n",
    "        num_columns = current_batch.shape[1]\n",
    "        for i in range(0, num_columns-subset_size,subset_size-overlap):\n",
    "            subsets.append(current_batch[:,i:i+subset_size])\n",
    "        return subsets\n",
    "\n",
    "\n",
    "def train_ae(model,criterion,optimizer,epochs,trainloader,testloader,subset_size = 10,overlap = 4,lr = 0.001 , verbose = True):\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        current_train_loss = 0\n",
    "        current_accuracy = []\n",
    "        for data, _ in tqdm(trainloader,desc = \"Training Epoch \"+str(epoch)):\n",
    "            data= data.to(device).float()\n",
    "            subsets = get_subsets(data,subset_size,overlap)\n",
    "            optimizer.zero_grad()\n",
    "            recons = []\n",
    "            subset_loss = 0\n",
    "            for subset in subsets:\n",
    "                output = model(subset)\n",
    "                recons.append(output)\n",
    "                subset_loss += criterion(data,output)\n",
    "            subset_loss = subset_loss.mean()\n",
    "            subset_loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch-{epoch} loss:\",subset_loss)\n",
    "        \n",
    "def train_classifier(model,criterion,optimizer,epochs,trainloader,testloader,lr=0.001,verbose = True,subset_size = 10,overlap = 2):\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        current_accuracy = []\n",
    "        for data, target in tqdm(trainloader,desc = \"Training Epoch \"+str(epoch)):\n",
    "            data, target = data.to(device).float(), target.to(device).long()\n",
    "            subsets = get_subsets(data,subset_size,overlap)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(output[0], 1)\n",
    "            total_train += target.size(0)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "            current_accuracy.append(correct_train/total_train)\n",
    "            running_train_loss+=loss\n",
    "        \n",
    "        running_train_loss /= len(trainloader)    \n",
    "        print(f\"epoch-{epoch} loss:{running_train_loss} accuracy:{correct_train/total_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubsetAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=24, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SubsetAutoencoder(encoder_sizes=[12,32,16,8],decoder_sizes=[8,16,32,24])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 1560/1560 [00:07<00:00, 222.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 loss: tensor(0.2082, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1560/1560 [00:06<00:00, 224.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-1 loss: tensor(0.2196, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1560/1560 [00:07<00:00, 222.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-2 loss: tensor(0.2081, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1560/1560 [00:06<00:00, 224.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-3 loss: tensor(0.2107, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1560/1560 [00:06<00:00, 236.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-4 loss: tensor(0.1855, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1560/1560 [00:06<00:00, 236.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-5 loss: tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1560/1560 [00:06<00:00, 224.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-6 loss: tensor(0.1974, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1560/1560 [00:06<00:00, 227.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-7 loss: tensor(0.1941, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1560/1560 [00:06<00:00, 226.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-8 loss: tensor(0.1831, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1560/1560 [00:06<00:00, 225.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-9 loss: tensor(0.1848, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1560/1560 [00:06<00:00, 232.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-10 loss: tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1560/1560 [00:06<00:00, 237.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-11 loss: tensor(0.1791, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1560/1560 [00:06<00:00, 241.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-12 loss: tensor(0.1803, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1560/1560 [00:06<00:00, 229.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-13 loss: tensor(0.1822, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1560/1560 [00:07<00:00, 221.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-14 loss: tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1560/1560 [00:06<00:00, 248.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-15 loss: tensor(0.1752, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 1560/1560 [00:06<00:00, 243.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-16 loss: tensor(0.1870, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 1560/1560 [00:06<00:00, 241.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-17 loss: tensor(0.1795, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 1560/1560 [00:06<00:00, 239.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-18 loss: tensor(0.1693, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 1560/1560 [00:06<00:00, 241.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-19 loss: tensor(0.1825, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "criterion = torch.nn.MSELoss()\n",
    "train_ae(model,criterion,optimizer,20,Noise_train_loader,Noise_test_loader,subset_size = 12,overlap = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (att_layer): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = EncoderClassifier(encoder,8,[8,5],12,6)\n",
    "classifier = classifier.to(device)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 1560/1560 [00:08<00:00, 181.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 loss:2.7344799041748047 accuracy:0.39201221955128207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1560/1560 [00:13<00:00, 119.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-1 loss:1.2297755479812622 accuracy:0.4793569711538462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1560/1560 [00:07<00:00, 210.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-2 loss:1.1868349313735962 accuracy:0.5060546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1560/1560 [00:07<00:00, 196.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-3 loss:1.1596285104751587 accuracy:0.527774439102564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1560/1560 [00:08<00:00, 191.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-4 loss:1.1336559057235718 accuracy:0.5445362580128205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1560/1560 [00:07<00:00, 195.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-5 loss:1.1135388612747192 accuracy:0.551747796474359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1560/1560 [00:07<00:00, 203.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-6 loss:1.1018449068069458 accuracy:0.5546173878205128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1560/1560 [00:08<00:00, 191.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-7 loss:1.0944889783859253 accuracy:0.5579577323717949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1560/1560 [00:07<00:00, 196.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-8 loss:1.0870729684829712 accuracy:0.5611378205128205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1560/1560 [00:13<00:00, 116.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-9 loss:1.0798307657241821 accuracy:0.5624048477564103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1560/1560 [00:11<00:00, 138.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-10 loss:1.0744620561599731 accuracy:0.5659104567307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1560/1560 [00:09<00:00, 170.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-11 loss:1.0699840784072876 accuracy:0.5660606971153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1560/1560 [00:08<00:00, 183.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-12 loss:1.0650140047073364 accuracy:0.5678986378205129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1560/1560 [00:07<00:00, 208.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-13 loss:1.061963438987732 accuracy:0.5717247596153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1560/1560 [00:08<00:00, 176.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-14 loss:1.058691143989563 accuracy:0.5733373397435897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1560/1560 [00:07<00:00, 198.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-15 loss:1.0545731782913208 accuracy:0.5752453926282052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 1560/1560 [00:07<00:00, 204.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-16 loss:1.0525054931640625 accuracy:0.5768529647435897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 1560/1560 [00:07<00:00, 210.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-17 loss:1.0499213933944702 accuracy:0.5791165865384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 1560/1560 [00:08<00:00, 186.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-18 loss:1.0466463565826416 accuracy:0.581064703525641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 1560/1560 [00:07<00:00, 196.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-19 loss:1.0435656309127808 accuracy:0.5849909855769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_classifier(classifier,CustomLoss(50,no_of_class=5),torch.optim.Adam,20,Noise_train_loader,Noise_test_loader,subset_size = 12,overlap = 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
