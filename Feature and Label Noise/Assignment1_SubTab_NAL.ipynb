{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noise_0_dataframe = pd.read_csv(\"../Data/Assignment1/data_0_noise\")\n",
    "Noise_Low_dataframe = pd.read_csv(\"../Data/Assignment1/data_Low_noise\")\n",
    "Noise_High_dataframe = pd.read_csv(\"../Data/Assignment1/data_High_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=Noise_Low_dataframe\n",
    "validation_dataframe=Noise_High_dataframe\n",
    "target_columns=\"target_10_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode = \"target_10_val\"\n",
    "class_index = list(dataframe[to_encode].unique())\n",
    "def encode(value, class_index = class_index):\n",
    "    return class_index.index(value)\n",
    "\n",
    "dataframe[to_encode] = dataframe[to_encode].apply(encode)\n",
    "validation_dataframe[to_encode] = validation_dataframe[to_encode].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, noise, transform=None, target_transform=None, drop=None, target=None):\n",
    "        self.dataframe = dataframe\n",
    "        if drop != None:\n",
    "            self.X = dataframe.drop(drop, axis=1).values\n",
    "        else:\n",
    "            self.X = dataframe.values\n",
    "        \n",
    "        self.y = dataframe[target].values\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.noise = noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, label = self.X[idx], self.y[idx]\n",
    "        return item, label\n",
    "\n",
    "    def get_noise(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(dataframe, \"0\",drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\",\"data_type\"],target=target_columns)\n",
    "Noise_train, Noise_test = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "Noise_train_loader = DataLoader(Noise_train, batch_size=128, shuffle=True)\n",
    "Noise_test_loader = DataLoader(Noise_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(torch.nn.Module):\n",
    "    def __init__(self, encoder, latent_dim, layers, subset_size, overlap, activation=torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.layers = []\n",
    "        linear = [torch.nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.latent_layer = torch.nn.Linear(latent_dim, layers[0])\n",
    "        \n",
    "        for i in range(len(linear)):\n",
    "            if (i == 0 or i == len(linear)-1):\n",
    "                self.layers.append(linear[i])\n",
    "            else:\n",
    "                self.layers.append(linear[i])\n",
    "                self.layers.append(activation)\n",
    "\n",
    "        self.layers = torch.nn.Sequential(*self.layers)\n",
    "        self.subset_size = subset_size\n",
    "        self.overlap = overlap\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.att_layer = torch.nn.Linear(latent_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.lrelu = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        subsets = []\n",
    "        num_columns = X.shape[1]\n",
    "        for i in range(0, num_columns-self.subset_size, self.subset_size-self.overlap):\n",
    "            subsets.append(X[:, i:i+self.subset_size])\n",
    "\n",
    "        preds = 0\n",
    "        h = 0\n",
    "        for subset in subsets:\n",
    "            pred = self.encoder(subset)\n",
    "            pred = self.latent_layer(pred)\n",
    "            h = self.att_layer(pred)\n",
    "            pred = self.lrelu(pred)\n",
    "            pred = self.layers(pred)\n",
    "            pred = self.softmax(pred)\n",
    "            preds += pred\n",
    "\n",
    "        h = h/len(subsets)\n",
    "        h = self.sigmoid(h)\n",
    "        preds = preds/len(subsets)\n",
    "\n",
    "        return preds, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetAutoencoder (torch.nn.Module):\n",
    "    def __init__(self, encoder_sizes,decoder_sizes,activation = torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        linear_encoder = [torch.nn.Linear(encoder_sizes[i],encoder_sizes[i+1]) for i in range(len(encoder_sizes)-1)]\n",
    "        linear_decoder = [torch.nn.Linear(decoder_sizes[i],decoder_sizes[i+1]) for i in range(len(decoder_sizes)-1)]\n",
    "        self.encoder = torch.nn.Sequential(*[l for layer in linear_encoder for l in (layer, activation)])\n",
    "        self.decoder = torch.nn.Sequential(*[l for layer in linear_decoder for l in (layer, activation)])\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.decoder(X)\n",
    "        return X\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self,reg,no_of_class=12):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.reg=reg\n",
    "        self.no_of_class=no_of_class\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        p=torch.Tensor(inputs[0]).to(device).t()\n",
    "        # Converting to 64 X 12 to 64 X 1\n",
    "        # _,p=torch.max(p,1)\n",
    "        targets=torch.eye(self.no_of_class).to(device)[targets].t()\n",
    "        tou=torch.Tensor(inputs[1]).to(device)\n",
    "        tou=tou.t()\n",
    "        # print(\"tou\",tou)\n",
    "        # print(tou)\n",
    "        loss_a= torch.t(targets)@torch.log(tou*(p-targets)+targets)\n",
    "        loss_b=self.reg*torch.log(tou)\n",
    "        loss_a=loss_a.diag().t()\n",
    "        # print(\"Loss a\",loss_a)\n",
    "        # print(\"Loss b\",loss_b)\n",
    "        loss=loss_a+loss_b\n",
    "        # print(\"loss\",loss.shape)\n",
    "        # print(\"return\",loss.mean().shape)\n",
    "        return -loss.mean()\n",
    "\n",
    "    def custom_p(p):\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsets(current_batch,subset_size,overlap):\n",
    "        subsets = []\n",
    "        num_columns = current_batch.shape[1]\n",
    "        for i in range(0, num_columns-subset_size,subset_size-overlap):\n",
    "            subsets.append(current_batch[:,i:i+subset_size])\n",
    "        return subsets\n",
    "\n",
    "\n",
    "def train_ae(model,criterion,optimizer,epochs,trainloader,testloader,subset_size = 10,overlap = 4,lr = 0.001 , verbose = True):\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        current_train_loss = 0\n",
    "        current_accuracy = []\n",
    "        for data, _ in tqdm(trainloader,desc = \"Training Epoch \"+str(epoch)):\n",
    "            data= data.to(device).float()\n",
    "            subsets = get_subsets(data,subset_size,overlap)\n",
    "            optimizer.zero_grad()\n",
    "            recons = []\n",
    "            subset_loss = 0\n",
    "            for subset in subsets:\n",
    "                output = model(subset)\n",
    "                recons.append(output)\n",
    "                subset_loss += criterion(data,output)\n",
    "            subset_loss = subset_loss.mean()\n",
    "            subset_loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch-{epoch} loss:\",subset_loss)\n",
    "        \n",
    "def train_classifier(model,criterion,optimizer,epochs,trainloader,testloader,lr=0.001,verbose = True,subset_size = 10,overlap = 2):\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        current_accuracy = []\n",
    "        for data, target in tqdm(trainloader,desc = \"Training Epoch \"+str(epoch)):\n",
    "            data, target = data.to(device).float(), target.to(device).long()\n",
    "            subsets = get_subsets(data,subset_size,overlap)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(output[0], 1)\n",
    "            total_train += target.size(0)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "            current_accuracy.append(correct_train/total_train)\n",
    "            running_train_loss+=loss\n",
    "        \n",
    "        running_train_loss /= len(trainloader)    \n",
    "        print(f\"epoch-{epoch} loss:{running_train_loss} accuracy:{correct_train/total_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubsetAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=24, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SubsetAutoencoder(encoder_sizes=[12,32,16,8],decoder_sizes=[8,16,32,24])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\amlenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Training Epoch 0: 100%|██████████| 1560/1560 [00:08<00:00, 188.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 loss: tensor(0.1827, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1560/1560 [00:14<00:00, 105.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-1 loss: tensor(0.1614, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1560/1560 [00:12<00:00, 120.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-2 loss: tensor(0.1546, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1560/1560 [00:13<00:00, 119.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-3 loss: tensor(0.1613, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1560/1560 [00:11<00:00, 140.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-4 loss: tensor(0.1530, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1560/1560 [00:10<00:00, 151.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-5 loss: tensor(0.1555, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1560/1560 [00:12<00:00, 122.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-6 loss: tensor(0.1542, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1560/1560 [00:09<00:00, 167.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-7 loss: tensor(0.1545, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1560/1560 [00:07<00:00, 205.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-8 loss: tensor(0.1544, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1560/1560 [00:12<00:00, 122.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-9 loss: tensor(0.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1560/1560 [00:07<00:00, 220.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-10 loss: tensor(0.1493, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1560/1560 [00:07<00:00, 211.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-11 loss: tensor(0.1455, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1560/1560 [00:06<00:00, 223.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-12 loss: tensor(0.1468, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1560/1560 [00:06<00:00, 228.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-13 loss: tensor(0.1485, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1560/1560 [00:06<00:00, 237.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-14 loss: tensor(0.1498, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1560/1560 [00:06<00:00, 228.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-15 loss: tensor(0.1481, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 1560/1560 [00:07<00:00, 217.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-16 loss: tensor(0.1436, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 1560/1560 [00:06<00:00, 235.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-17 loss: tensor(0.1385, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 1560/1560 [00:07<00:00, 205.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-18 loss: tensor(0.1502, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 1560/1560 [00:07<00:00, 205.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-19 loss: tensor(0.1500, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "criterion = torch.nn.MSELoss()\n",
    "train_ae(model,criterion,optimizer,20,Noise_train_loader,Noise_test_loader,subset_size = 12,overlap = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (latent_layer): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=5, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (att_layer): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = EncoderClassifier(encoder,8,[8,5],12,6)\n",
    "classifier = classifier.to(device)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 1950/1950 [00:11<00:00, 166.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 loss:1.0764548778533936 accuracy:0.6514543269230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1950/1950 [00:10<00:00, 178.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-1 loss:0.7552322149276733 accuracy:0.737479967948718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1950/1950 [00:11<00:00, 174.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-2 loss:0.737666666507721 accuracy:0.7393429487179487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1950/1950 [00:10<00:00, 177.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-3 loss:0.7285879254341125 accuracy:0.7414663461538461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1950/1950 [00:11<00:00, 175.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-4 loss:0.7212272882461548 accuracy:0.7417227564102564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1950/1950 [00:11<00:00, 174.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-5 loss:0.7141486406326294 accuracy:0.7441346153846153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1950/1950 [00:11<00:00, 175.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-6 loss:0.7087969183921814 accuracy:0.7444551282051282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1950/1950 [00:11<00:00, 163.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-7 loss:0.7038829922676086 accuracy:0.7446394230769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1950/1950 [00:11<00:00, 166.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-8 loss:0.6993588209152222 accuracy:0.7461658653846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1950/1950 [00:11<00:00, 164.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-9 loss:0.696841299533844 accuracy:0.7463701923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1950/1950 [00:11<00:00, 163.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-10 loss:0.6933028697967529 accuracy:0.7468349358974359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1950/1950 [00:11<00:00, 170.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-11 loss:0.691066324710846 accuracy:0.7474639423076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1950/1950 [00:11<00:00, 170.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-12 loss:0.6883957386016846 accuracy:0.7477564102564103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1950/1950 [00:11<00:00, 169.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-13 loss:0.6868290305137634 accuracy:0.7483533653846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1950/1950 [00:11<00:00, 176.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-14 loss:0.6835590600967407 accuracy:0.7492668269230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1950/1950 [00:11<00:00, 170.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-15 loss:0.6803105473518372 accuracy:0.7496474358974359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 1950/1950 [00:11<00:00, 176.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-16 loss:0.6766517162322998 accuracy:0.7511538461538462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 1950/1950 [00:11<00:00, 167.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-17 loss:0.6740013957023621 accuracy:0.7522996794871795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 1950/1950 [00:11<00:00, 176.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-18 loss:0.6719038486480713 accuracy:0.7524959935897436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 1950/1950 [00:11<00:00, 162.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-19 loss:0.6697731614112854 accuracy:0.7527043269230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_classifier(classifier,CustomLoss(50,no_of_class=5),torch.optim.Adam,20,Noise_train_loader,Noise_test_loader,subset_size = 12,overlap = 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
