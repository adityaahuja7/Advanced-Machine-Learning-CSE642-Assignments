{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from tabpfn import TabPFNClassifier\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, noise, transform=None, target_transform=None, drop=None, target=None):\n",
    "        self.dataframe = dataframe\n",
    "        if drop != None:\n",
    "            self.X = dataframe.drop(drop, axis=1).values\n",
    "        else:\n",
    "            self.X = dataframe.values\n",
    "        \n",
    "        self.y = dataframe[target].values\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.noise = noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, label = self.X[idx], self.y[idx]\n",
    "        return item, label\n",
    "\n",
    "    def get_noise(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noise_0_dataframe = pd.read_csv(\"../Data/Assignment1/data_0_noise\")\n",
    "Noise_Low_dataframe = pd.read_csv(\"../Data/Assignment1/data_Low_noise\")\n",
    "Noise_High_dataframe = pd.read_csv(\"../Data/Assignment1/data_High_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = list(Noise_0_dataframe[\"era\"].unique())\n",
    "class_index_noise = list(Noise_Low_dataframe[\"era\"].unique())\n",
    "class_index_t10v_noise = list(Noise_Low_dataframe[\"target_10_val\"].unique())\n",
    "\n",
    "def encode(value, class_index = class_index):\n",
    "    return class_index.index(value)\n",
    "\n",
    "def encode_noise(value, class_index = class_index_noise):\n",
    "    return class_index.index(value)\n",
    "\n",
    "def encode_noise_t10v(value, class_index = class_index_t10v_noise):\n",
    "    return class_index.index(value)\n",
    "\n",
    "\n",
    "Noise_0_dataframe[\"era\"] = Noise_0_dataframe[\"era\"].apply(encode)\n",
    "Noise_Low_dataframe[\"era\"] = Noise_Low_dataframe[\"era\"].apply(encode_noise)\n",
    "Noise_High_dataframe[\"era\"] = Noise_High_dataframe[\"era\"].apply(encode_noise)\n",
    "Noise_Low_dataframe[\"target_10_val\"] = Noise_Low_dataframe[\"target_10_val\"].apply(encode_noise_t10v)\n",
    "Noise_High_dataframe[\"target_10_val\"] = Noise_High_dataframe[\"target_10_val\"].apply(encode_noise_t10v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noise_0_dataset_era = CustomDataset(Noise_0_dataframe, \"0\",drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\"], target = \"era\")\n",
    "Noise_Low_dataset_era = CustomDataset(Noise_Low_dataframe, \"Low\", drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\",\"data_type\"], target = \"era\")\n",
    "Noise_High_dataset_era = CustomDataset(Noise_High_dataframe, \"High\", drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\",\"data_type\"], target = \"era\")\n",
    "Noise_Low_dataset_t10v = CustomDataset(Noise_Low_dataframe, \"Low\", drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\",\"data_type\"], target = \"target_10_val\")\n",
    "Noise_High_dataset_t10v = CustomDataset(Noise_High_dataframe, \"High\", drop = [\"row_num\",\"day\",\"era\",\"target_10_val\",\"target_5_val\",\"data_type\"], target = \"target_10_val\")\n",
    "Noise_0_train_era, Noise_0_test_era = random_split(Noise_0_dataset_era, [int(len(Noise_0_dataset_era)*0.8), int(len(Noise_0_dataset_era)*0.2)])\n",
    "Noise_Low_train_era, Noise_Low_test_era = random_split(Noise_Low_dataset_era, [int(len(Noise_Low_dataset_era)*0.8), int(len(Noise_Low_dataset_era)*0.2)])\n",
    "Noise_High_train_era, Noise_High_test_era = random_split(Noise_High_dataset_era, [int(len(Noise_High_dataset_era)*0.8), int(len(Noise_High_dataset_era)*0.2)])\n",
    "Noise_Low_train_t10v, Noise_Low_test_t10v = random_split(Noise_Low_dataset_t10v, [int(len(Noise_Low_dataset_t10v)*0.8), int(len(Noise_Low_dataset_t10v)*0.2)])\n",
    "Noise_High_train_t10v, Noise_High_test_t10v = random_split(Noise_High_dataset_t10v, [int(len(Noise_High_dataset_t10v)*0.8), int(len(Noise_High_dataset_t10v)*0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting batch_size=1000 for target_10_Val as tabpfn requires 1000 rows max\n",
    "\n",
    "#Setting up dataloaders\n",
    "Noise_0_era_train_loader = DataLoader(Noise_0_train_era, batch_size=512, shuffle=True)\n",
    "Noise_0_era_test_loader = DataLoader(Noise_0_test_era, batch_size=512, shuffle=True)\n",
    "########################################################################################\n",
    "Noise_Low_era_train_loader = DataLoader(Noise_Low_train_era, batch_size=512, shuffle=True)\n",
    "Noise_Low_era_test_loader = DataLoader(Noise_Low_test_era, batch_size=512, shuffle=True)\n",
    "########################################################################################\n",
    "Noise_High_era_train_loader = DataLoader(Noise_High_train_era, batch_size=512, shuffle=True)\n",
    "Noise_High_era_test_loader = DataLoader(Noise_High_test_era, batch_size=512, shuffle=True)\n",
    "########################################################################################\n",
    "Noise_Low_t10v_train_loader = DataLoader(Noise_Low_train_t10v, batch_size=1000, shuffle=True)\n",
    "Noise_Low_t10v_test_loader = DataLoader(Noise_Low_test_t10v, batch_size=512, shuffle=True)\n",
    "########################################################################################\n",
    "Noise_High_t10v_train_loader = DataLoader(Noise_High_train_t10v, batch_size=1000, shuffle=True)\n",
    "Noise_High_t10v_test_loader = DataLoader(Noise_High_test_t10v, batch_size=512, shuffle=True)\n",
    "########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicition_assembler(predictions,probabilities):\n",
    "    final_winner=[]\n",
    "    no_of_models=len(predictions)\n",
    "    batch_size=len(predictions[0])\n",
    "    for i in range(batch_size):\n",
    "        winner_dict={}\n",
    "        for j in range(no_of_models):\n",
    "            if predictions[j][i] not in winner_dict.keys():\n",
    "                winner_dict[predictions[j][i]]=0\n",
    "            winner_dict[predictions[j][i]]+=probabilities[j][i]\n",
    "        final_winner.append(max(winner_dict,key=winner_dict.get))\n",
    "    return final_winner\n",
    "\n",
    "def fit_test_tabpfn(train_dataloader,test_dataloader,no_of_models_to_ensemble=1,ensemble_config=1):\n",
    "    all_tabpfns=[]\n",
    "    for data,target in tqdm(train_dataloader, desc=\"FITTING\"):\n",
    "        classifier = TabPFNClassifier(device=device, N_ensemble_configurations=ensemble_config)\n",
    "        classifier.fit(data,target)\n",
    "        all_tabpfns.append(classifier)\n",
    "\n",
    "    total=0\n",
    "    correct=0\n",
    "    for data,target in tqdm(test_dataloader, desc=\"TESTING\"):\n",
    "        each_model_prob=[]\n",
    "        each_model_pred=[]\n",
    "        random_models = random.sample(all_tabpfns, no_of_models_to_ensemble)\n",
    "        for model in random_models:\n",
    "            y_pred,p_pred=model.predict(data,return_winning_probability=True)\n",
    "            each_model_prob.append(p_pred)\n",
    "            each_model_pred.append(y_pred)\n",
    "        y_pred_summ=predicition_assembler(each_model_pred,each_model_prob)\n",
    "        total+=len(y_pred_summ)\n",
    "        correct+=sum(1 for p, t in zip(y_pred_summ, target) if p == t)\n",
    "    print(f\"Accuracy: {correct/total:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FITTING: 100%|██████████| 250/250 [00:02<00:00, 98.02it/s] \n",
      "TESTING: 100%|██████████| 122/122 [13:20<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fit_test_tabpfn(train_dataloader=Noise_Low_t10v_train_loader,test_dataloader=Noise_Low_t10v_test_loader,no_of_models_to_ensemble=10,ensemble_config=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Noise_Low_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FITTING: 100%|██████████| 200/200 [00:01<00:00, 101.23it/s]\n",
      "TESTING: 100%|██████████| 98/98 [07:51<00:00,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fit_test_tabpfn(train_dataloader=Noise_High_t10v_train_loader,test_dataloader=Noise_High_t10v_test_loader,no_of_models_to_ensemble=10,ensemble_config=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(dict,key,value):\n",
    "    if key not in dict.keys():\n",
    "        dict[key]=0\n",
    "    dict[key]+=value\n",
    "    return dict\n",
    "\n",
    "def predicition_assembler(predictions1,probabilities1,predictions2,probabilities2,split):\n",
    "    final_winner=[]\n",
    "    no_of_models=len(predictions1)\n",
    "    batch_size=len(predictions1[0])\n",
    "    for i in range(batch_size):\n",
    "        winner_dict={}\n",
    "        for j in range(no_of_models):\n",
    "            value1=probabilities1[j][i]\n",
    "            value2=probabilities2[j][i]\n",
    "            if value1>value2:\n",
    "                value=value1\n",
    "                key=predictions1[j][i]\n",
    "            else:\n",
    "                value=value2\n",
    "                key=predictions2[j][i]+split+1\n",
    "            winner_dict=add_to_dict(winner_dict,key,value)\n",
    "        final_winner.append(max(winner_dict,key=winner_dict.get))\n",
    "    return final_winner\n",
    "\n",
    "def fit_test_tabpfn_for_era(train_dataloader,test_dataloader,no_of_models_to_ensemble=1,ensemble_config=1,split=6):\n",
    "    split_1=[]\n",
    "    split_2=[]\n",
    "    filtered_data=torch.empty(0)\n",
    "    filtered_target=torch.empty(0)\n",
    "    count=0\n",
    "    for data,target in tqdm(train_dataloader, desc=\"FITTING FOR SPLIT1\"):\n",
    "        classifier = TabPFNClassifier(device=device, N_ensemble_configurations=ensemble_config)\n",
    "        target[target>split]=9\n",
    "        valid_indices = target != 9\n",
    "        filtered_data =torch.cat([filtered_data,data[valid_indices]],dim=0)\n",
    "        filtered_target =torch.cat([filtered_target,target[valid_indices]],dim=0)\n",
    "        # all other\n",
    "        if len(filtered_data)>=1000 or count==len(train_dataloader)-1:\n",
    "            to_fit_data=filtered_data[:1000]\n",
    "            to_fit_target=filtered_target[:1000]\n",
    "            filtered_target=filtered_target[1000:]\n",
    "            filtered_data=filtered_data[1000:]\n",
    "            classifier.fit(to_fit_data,to_fit_target)\n",
    "            split_1.append(classifier)\n",
    "        count+=1\n",
    "\n",
    "    filtered_data=torch.empty(0)\n",
    "    filtered_target=torch.empty(0)\n",
    "    count=0\n",
    "    for data,target in tqdm(train_dataloader, desc=\"FITTING FOR SPLIT2\"):\n",
    "        classifier = TabPFNClassifier(device=device, N_ensemble_configurations=ensemble_config)\n",
    "        target-=(split+1)\n",
    "        #all other\n",
    "        target[target<0]=9\n",
    "        target[target>split]=9\n",
    "        valid_indices = target != 9\n",
    "        filtered_data =torch.cat([filtered_data,data[valid_indices]],dim=0)\n",
    "        filtered_target =torch.cat([filtered_target,target[valid_indices]],dim=0)\n",
    "        if len(filtered_data)>=1000 or count==len(train_dataloader)-1 :\n",
    "            to_fit_data=filtered_data[:1000]\n",
    "            to_fit_target=filtered_target[:1000]\n",
    "            filtered_target=filtered_target[1000:]\n",
    "            filtered_data=filtered_data[1000:]\n",
    "            classifier.fit(to_fit_data,to_fit_target)\n",
    "            split_2.append(classifier)\n",
    "        count+=1\n",
    "    \n",
    "    print(\"NO OF MODELS TO ENSEMBLE:\",no_of_models_to_ensemble)\n",
    "    if no_of_models_to_ensemble>min(len(split_1),len(split_2)):\n",
    "        no_of_models_to_ensemble=min(len(split_1),len(split_2))\n",
    "    \n",
    "    print(\"MAX MODELS AVAILABLE:\",min(len(split_1),len(split_2)))\n",
    "        \n",
    "    total=0\n",
    "    correct=0\n",
    "    for data,target in tqdm(test_dataloader, desc=\"TESTING\"):\n",
    "        each_model_prob_split1=[]\n",
    "        each_model_pred_split1=[]\n",
    "        each_model_prob_split2=[]\n",
    "        each_model_pred_split2=[]\n",
    "        # Zip the lists together\n",
    "        zipped_splits = list(zip(split_1, split_2))\n",
    "        random_models = random.sample(zipped_splits, no_of_models_to_ensemble)\n",
    "        random_models_split_1, random_models_split_2 = zip(*random_models)\n",
    "        for model in random_models_split_1:\n",
    "            y_pred,p_pred=model.predict(data,return_winning_probability=True)\n",
    "            each_model_prob_split1.append(p_pred)\n",
    "            each_model_pred_split1.append(y_pred)\n",
    "        for model in random_models_split_2:\n",
    "            y_pred,p_pred=model.predict(data,return_winning_probability=True)\n",
    "            each_model_prob_split2.append(p_pred)\n",
    "            each_model_pred_split2.append(y_pred)\n",
    "        y_pred_summ=predicition_assembler(each_model_pred_split1,each_model_prob_split1,each_model_pred_split2,each_model_prob_split2,split=split)\n",
    "        total+=len(y_pred_summ)\n",
    "        correct+=sum(1 for p, t in zip(y_pred_summ, target) if p == t)\n",
    "    print(f\"Accuracy: {correct/total:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FITTING FOR SPLIT1: 100%|██████████| 13/13 [00:00<00:00, 166.69it/s]\n",
      "FITTING FOR SPLIT2: 100%|██████████| 13/13 [00:00<00:00, 135.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO OF MODELS TO ENSEMBLE: 3\n",
      "MAX MODELS AVAILABLE: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TESTING: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fit_test_tabpfn_for_era(train_dataloader=Noise_0_era_train_loader,test_dataloader=Noise_0_era_test_loader,no_of_models_to_ensemble=10,ensemble_config=1,split=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FITTING FOR SPLIT1: 100%|██████████| 488/488 [00:03<00:00, 123.44it/s]\n",
      "FITTING FOR SPLIT2: 100%|██████████| 488/488 [00:04<00:00, 118.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO OF MODELS TO ENSEMBLE: 10\n",
      "MAX MODELS AVAILABLE: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TESTING:  28%|██▊       | 34/122 [01:38<04:02,  2.75s/it]"
     ]
    }
   ],
   "source": [
    "fit_test_tabpfn_for_era(train_dataloader=Noise_Low_era_train_loader,test_dataloader=Noise_Low_era_test_loader,no_of_models_to_ensemble=10,ensemble_config=1,split=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FITTING FOR SPLIT1: 100%|██████████| 390/390 [00:03<00:00, 127.56it/s]\n",
      "FITTING FOR SPLIT2: 100%|██████████| 390/390 [00:03<00:00, 121.61it/s]\n",
      "TESTING: 100%|██████████| 98/98 [02:23<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fit_test_tabpfn_for_era(train_dataloader=Noise_High_era_train_loader,test_dataloader=Noise_High_era_test_loader,no_of_models_to_ensemble=10,ensemble_config=1,split=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
